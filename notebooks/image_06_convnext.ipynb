{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2qA-0hoUjTV"
   },
   "source": [
    "# Rakuten Image Classification - ConvNeXt\n",
    "\n",
    "## ConvNeXt Model Exploration\n",
    "Testing ConvNeXt architecture to compare with Swin Transformer (best: 74.64%)\n",
    "\n",
    "**ConvNeXt Features:**\n",
    "- Modernized CNN architecture inspired by Vision Transformers\n",
    "- Efficient training with larger batch sizes and higher resolutions\n",
    "- Strong performance on image classification tasks\n",
    "\n",
    "**Training Strategy:**\n",
    "- Higher resolution: 384x384 (vs 224x224 for Swin)\n",
    "- Anti-overfitting: Mixup, CutMix, Stochastic Depth, **EMA**\n",
    "- Optimizer: AdamW with LayerNorm-aware weight decay\n",
    "- Data: Colab-friendly Google Drive loading (85% dev / 15% holdout)\n",
    "- WandB tracking for experiment monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyJNrwcdUjTW"
   },
   "source": [
    "## 1. Setup Environment & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hTj_1irUjTW"
   },
   "outputs": [],
   "source": "# @title Install Dependencies\n!pip install -q timm gdown pandas scikit-learn matplotlib wandb torch torchvision tqdm\n\nimport os\nimport gc\nfrom datetime import datetime  # âœ… Fixed: Only import datetime class\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport gdown\nimport timm\nimport wandb\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom torch.cuda.amp import GradScaler\nfrom timm.utils import ModelEmaV2  # EMA support\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nset_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"âœ… Environment setup complete. Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   CUDA Version: {torch.version.cuda}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdQb3ZkBUjTW"
   },
   "outputs": [],
   "source": [
    "# @title WandB Login\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob-tC1AXUjTX"
   },
   "source": [
    "## 2. Download Data with Proper Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxGIKMw3UjTX"
   },
   "outputs": [],
   "source": [
    "# @title Download CSV Data\n",
    "def load_csv_from_gdrive(share_url: str, **read_csv_kwargs) -> pd.DataFrame:\n",
    "    try:\n",
    "        file_id = share_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "        download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        return pd.read_csv(download_url, **read_csv_kwargs)\n",
    "    except IndexError:\n",
    "        print(f\"Error parsing URL: {share_url}\")\n",
    "        return None\n",
    "\n",
    "print(\"Downloading CSV data...\")\n",
    "X_train_url = \"https://drive.google.com/file/d/1geSiJTTjamysiSbJ8-W9gR1kv-x6HyEd/view?usp=drive_link\"\n",
    "y_train_url = \"https://drive.google.com/file/d/16czWmLR5Ff0s5aYIqy1rHT7hc6Gcpfw3/view?usp=sharing\"\n",
    "\n",
    "try:\n",
    "    X_train_full = load_csv_from_gdrive(X_train_url)\n",
    "    y_train_full = load_csv_from_gdrive(y_train_url)\n",
    "\n",
    "    if X_train_full is not None and y_train_full is not None:\n",
    "        print(f\"Total data loaded: {len(X_train_full):,} samples\")\n",
    "\n",
    "        # ============================================================================\n",
    "        # PROPER DATA SPLIT (85% dev / 15% holdout)\n",
    "        # ============================================================================\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SPLITTING DATA (85% dev / 15% holdout)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        X_dev, X_holdout, y_dev, y_holdout = train_test_split(\n",
    "            X_train_full,\n",
    "            y_train_full['prdtypecode'],\n",
    "            test_size=0.15,\n",
    "            random_state=42,\n",
    "            stratify=y_train_full['prdtypecode']\n",
    "        )\n",
    "\n",
    "        df_dev = X_dev.copy()\n",
    "        df_dev['prdtypecode'] = y_dev\n",
    "\n",
    "        df_holdout = X_holdout.copy()\n",
    "        df_holdout['prdtypecode'] = y_holdout\n",
    "\n",
    "        print(f\"âœ“ Development set: {len(df_dev):,} samples (85%)\")\n",
    "        print(f\"âœ“ Hold-out test set: {len(df_holdout):,} samples (15%)\")\n",
    "        print(f\"âœ“ Classes: {df_dev['prdtypecode'].nunique()}\")\n",
    "        print(\"\\nâš ï¸  CRITICAL: Holdout set will ONLY be used for final evaluation!\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        raise ValueError(\"Failed to load DataFrames\")\n",
    "except Exception as e:\n",
    "    print(f\"CSV download failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0818Ge4vUjTX"
   },
   "outputs": [],
   "source": [
    "# @title Download Images\n",
    "IMAGE_FILE_ID = \"15ZkS0iTQ7j3mHpxil4mABlXwP-jAN_zi\"\n",
    "\n",
    "if not os.path.exists(\"/content/images\"):\n",
    "    print(\"\\nDownloading images...\")\n",
    "    os.makedirs(\"/content/tmp\", exist_ok=True)\n",
    "    os.makedirs(\"/content/images\", exist_ok=True)\n",
    "    !gdown --id $IMAGE_FILE_ID -O /content/tmp/images.zip\n",
    "\n",
    "    print(\"Unzipping images...\")\n",
    "    !unzip -q -o /content/tmp/images.zip -d /content/images\n",
    "    print(\"Images unzipped\")\n",
    "else:\n",
    "    print(\"\\nImages already exist, skipping download\")\n",
    "\n",
    "IMG_ROOT = \"/content/images/images/image_train\"\n",
    "print(f\"Image Root: {IMG_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6V8noNsmUjTX"
   },
   "source": [
    "## 3. Label Encoding (FIT ON DEV ONLY!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swkaPg43UjTY"
   },
   "outputs": [],
   "source": [
    "# @title Label Encoding\n",
    "print(\"=\"*80)\n",
    "print(\"LABEL ENCODING (DEV SET ONLY)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Encode labels on DEV SET ONLY\n",
    "le = LabelEncoder()\n",
    "le.fit(df_dev['prdtypecode'])  # FIT ONLY ON DEV\n",
    "\n",
    "df_dev['encoded_label'] = le.transform(df_dev['prdtypecode'])\n",
    "df_holdout['encoded_label'] = le.transform(df_holdout['prdtypecode'])\n",
    "\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "print(f\"âœ“ LabelEncoder fitted on dev set ONLY (no data leakage)\")\n",
    "print(f\"âœ“ Number of classes: {NUM_CLASSES}\")\n",
    "assert NUM_CLASSES == 27, f\"Expected 27 classes, got {NUM_CLASSES}\"\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2ruHCFvUjTY"
   },
   "source": [
    "## 4. Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddTjuDbqUjTY"
   },
   "outputs": [],
   "source": [
    "# @title Split Dev into Train/Val\n",
    "print(\"=\"*80)\n",
    "print(\"SPLITTING DEV SET (85% train / 15% val)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df_dev,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=df_dev['encoded_label']\n",
    ")\n",
    "\n",
    "total_samples = len(df_dev) + len(df_holdout)\n",
    "print(f\"âœ“ Training:   {len(train_df):,} samples (~{len(train_df)/total_samples*100:.1f}%)\")\n",
    "print(f\"âœ“ Validation: {len(val_df):,} samples (~{len(val_df)/total_samples*100:.1f}%)\")\n",
    "print(f\"âœ“ Hold-out:   {len(df_holdout):,} samples (15.0%)\")\n",
    "print(\"\\nâš ï¸  Model selection will use Train/Val ONLY\")\n",
    "print(\"âš ï¸  Holdout will be evaluated at the END\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54bCILcbUjTY"
   },
   "source": [
    "## 5. Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5p1KeUcXUjTY"
   },
   "outputs": [],
   "source": [
    "# @title Dataset Class (Optimized for Performance)\n",
    "class RakutenImageDataset(Dataset):\n",
    "    def __init__(self, df, img_root, transform=None):\n",
    "        # âœ… Pre-convert to lists for faster access (avoid .iloc performance issue)\n",
    "        self.image_ids = df['imageid'].tolist()\n",
    "        self.product_ids = df['productid'].tolist()\n",
    "        self.labels = df['encoded_label'].tolist()\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"âœ“ Dataset initialized with {len(self.labels):,} samples (optimized)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # âœ… Direct list access\n",
    "        imageid = self.image_ids[idx]\n",
    "        productid = self.product_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Image Processing\n",
    "        img_name = f\"image_{imageid}_product_{productid}.jpg\"\n",
    "        img_path = os.path.join(self.img_root, img_name)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, OSError):\n",
    "            # Fallback for missing/corrupt images\n",
    "            image = Image.new('RGB', (384, 384), (0, 0, 0))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "print(f\"Dataset class ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ci_85LQ8UjTY"
   },
   "source": [
    "## 6. Model Definition - ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiYYYkJ9UjTY"
   },
   "outputs": [],
   "source": [
    "# @title ConvNeXt Model\n",
    "class RakutenConvNeXt(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNeXt for Rakuten product classification.\n",
    "\n",
    "    Anti-Overfitting Features:\n",
    "    - Stochastic Depth (drop_path_rate)\n",
    "    - LayerNorm + Dropout in classification head\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'convnext_base',\n",
    "        num_classes: int = 27,\n",
    "        pretrained: bool = True,\n",
    "        drop_path_rate: float = 0.3\n",
    "    ):\n",
    "        super(RakutenConvNeXt, self).__init__()\n",
    "\n",
    "        # Load ConvNeXt backbone with Stochastic Depth\n",
    "        # Note: ConvNeXt doesn't accept img_size parameter in timm\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,  # Remove default head\n",
    "            global_pool='avg',\n",
    "            drop_path_rate=drop_path_rate  # Stochastic Depth\n",
    "        )\n",
    "\n",
    "        feature_dim = self.backbone.num_features\n",
    "\n",
    "        # Classification head with LayerNorm + Dropout\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.model_name = model_name\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "\n",
    "        print(f\"âœ“ RakutenConvNeXt initialized:\")\n",
    "        print(f\"  - Model: {model_name}\")\n",
    "        print(f\"  - Drop Path Rate: {drop_path_rate}\")\n",
    "        print(f\"  - Head: {feature_dim} â†’ 512 â†’ {num_classes}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.backbone(x)\n",
    "        logits = self.head(features)\n",
    "        return logits\n",
    "\n",
    "print(\"âœ“ RakutenConvNeXt class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt6A8aG2UjTY"
   },
   "source": [
    "## 7. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc7q9NEKUjTY"
   },
   "outputs": [],
   "source": [
    "# @title Training Configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_name\": \"convnext_base\",  # Using ConvNeXt Base\n",
    "    \"img_size\": 384,  # Higher resolution for better performance\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "\n",
    "    # Training - Optimized for A100 Colab Pro\n",
    "    \"batch_size\": 64,  # Adjusted for 384x384 on A100\n",
    "    \"num_epochs\": 30,\n",
    "    \"learning_rate\": 1e-4,  # ConvNeXt works well with higher LR\n",
    "    \"weight_decay\": 0.05,\n",
    "\n",
    "    # Anti-Overfitting\n",
    "    \"drop_path_rate\": 0.3,\n",
    "    \"mixup_alpha\": 0.8,\n",
    "    \"cutmix_alpha\": 1.0,\n",
    "    \"label_smoothing\": 0.1,\n",
    "    \"use_ema\": True,  # âœ… EMA enabled\n",
    "    \"ema_decay\": 0.9999,  # EMA decay rate\n",
    "\n",
    "    # Other\n",
    "    \"early_stopping_patience\": 5,\n",
    "    \"use_amp\": True,\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONVNEXT TRAINING CONFIGURATION (Colab A100)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Image Size: {CONFIG['img_size']}x{CONFIG['img_size']} (higher resolution)\")\n",
    "print(f\"Batch Size: {CONFIG['batch_size']} (optimized for A100 + 384x384)\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Learning Rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Weight Decay: {CONFIG['weight_decay']}\")\n",
    "print(f\"\\nAnti-Overfitting:\")\n",
    "print(f\"  - Drop Path: {CONFIG['drop_path_rate']}\")\n",
    "print(f\"  - Mixup Alpha: {CONFIG['mixup_alpha']}\")\n",
    "print(f\"  - CutMix Alpha: {CONFIG['cutmix_alpha']}\")\n",
    "print(f\"  - Label Smoothing: {CONFIG['label_smoothing']}\")\n",
    "print(f\"  - EMA: {CONFIG['use_ema']} (decay={CONFIG['ema_decay']})\")\n",
    "print(f\"\\nAMP: {CONFIG['use_amp']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPAEQB0RUjTZ"
   },
   "source": [
    "## 8. Data Transforms & Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHEGlqNiUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Data Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(384, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(438),\n",
    "    transforms.CenterCrop(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = RakutenImageDataset(train_df, IMG_ROOT, transform=train_transform)\n",
    "val_dataset = RakutenImageDataset(val_df, IMG_ROOT, transform=val_transform)\n",
    "holdout_dataset = RakutenImageDataset(df_holdout, IMG_ROOT, transform=val_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "holdout_loader = DataLoader(\n",
    "    holdout_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Train batches: {len(train_loader):,}\")\n",
    "print(f\"âœ“ Val batches: {len(val_loader):,}\")\n",
    "print(f\"âœ“ Holdout batches: {len(holdout_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaHdnX9cUjTZ"
   },
   "source": [
    "## 9. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3urnABxUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Initialize Model\n",
    "model = RakutenConvNeXt(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    num_classes=NUM_CLASSES,\n",
    "    pretrained=True,\n",
    "    drop_path_rate=CONFIG[\"drop_path_rate\"]\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize EMA\n",
    "model_ema = None\n",
    "if CONFIG[\"use_ema\"]:\n",
    "    model_ema = ModelEmaV2(model, decay=CONFIG[\"ema_decay\"])\n",
    "    print(f\"âœ“ EMA initialized with decay={CONFIG['ema_decay']}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoOYRzf4UjTZ"
   },
   "source": [
    "## 10. Training Setup (Mixup/CutMix + Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4LyoDQFUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Mixup/CutMix & Optimizer Setup\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "\n",
    "# Initialize Mixup/CutMix\n",
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=CONFIG[\"mixup_alpha\"],\n",
    "    cutmix_alpha=CONFIG[\"cutmix_alpha\"],\n",
    "    cutmix_minmax=None,\n",
    "    prob=1.0,  # Apply to all batches\n",
    "    switch_prob=0.5,  # 50% Mixup, 50% CutMix\n",
    "    mode='batch',\n",
    "    label_smoothing=CONFIG[\"label_smoothing\"],\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Loss functions\n",
    "criterion_train = SoftTargetCrossEntropy()  # For Mixup (soft labels)\n",
    "criterion_val = nn.CrossEntropyLoss()       # For validation (hard labels)\n",
    "\n",
    "print(\"âœ“ Mixup & CutMix initialized\")\n",
    "print(f\"  Mixup alpha: {CONFIG['mixup_alpha']}\")\n",
    "print(f\"  CutMix alpha: {CONFIG['cutmix_alpha']}\")\n",
    "\n",
    "# Optimizer - AdamW with LayerNorm-aware weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Scheduler - Cosine Annealing\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=CONFIG[\"num_epochs\"],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# AMP Scaler\n",
    "scaler = GradScaler() if CONFIG[\"use_amp\"] else None\n",
    "\n",
    "print(\"âœ“ Optimizer: AdamW with Cosine Annealing\")\n",
    "print(f\"âœ“ AMP: {CONFIG['use_amp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8qx6k88UjTZ"
   },
   "source": [
    "## 11. Training Loop with WandB & EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiYlHijgUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Initialize WandB\n",
    "import sys\n",
    "ENVIRONMENT = \"colab\" if 'google.colab' in sys.modules else \"local\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"rakuten-image\",\n",
    "    entity=\"xiaosong-dev-formation-data-science\",\n",
    "    name=f\"convnext_ema_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "    tags=[\"image\", \"convnext\", \"v1\", \"ema\", \"production\", ENVIRONMENT],\n",
    "    config=CONFIG,\n",
    "    notes=\"ConvNeXt Base @ 384x384 with EMA + anti-overfitting (Mixup, CutMix, Stochastic Depth)\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ WandB initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ym1hvlSAUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Training Loop with EMA\n",
    "best_val_acc = 0.0\n",
    "best_val_f1 = 0.0\n",
    "best_ema_acc = 0.0\n",
    "best_ema_f1 = 0.0\n",
    "patience_counter = 0\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_f1\": [],\n",
    "    \"ema_val_acc\": [],\n",
    "    \"ema_val_f1\": []\n",
    "}\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if CONFIG[\"use_amp\"]:\n",
    "                with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = val_loss / len(loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return avg_loss, acc * 100, f1\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ STARTING CONVNEXT TRAINING WITH EMA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # ========================================================================\n",
    "    # TRAINING with Mixup/CutMix\n",
    "    # ========================================================================\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for images, labels in train_pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Apply Mixup/CutMix\n",
    "        images, labels = mixup_fn(images, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if CONFIG[\"use_amp\"]:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "                loss = criterion_train(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion_train(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update EMA\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # ========================================================================\n",
    "    # VALIDATION (no Mixup) - Regular Model\n",
    "    # ========================================================================\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion_val)\n",
    "\n",
    "    # ========================================================================\n",
    "    # VALIDATION - EMA Model\n",
    "    # ========================================================================\n",
    "    ema_val_acc, ema_val_f1 = 0.0, 0.0\n",
    "    if model_ema is not None:\n",
    "        _, ema_val_acc, ema_val_f1 = evaluate(model_ema.module, val_loader, criterion_val)\n",
    "\n",
    "    # ========================================================================\n",
    "    # LOGGING & CHECKPOINTING\n",
    "    # ========================================================================\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_f1\"].append(val_f1)\n",
    "    history[\"ema_val_acc\"].append(ema_val_acc)\n",
    "    history[\"ema_val_f1\"].append(ema_val_f1)\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "        \"ema_val_acc\": ema_val_acc,\n",
    "        \"ema_val_f1\": ema_val_f1,\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "\n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val (Regular): Acc={val_acc:.2f}%, F1={val_f1:.4f}\")\n",
    "    if model_ema is not None:\n",
    "        print(f\"  Val (EMA):     Acc={ema_val_acc:.2f}%, F1={ema_val_f1:.4f}\")\n",
    "\n",
    "    # Save best model (use EMA if better)\n",
    "    current_best_acc = max(val_acc, ema_val_acc)\n",
    "    if current_best_acc > best_val_acc:\n",
    "        best_val_acc = current_best_acc\n",
    "        patience_counter = 0\n",
    "\n",
    "        # Save the better model\n",
    "        if ema_val_acc > val_acc and model_ema is not None:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model_ema.module.state_dict(),\n",
    "                'val_acc': ema_val_acc,\n",
    "                'val_f1': ema_val_f1,\n",
    "                'is_ema': True\n",
    "            }, \"convnext_best.pth\")\n",
    "            print(f\"  âœ… Best EMA model saved! (Acc: {ema_val_acc:.2f}%, F1: {ema_val_f1:.4f})\")\n",
    "        else:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_f1': val_f1,\n",
    "                'is_ema': False\n",
    "            }, \"convnext_best.pth\")\n",
    "            print(f\"  âœ… Best model saved! (Acc: {val_acc:.2f}%, F1: {val_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  â³ No improvement ({patience_counter}/{CONFIG['early_stopping_patience']})\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if patience_counter >= CONFIG[\"early_stopping_patience\"]:\n",
    "        print(f\"\\nâš ï¸ Early stopping triggered after {epoch + 1} epochs\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Val Acc: {best_val_acc:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp0JnbT6UjTZ"
   },
   "source": [
    "## 12. Final Evaluation on Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAqtolGYUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Load Best Model & Evaluate on Holdout\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION ON HOLDOUT TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(\"âš ï¸  This is the FIRST and ONLY time holdout data is used!\\n\")\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(\"convnext_best.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "is_ema = checkpoint.get('is_ema', False)\n",
    "\n",
    "print(f\"Loaded {'EMA' if is_ema else 'Regular'} model from epoch {checkpoint['epoch']}\")\n",
    "\n",
    "holdout_loss, holdout_acc, holdout_f1 = evaluate(model, holdout_loader, criterion_val)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Validation Acc: {checkpoint['val_acc']:.2f}%\")\n",
    "print(f\"Best Validation F1:  {checkpoint['val_f1']:.4f}\")\n",
    "print(f\"\\nHoldout Test Acc:    {holdout_acc:.2f}%\")\n",
    "print(f\"Holdout Test F1:     {holdout_f1:.4f}\")\n",
    "print(f\"\\nDifference:          {holdout_acc - checkpoint['val_acc']:+.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Log final results to WandB\n",
    "wandb.log({\n",
    "    \"final/best_val_acc\": checkpoint['val_acc'],\n",
    "    \"final/best_val_f1\": checkpoint['val_f1'],\n",
    "    \"final/holdout_acc\": holdout_acc,\n",
    "    \"final/holdout_f1\": holdout_f1,\n",
    "    \"final/is_ema\": is_ema\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tEAtUXsUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Detailed Classification Report\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(holdout_loader, desc=\"Final Prediction\"):\n",
    "        images = images.to(device)\n",
    "\n",
    "        if CONFIG[\"use_amp\"]:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"\\nClassification Report (Holdout):\")\n",
    "print(classification_report(all_labels, all_preds, digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJHY_kvbUjTZ"
   },
   "source": [
    "## 13. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7FK8Up6UjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Training Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history[\"train_loss\"], label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0].plot(history[\"val_loss\"], label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy - Regular vs EMA\n",
    "axes[1].plot(history[\"val_acc\"], label='Val Acc (Regular)', marker='o', linewidth=2)\n",
    "if CONFIG[\"use_ema\"]:\n",
    "    axes[1].plot(history[\"ema_val_acc\"], label='Val Acc (EMA)', marker='s', linewidth=2, linestyle='--')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Validation Accuracy (Regular vs EMA)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('convnext_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved: convnext_training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyBw3KHuUjTa"
   },
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70ouzWRiUjTa"
   },
   "outputs": [],
   "source": [
    "# @title Training Summary\n",
    "print(\"=\"*80)\n",
    "print(\"CONVNEXT TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {CONFIG['model_name']} @ {CONFIG['img_size']}x{CONFIG['img_size']}\")\n",
    "print(f\"Total samples: {len(train_df) + len(val_df) + len(df_holdout):,}\")\n",
    "print(f\"Training:   {len(train_df):,}\")\n",
    "print(f\"Validation: {len(val_df):,}\")\n",
    "print(f\"Hold-out:   {len(df_holdout):,}\")\n",
    "print(f\"Classes: {NUM_CLASSES}\")\n",
    "print(f\"\\nBest Val Acc: {checkpoint['val_acc']:.2f}% {'(EMA)' if is_ema else '(Regular)'}\")\n",
    "print(f\"Best Val F1:  {checkpoint['val_f1']:.4f}\")\n",
    "print(f\"\\nHoldout Acc:  {holdout_acc:.2f}%\")\n",
    "print(f\"Holdout F1:   {holdout_f1:.4f}\")\n",
    "print(f\"\\nComparison with Swin V2 (best: 74.64%):\")\n",
    "if holdout_acc > 74.64:\n",
    "    print(f\"  âœ… ConvNeXt is BETTER: +{holdout_acc - 74.64:.2f}%\")\n",
    "elif holdout_acc < 74.64:\n",
    "    print(f\"  âŒ ConvNeXt is WORSE: {holdout_acc - 74.64:.2f}%\")\n",
    "else:\n",
    "    print(f\"  âž– ConvNeXt is EQUAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "wandb.finish()\n",
    "print(\"\\nâœ… Training complete! WandB run finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDWH5B5iUjTa"
   },
   "source": [
    "## 15. Save to Google Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sg1JnyyDUjTa"
   },
   "outputs": [],
   "source": [
    "# @title Save Model to Google Drive\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "target_dir = \"/content/drive/MyDrive/Rakuten_models\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "model_type = \"ema\" if is_ema else \"regular\"\n",
    "target_file = os.path.join(target_dir, f\"convnext_{model_type}_{timestamp}.pth\")\n",
    "shutil.copy(\"convnext_best.pth\", target_file)\n",
    "print(f\"âœ“ Model saved to: {target_file}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}