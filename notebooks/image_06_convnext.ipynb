{
 "cells": [
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# PHASE 3: EXPORT VAL PREDICTIONS (ALIGNMENT-SAFE)\n# ============================================================================\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPORTING VAL PREDICTIONS FOR FUSION\")\nprint(\"=\"*80)\n\nfrom src.data.label_mapping import CANONICAL_CLASSES, reorder_probs_to_canonical\nfrom src.export.model_exporter import export_predictions\n\n# Index-returning dataset wrapper for alignment safety\nclass IndexedDataset(Dataset):\n    def __init__(self, base_dataset_full, indices):\n        \"\"\"Dataset that returns (image, real_idx) for alignment verification.\n        \n        Args:\n            base_dataset_full: Full RakutenImageDataset over df_full\n            indices: Subset indices to use (e.g., splits[\"val_idx\"])\n        \"\"\"\n        self.base_dataset = base_dataset_full\n        self.indices = indices\n    \n    def __len__(self):\n        return len(self.indices)\n    \n    def __getitem__(self, i):\n        real_idx = int(self.indices[i])\n        img, _ = self.base_dataset[real_idx]\n        return img, real_idx\n\n# Create full dataset for export (requires encoded_label column)\ndf_full_labeled = df_full.copy()\ndf_full_labeled['encoded_label'] = le.transform(df_full_labeled['prdtypecode'])\n\nfull_dataset_for_export = RakutenImageDataset(\n    df=df_full_labeled,\n    img_root=IMG_ROOT,\n    transform=val_transform\n)\n\n# Create indexed dataset for val split\nval_dataset_indexed = IndexedDataset(full_dataset_for_export, splits[\"val_idx\"])\nval_loader_indexed = DataLoader(\n    val_dataset_indexed,\n    batch_size=CONFIG[\"batch_size\"],\n    shuffle=False,\n    num_workers=CONFIG.get(\"num_workers\", 0),\n    pin_memory=True\n)\n\n# Load best model from checkpoint\nprint(\"Loading best model from convnext_best.pth\")\ncheckpoint = torch.load(\"convnext_best.pth\", map_location=device, weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Collect predictions and indices\nval_probs_list = []\nval_seen_idx_list = []\n\nprint(\"Running inference on val split...\")\nwith torch.no_grad():\n    for images, indices in tqdm(val_loader_indexed, desc=\"Val Inference\"):\n        images = images.to(device)\n        \n        if CONFIG[\"use_amp\"]:\n            with torch.amp.autocast(device_type=\"cuda\"):\n                outputs = model(images)\n        else:\n            outputs = model(images)\n        \n        probs = torch.softmax(outputs, dim=-1)\n        val_probs_list.append(probs.cpu().numpy())\n        val_seen_idx_list.append(indices.cpu().numpy())\n\n# Concatenate results\nval_probs = np.concatenate(val_probs_list, axis=0)\nval_idx = np.concatenate(val_seen_idx_list)\n\n# Defensive assertions: verify alignment\nprint(\"\\nAlignment verification:\")\nprint(f\"  Collected {len(val_idx)} samples\")\nprint(f\"  Expected {len(splits['val_idx'])} samples\")\n\nassert len(val_idx) == len(splits[\"val_idx\"]), f\"Sample count mismatch: {len(val_idx)} != {len(splits['val_idx'])}\"\nassert np.array_equal(np.sort(val_idx), np.sort(splits[\"val_idx\"])), \"Index set mismatch\"\nassert np.array_equal(val_idx, splits[\"val_idx\"]), \"Index order mismatch (shuffle=False violation)\"\nprint(\"  ‚úì Alignment verified\")\n\n# Get ground truth labels from df_full using collected indices\nval_labels = df_full.iloc[val_idx][\"prdtypecode\"].values\n\n# Verify encoder classes match model output shape\nassert len(le.classes_) == val_probs.shape[1], f\"Encoder classes ({len(le.classes_)}) != probs shape[1] ({val_probs.shape[1]})\"\n\n# Reorder probabilities to canonical class order\nprint(\"\\nReordering probabilities to canonical class order...\")\nval_probs_aligned = reorder_probs_to_canonical(val_probs, le.classes_, CANONICAL_CLASSES)\nprint(f\"  Input shape: {val_probs.shape} ‚Üí Output shape: {val_probs_aligned.shape}\")\n\n# Export predictions\nprint(\"\\nExporting predictions...\")\nexport_result = export_predictions(\n    out_dir=\"artifacts/exports\",\n    model_name=\"convnext\",\n    split_name=\"val\",\n    idx=val_idx,\n    split_signature=sig,\n    probs=val_probs_aligned,\n    classes=CANONICAL_CLASSES,\n    y_true=val_labels,\n    extra_meta={\n        \"model_architecture\": CONFIG[\"model_name\"],\n        \"checkpoint\": \"convnext_best.pth\",\n        \"image_size\": CONFIG[\"img_size\"],\n        \"batch_size\": CONFIG[\"batch_size\"],\n        \"drop_path_rate\": CONFIG[\"drop_path_rate\"],\n        \"use_ema\": CONFIG[\"use_ema\"],\n    }\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPORT SUMMARY\")\nprint(\"=\"*80)\nprint(f\"NPZ file:     {export_result['npz_path']}\")\nprint(f\"Metadata:     {export_result['meta_json_path']}\")\nprint(f\"Classes_fp:   {export_result['classes_fp']}\")\nprint(f\"Split_sig:    {export_result['split_signature']}\")\nprint(f\"Num samples:  {export_result['num_samples']}\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2qA-0hoUjTV"
   },
   "source": [
    "# Rakuten Image Classification - ConvNeXt\n",
    "\n",
    "## ConvNeXt Model Exploration\n",
    "Testing ConvNeXt architecture to compare with Swin Transformer (best: 74.64%)\n",
    "\n",
    "**ConvNeXt Features:**\n",
    "- Modernized CNN architecture inspired by Vision Transformers\n",
    "- Efficient training with larger batch sizes and higher resolutions\n",
    "- Strong performance on image classification tasks\n",
    "\n",
    "**Training Strategy:**\n",
    "- Higher resolution: 384x384 (vs 224x224 for Swin)\n",
    "- Anti-overfitting: Mixup, CutMix, Stochastic Depth, **EMA**\n",
    "- Optimizer: AdamW with LayerNorm-aware weight decay\n",
    "- Data: Colab-friendly Google Drive loading (85% dev / 15% holdout)\n",
    "- WandB tracking for experiment monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyJNrwcdUjTW"
   },
   "source": [
    "## 1. Setup Environment & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hTj_1irUjTW"
   },
   "outputs": [],
   "source": [
    "# @title Install Dependencies",
    "!pip install -q timm gdown pandas scikit-learn matplotlib torch torchvision tqdm",
    "",
    "import os",
    "import gc",
    "from datetime import datetime  # ‚úÖ Fixed: Only import datetime class",
    "import torch",
    "import torch.nn as nn",
    "import pandas as pd",
    "import numpy as np",
    "import gdown",
    "import timm",
    "from torch.utils.data import Dataset, DataLoader",
    "from PIL import Image",
    "from sklearn.model_selection import train_test_split",
    "from sklearn.preprocessing import LabelEncoder",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report",
    "from tqdm.auto import tqdm",
    "from torchvision import transforms",
    "from torch.cuda.amp import GradScaler",
    "from timm.utils import ModelEmaV2  # EMA support",
    "import matplotlib.pyplot as plt",
    "",
    "# Set seed for reproducibility",
    "def set_seed(seed=42):",
    "    torch.manual_seed(seed)",
    "    torch.cuda.manual_seed(seed)",
    "    torch.cuda.manual_seed_all(seed)",
    "    np.random.seed(seed)",
    "    torch.backends.cudnn.deterministic = True",
    "    torch.backends.cudnn.benchmark = True",
    "",
    "set_seed(42)",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
    "print(f\"‚úÖ Environment setup complete. Using device: {device}\")",
    "if torch.cuda.is_available():",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob-tC1AXUjTX"
   },
   "source": [
    "## 2. Download Data with Proper Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxGIKMw3UjTX"
   },
   "outputs": [],
   "source": "# @title Download CSV Data\ndef load_csv_from_gdrive(share_url: str, **read_csv_kwargs) -> pd.DataFrame:\n    try:\n        file_id = share_url.split(\"/d/\")[1].split(\"/\")[0]\n        download_url = f\"https://drive.google.com/uc?id={file_id}\"\n        return pd.read_csv(download_url, **read_csv_kwargs)\n    except IndexError:\n        print(f\"Error parsing URL: {share_url}\")\n        return None\n\nprint(\"Downloading CSV data...\")\nX_train_url = \"https://drive.google.com/file/d/1geSiJTTjamysiSbJ8-W9gR1kv-x6HyEd/view?usp=drive_link\"\ny_train_url = \"https://drive.google.com/file/d/16czWmLR5Ff0s5aYIqy1rHT7hc6Gcpfw3/view?usp=sharing\"\n\ntry:\n    X_train_full = load_csv_from_gdrive(X_train_url)\n    y_train_full = load_csv_from_gdrive(y_train_url)\n\n    if X_train_full is not None and y_train_full is not None:\n        df_full = X_train_full.copy()\n        df_full['prdtypecode'] = y_train_full['prdtypecode']\n        print(f\"Total data loaded: {len(df_full):,} samples\")\n        print(f\"Classes: {df_full['prdtypecode'].nunique()}\")\n    else:\n        raise ValueError(\"Failed to load DataFrames\")\nexcept Exception as e:\n    print(f\"CSV download failed: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0818Ge4vUjTX"
   },
   "outputs": [],
   "source": [
    "# @title Download Images\n",
    "IMAGE_FILE_ID = \"15ZkS0iTQ7j3mHpxil4mABlXwP-jAN_zi\"\n",
    "\n",
    "if not os.path.exists(\"/content/images\"):\n",
    "    print(\"\\nDownloading images...\")\n",
    "    os.makedirs(\"/content/tmp\", exist_ok=True)\n",
    "    os.makedirs(\"/content/images\", exist_ok=True)\n",
    "    !gdown --id $IMAGE_FILE_ID -O /content/tmp/images.zip\n",
    "\n",
    "    print(\"Unzipping images...\")\n",
    "    !unzip -q -o /content/tmp/images.zip -d /content/images\n",
    "    print(\"Images unzipped\")\n",
    "else:\n",
    "    print(\"\\nImages already exist, skipping download\")\n",
    "\n",
    "IMG_ROOT = \"/content/images/images/image_train\"\n",
    "print(f\"Image Root: {IMG_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# @title Load Canonical Splits\nimport sys\nfrom pathlib import Path\n\ndef find_repo_root(start: Path) -> Path:\n    start = start.resolve()\n    for p in [start, *start.parents]:\n        if (p / \"src\").exists() and (p / \"data\").exists():\n            return p\n    raise RuntimeError(\"Repo root not found. Ensure the DS_rakuten repo is present with 'src/' and 'data/'.\")\n\nrepo_root = find_repo_root(Path.cwd())\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nfrom src.data.split_manager import load_splits, split_signature\n\nsplits = load_splits(verbose=True)\nsig = split_signature(splits)\n\nprint(\"\\nCanonical split sizes:\")\nprint(f\"  Train: {len(splits['train_idx']):,}\")\nprint(f\"  Val:   {len(splits['val_idx']):,}\")\nprint(f\"  Test:  {len(splits['test_idx']):,}\")\nprint(f\"  Total: {len(splits['train_idx']) + len(splits['val_idx']) + len(splits['test_idx']):,}\")\nprint(f\"\\nSplit signature: {sig}\")\nprint(f\"Repo root: {repo_root}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6V8noNsmUjTX"
   },
   "source": "# @title Label Encoding\nprint(\"=\"*80)\nprint(\"LABEL ENCODING (FIT ON DEV ONLY)\")\nprint(\"=\"*80)\n\ndev_idx = np.concatenate([splits[\"train_idx\"], splits[\"val_idx\"]])\n\nle = LabelEncoder()\nle.fit(df_full.iloc[dev_idx][\"prdtypecode\"])\n\nNUM_CLASSES = len(le.classes_)\nprint(f\"LabelEncoder fitted on DEV only (train+val).\")\nprint(f\"Number of classes: {NUM_CLASSES}\")\nassert NUM_CLASSES == 27, f\"Expected 27 classes, got {NUM_CLASSES}\"\n\n# Fingerprint of class order for alignment (important for fusion)\nimport hashlib, json\nclasses_fingerprint = hashlib.sha256(json.dumps(le.classes_.tolist()).encode(\"utf-8\")).hexdigest()[:16]\nprint(f\"Classes fingerprint: {classes_fingerprint}\")\n\nprint(\"=\"*80)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2ruHCFvUjTY"
   },
   "source": "# @title Create Train/Val/Test Splits from Canonical Indices\nprint(\"=\"*80)\nprint(\"CREATING TRAIN/VAL/TEST SPLITS FROM CANONICAL INDICES\")\nprint(\"=\"*80)\n\n# Create splits using canonical indices (no train_test_split!)\ndf_train = df_full.iloc[splits[\"train_idx\"]].copy()\ndf_val   = df_full.iloc[splits[\"val_idx\"]].copy()\ndf_test  = df_full.iloc[splits[\"test_idx\"]].copy()\n\n# Encode labels per split\ndf_train[\"encoded_label\"] = le.transform(df_train[\"prdtypecode\"])\ndf_val[\"encoded_label\"]   = le.transform(df_val[\"prdtypecode\"])\ndf_test[\"encoded_label\"]  = le.transform(df_test[\"prdtypecode\"])\n\n# Compute sizes and percentages\ntotal_samples = len(df_train) + len(df_val) + len(df_test)\npct_train = 100 * len(df_train) / total_samples\npct_val   = 100 * len(df_val) / total_samples\npct_test  = 100 * len(df_test) / total_samples\n\nprint(f\"Training:   {len(df_train):6,} samples ({pct_train:.1f}%)\")\nprint(f\"Validation: {len(df_val):6,} samples ({pct_val:.1f}%)\")\nprint(f\"Test:       {len(df_test):6,} samples ({pct_test:.1f}%)\")\nprint(f\"Total:      {total_samples:6,}\")\nassert total_samples == 84916, f\"Expected 84916 total samples, got {total_samples}\"\n\nprint(f\"\\nSplit signature: {sig}\")\nprint(f\"Classes fingerprint: {classes_fingerprint}\")\n\nprint(\"\\nModel selection will use Train/Val ONLY\")\nprint(\"Test set will be evaluated at the END\")\nprint(\"=\"*80)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54bCILcbUjTY"
   },
   "source": [
    "## 5. Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5p1KeUcXUjTY"
   },
   "outputs": [],
   "source": [
    "# @title Dataset Class (Optimized for Performance)\n",
    "class RakutenImageDataset(Dataset):\n",
    "    def __init__(self, df, img_root, transform=None):\n",
    "        # ‚úÖ Pre-convert to lists for faster access (avoid .iloc performance issue)\n",
    "        self.image_ids = df['imageid'].tolist()\n",
    "        self.product_ids = df['productid'].tolist()\n",
    "        self.labels = df['encoded_label'].tolist()\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"‚úì Dataset initialized with {len(self.labels):,} samples (optimized)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ‚úÖ Direct list access\n",
    "        imageid = self.image_ids[idx]\n",
    "        productid = self.product_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Image Processing\n",
    "        img_name = f\"image_{imageid}_product_{productid}.jpg\"\n",
    "        img_path = os.path.join(self.img_root, img_name)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, OSError):\n",
    "            # Fallback for missing/corrupt images\n",
    "            image = Image.new('RGB', (384, 384), (0, 0, 0))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "print(f\"Dataset class ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ci_85LQ8UjTY"
   },
   "source": [
    "## 6. Model Definition - ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiYYYkJ9UjTY"
   },
   "outputs": [],
   "source": [
    "# @title ConvNeXt Model\n",
    "class RakutenConvNeXt(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNeXt for Rakuten product classification.\n",
    "\n",
    "    Anti-Overfitting Features:\n",
    "    - Stochastic Depth (drop_path_rate)\n",
    "    - LayerNorm + Dropout in classification head\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'convnext_base',\n",
    "        num_classes: int = 27,\n",
    "        pretrained: bool = True,\n",
    "        drop_path_rate: float = 0.3\n",
    "    ):\n",
    "        super(RakutenConvNeXt, self).__init__()\n",
    "\n",
    "        # Load ConvNeXt backbone with Stochastic Depth\n",
    "        # Note: ConvNeXt doesn't accept img_size parameter in timm\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,  # Remove default head\n",
    "            global_pool='avg',\n",
    "            drop_path_rate=drop_path_rate  # Stochastic Depth\n",
    "        )\n",
    "\n",
    "        feature_dim = self.backbone.num_features\n",
    "\n",
    "        # Classification head with LayerNorm + Dropout\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.model_name = model_name\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "\n",
    "        print(f\"‚úì RakutenConvNeXt initialized:\")\n",
    "        print(f\"  - Model: {model_name}\")\n",
    "        print(f\"  - Drop Path Rate: {drop_path_rate}\")\n",
    "        print(f\"  - Head: {feature_dim} ‚Üí 512 ‚Üí {num_classes}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.backbone(x)\n",
    "        logits = self.head(features)\n",
    "        return logits\n",
    "\n",
    "print(\"‚úì RakutenConvNeXt class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt6A8aG2UjTY"
   },
   "source": [
    "## 7. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc7q9NEKUjTY"
   },
   "outputs": [],
   "source": [
    "# @title Training Configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_name\": \"convnext_base\",  # Using ConvNeXt Base\n",
    "    \"img_size\": 384,  # Higher resolution for better performance\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "\n",
    "    # Training - Optimized for A100 Colab Pro\n",
    "    \"batch_size\": 64,  # Adjusted for 384x384 on A100\n",
    "    \"num_epochs\": 30,\n",
    "    \"learning_rate\": 1e-4,  # ConvNeXt works well with higher LR\n",
    "    \"weight_decay\": 0.05,\n",
    "\n",
    "    # Anti-Overfitting\n",
    "    \"drop_path_rate\": 0.3,\n",
    "    \"mixup_alpha\": 0.8,\n",
    "    \"cutmix_alpha\": 1.0,\n",
    "    \"label_smoothing\": 0.1,\n",
    "    \"use_ema\": True,  # ‚úÖ EMA enabled\n",
    "    \"ema_decay\": 0.9999,  # EMA decay rate\n",
    "\n",
    "    # Other\n",
    "    \"early_stopping_patience\": 5,\n",
    "    \"use_amp\": True,\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONVNEXT TRAINING CONFIGURATION (Colab A100)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Image Size: {CONFIG['img_size']}x{CONFIG['img_size']} (higher resolution)\")\n",
    "print(f\"Batch Size: {CONFIG['batch_size']} (optimized for A100 + 384x384)\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Learning Rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Weight Decay: {CONFIG['weight_decay']}\")\n",
    "print(f\"\\nAnti-Overfitting:\")\n",
    "print(f\"  - Drop Path: {CONFIG['drop_path_rate']}\")\n",
    "print(f\"  - Mixup Alpha: {CONFIG['mixup_alpha']}\")\n",
    "print(f\"  - CutMix Alpha: {CONFIG['cutmix_alpha']}\")\n",
    "print(f\"  - Label Smoothing: {CONFIG['label_smoothing']}\")\n",
    "print(f\"  - EMA: {CONFIG['use_ema']} (decay={CONFIG['ema_decay']})\")\n",
    "print(f\"\\nAMP: {CONFIG['use_amp']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPAEQB0RUjTZ"
   },
   "source": [
    "## 8. Data Transforms & Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHEGlqNiUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Data Transforms",
    "train_transform = transforms.Compose([",
    "    transforms.RandomResizedCrop(384, scale=(0.8, 1.0)),",
    "    transforms.RandomHorizontalFlip(p=0.5),",
    "    transforms.RandAugment(num_ops=2, magnitude=9),",
    "    transforms.ToTensor(),",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])",
    "])",
    "",
    "val_transform = transforms.Compose([",
    "    transforms.Resize(438),",
    "    transforms.CenterCrop(384),",
    "    transforms.ToTensor(),",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])",
    "])",
    "",
    "# Create Datasets",
    "train_dataset = RakutenImageDataset(train_df, IMG_ROOT, transform=train_transform)",
    "val_dataset = RakutenImageDataset(val_df, IMG_ROOT, transform=val_transform)",
    "holdout_dataset = RakutenImageDataset(df_test, IMG_ROOT, transform=val_transform)",
    "",
    "# Create DataLoaders",
    "train_loader = DataLoader(",
    "    train_dataset,",
    "    batch_size=CONFIG[\"batch_size\"],",
    "    shuffle=True,",
    "    num_workers=CONFIG[\"num_workers\"],",
    "    pin_memory=True,",
    "    drop_last=True",
    ")",
    "",
    "val_loader = DataLoader(",
    "    val_dataset,",
    "    batch_size=CONFIG[\"batch_size\"],",
    "    shuffle=False,",
    "    num_workers=CONFIG[\"num_workers\"],",
    "    pin_memory=True",
    ")",
    "",
    "holdout_loader = DataLoader(",
    "    holdout_dataset,",
    "    batch_size=CONFIG[\"batch_size\"],",
    "    shuffle=False,",
    "    num_workers=CONFIG[\"num_workers\"],",
    "    pin_memory=True",
    ")",
    "",
    "print(f\"‚úì Train batches: {len(train_loader):,}\")",
    "print(f\"‚úì Val batches: {len(val_loader):,}\")",
    "print(f\"‚úì Holdout batches: {len(holdout_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaHdnX9cUjTZ"
   },
   "source": [
    "## 9. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3urnABxUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Initialize Model\n",
    "model = RakutenConvNeXt(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    num_classes=NUM_CLASSES,\n",
    "    pretrained=True,\n",
    "    drop_path_rate=CONFIG[\"drop_path_rate\"]\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize EMA\n",
    "model_ema = None\n",
    "if CONFIG[\"use_ema\"]:\n",
    "    model_ema = ModelEmaV2(model, decay=CONFIG[\"ema_decay\"])\n",
    "    print(f\"‚úì EMA initialized with decay={CONFIG['ema_decay']}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoOYRzf4UjTZ"
   },
   "source": [
    "## 10. Training Setup (Mixup/CutMix + Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4LyoDQFUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Mixup/CutMix & Optimizer Setup\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "\n",
    "# Initialize Mixup/CutMix\n",
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=CONFIG[\"mixup_alpha\"],\n",
    "    cutmix_alpha=CONFIG[\"cutmix_alpha\"],\n",
    "    cutmix_minmax=None,\n",
    "    prob=1.0,  # Apply to all batches\n",
    "    switch_prob=0.5,  # 50% Mixup, 50% CutMix\n",
    "    mode='batch',\n",
    "    label_smoothing=CONFIG[\"label_smoothing\"],\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Loss functions\n",
    "criterion_train = SoftTargetCrossEntropy()  # For Mixup (soft labels)\n",
    "criterion_val = nn.CrossEntropyLoss()       # For validation (hard labels)\n",
    "\n",
    "print(\"‚úì Mixup & CutMix initialized\")\n",
    "print(f\"  Mixup alpha: {CONFIG['mixup_alpha']}\")\n",
    "print(f\"  CutMix alpha: {CONFIG['cutmix_alpha']}\")\n",
    "\n",
    "# Optimizer - AdamW with LayerNorm-aware weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Scheduler - Cosine Annealing\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=CONFIG[\"num_epochs\"],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# AMP Scaler\n",
    "scaler = GradScaler() if CONFIG[\"use_amp\"] else None\n",
    "\n",
    "print(\"‚úì Optimizer: AdamW with Cosine Annealing\")\n",
    "print(f\"‚úì AMP: {CONFIG['use_amp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8qx6k88UjTZ"
   },
   "source": [
    "## 11. Training Loop with WandB & EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ym1hvlSAUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Training Loop with EMA",
    "best_val_acc = 0.0",
    "best_val_f1 = 0.0",
    "best_ema_acc = 0.0",
    "best_ema_f1 = 0.0",
    "patience_counter = 0",
    "history = {",
    "    \"train_loss\": [],",
    "    \"val_loss\": [],",
    "    \"val_acc\": [],",
    "    \"val_f1\": [],",
    "    \"ema_val_acc\": [],",
    "    \"ema_val_f1\": []",
    "}",
    "",
    "def evaluate(model, loader, criterion):",
    "    model.eval()",
    "    val_loss = 0.0",
    "    all_preds = []",
    "    all_labels = []",
    "",
    "    with torch.no_grad():",
    "        for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):",
    "            images = images.to(device)",
    "            labels = labels.to(device)",
    "",
    "            if CONFIG[\"use_amp\"]:",
    "                with torch.amp.autocast(device_type=\"cuda\"):",
    "                    outputs = model(images)",
    "                    loss = criterion(outputs, labels)",
    "            else:",
    "                outputs = model(images)",
    "                loss = criterion(outputs, labels)",
    "",
    "            val_loss += loss.item()",
    "            predictions = torch.argmax(outputs, dim=-1)",
    "",
    "            all_preds.extend(predictions.cpu().numpy())",
    "            all_labels.extend(labels.cpu().numpy())",
    "",
    "    avg_loss = val_loss / len(loader)",
    "    acc = accuracy_score(all_labels, all_preds)",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')",
    "",
    "    return avg_loss, acc * 100, f1",
    "",
    "print(\"=\"*80)",
    "print(\"üöÄ STARTING CONVNEXT TRAINING WITH EMA\")",
    "print(\"=\"*80)",
    "",
    "for epoch in range(CONFIG[\"num_epochs\"]):",
    "    print(f\"\\nEpoch {epoch + 1}/{CONFIG['num_epochs']}\")",
    "    print(\"=\"*80)",
    "",
    "    # ========================================================================",
    "    # TRAINING with Mixup/CutMix",
    "    # ========================================================================",
    "    model.train()",
    "    train_loss = 0.0",
    "",
    "    train_pbar = tqdm(train_loader, desc=\"Training\")",
    "    for images, labels in train_pbar:",
    "        images, labels = images.to(device), labels.to(device)",
    "",
    "        # Apply Mixup/CutMix",
    "        images, labels = mixup_fn(images, labels)",
    "",
    "        optimizer.zero_grad()",
    "",
    "        if CONFIG[\"use_amp\"]:",
    "            with torch.amp.autocast(device_type=\"cuda\"):",
    "                outputs = model(images)",
    "                loss = criterion_train(outputs, labels)",
    "",
    "            scaler.scale(loss).backward()",
    "            scaler.unscale_(optimizer)",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)",
    "            scaler.step(optimizer)",
    "            scaler.update()",
    "        else:",
    "            outputs = model(images)",
    "            loss = criterion_train(outputs, labels)",
    "            loss.backward()",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)",
    "            optimizer.step()",
    "",
    "        # Update EMA",
    "        if model_ema is not None:",
    "            model_ema.update(model)",
    "",
    "        train_loss += loss.item()",
    "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})",
    "",
    "    avg_train_loss = train_loss / len(train_loader)",
    "",
    "    # ========================================================================",
    "    # VALIDATION (no Mixup) - Regular Model",
    "    # ========================================================================",
    "    val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion_val)",
    "",
    "    # ========================================================================",
    "    # VALIDATION - EMA Model",
    "    # ========================================================================",
    "    ema_val_acc, ema_val_f1 = 0.0, 0.0",
    "    if model_ema is not None:",
    "        _, ema_val_acc, ema_val_f1 = evaluate(model_ema.module, val_loader, criterion_val)",
    "",
    "    # ========================================================================",
    "    # LOGGING & CHECKPOINTING",
    "    # ========================================================================",
    "    history[\"train_loss\"].append(avg_train_loss)",
    "    history[\"val_loss\"].append(val_loss)",
    "    history[\"val_acc\"].append(val_acc)",
    "    history[\"val_f1\"].append(val_f1)",
    "    history[\"ema_val_acc\"].append(ema_val_acc)",
    "    history[\"ema_val_f1\"].append(ema_val_f1)",
    "",
    "",
    "    print(f\"\\nüìä Results:\")",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")",
    "    print(f\"  Val (Regular): Acc={val_acc:.2f}%, F1={val_f1:.4f}\")",
    "    if model_ema is not None:",
    "        print(f\"  Val (EMA):     Acc={ema_val_acc:.2f}%, F1={ema_val_f1:.4f}\")",
    "",
    "    # Save best model (use EMA if better)",
    "    current_best_acc = max(val_acc, ema_val_acc)",
    "    if current_best_acc > best_val_acc:",
    "        best_val_acc = current_best_acc",
    "        patience_counter = 0",
    "",
    "        # Save the better model",
    "        if ema_val_acc > val_acc and model_ema is not None:",
    "            torch.save({",
    "                'epoch': epoch + 1,",
    "                'model_state_dict': model_ema.module.state_dict(),",
    "                'val_acc': ema_val_acc,",
    "                'val_f1': ema_val_f1,",
    "                'is_ema': True",
    "            }, \"convnext_best.pth\")",
    "            print(f\"  ‚úÖ Best EMA model saved! (Acc: {ema_val_acc:.2f}%, F1: {ema_val_f1:.4f})\")",
    "        else:",
    "            torch.save({",
    "                'epoch': epoch + 1,",
    "                'model_state_dict': model.state_dict(),",
    "                'val_acc': val_acc,",
    "                'val_f1': val_f1,",
    "                'is_ema': False",
    "            }, \"convnext_best.pth\")",
    "            print(f\"  ‚úÖ Best model saved! (Acc: {val_acc:.2f}%, F1: {val_f1:.4f})\")",
    "    else:",
    "        patience_counter += 1",
    "        print(f\"  ‚è≥ No improvement ({patience_counter}/{CONFIG['early_stopping_patience']})\")",
    "",
    "    scheduler.step()",
    "",
    "    if patience_counter >= CONFIG[\"early_stopping_patience\"]:",
    "        print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch + 1} epochs\")",
    "        break",
    "",
    "print(\"\\n\" + \"=\"*80)",
    "print(\"üéâ TRAINING COMPLETE\")",
    "print(\"=\"*80)",
    "print(f\"Best Val Acc: {best_val_acc:.2f}%\")",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp0JnbT6UjTZ"
   },
   "source": [
    "## 12. Final Evaluation on Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAqtolGYUjTZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tEAtUXsUjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Detailed Classification Report\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(holdout_loader, desc=\"Final Prediction\"):\n",
    "        images = images.to(device)\n",
    "\n",
    "        if CONFIG[\"use_amp\"]:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"\\nClassification Report (Holdout):\")\n",
    "print(classification_report(all_labels, all_preds, digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJHY_kvbUjTZ"
   },
   "source": [
    "## 13. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7FK8Up6UjTZ"
   },
   "outputs": [],
   "source": [
    "# @title Training Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history[\"train_loss\"], label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0].plot(history[\"val_loss\"], label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy - Regular vs EMA\n",
    "axes[1].plot(history[\"val_acc\"], label='Val Acc (Regular)', marker='o', linewidth=2)\n",
    "if CONFIG[\"use_ema\"]:\n",
    "    axes[1].plot(history[\"ema_val_acc\"], label='Val Acc (EMA)', marker='s', linewidth=2, linestyle='--')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Validation Accuracy (Regular vs EMA)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('convnext_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved: convnext_training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyBw3KHuUjTa"
   },
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70ouzWRiUjTa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDWH5B5iUjTa"
   },
   "source": [
    "## 15. Save to Google Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sg1JnyyDUjTa"
   },
   "outputs": [],
   "source": [
    "# @title Save Model to Google Drive\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "target_dir = \"/content/drive/MyDrive/Rakuten_models\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "model_type = \"ema\" if is_ema else \"regular\"\n",
    "target_file = os.path.join(target_dir, f\"convnext_{model_type}_{timestamp}.pth\")\n",
    "shutil.copy(\"convnext_best.pth\", target_file)\n",
    "print(f\"‚úì Model saved to: {target_file}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}