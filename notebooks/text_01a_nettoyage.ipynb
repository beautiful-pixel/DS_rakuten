{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAkn7HTWr-bI"
   },
   "source": [
    "# I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "<h2 align=\"left\"> 1.1. Définition du problème"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "L'objectif du projet est de développer un modèle de classification de\n",
    "produits e-commerce pour la place de marché Rakuten, capable de prédire un code produit à partir de données multimodales :\n",
    "- texte(catégories et description produits),\n",
    "- images associées.\n",
    "- \n",
    "Cette classification est utile pour :\n",
    "- exploiter efficacement le contenu fourni par les vendeurs,\n",
    "- éviter les doublons,\n",
    "- faciliter la gestion du catalogue,\n",
    "- améliorer la recommandation et la personnalisation du parcours\n",
    "utilisateur.\n",
    "\n",
    "Ce projet s'inscrit dans le cadre du challenge :\n",
    "**https://challengedata.ens.fr/challenges/35**\n",
    "\n",
    "Dans ce document, nous chercherons à:\n",
    "- comprendre le jeu de données,\n",
    "- nettoyer le texte,\n",
    "- analyser les differentes catégories,\n",
    "- détecter les doublons et biais,\n",
    "- créer des features et vérifier si elles sont pertinentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "<h2 align=\"left\"> 1.2. Description du jeu de données et structure du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<strong>Structure des données source</strong>\n",
    "\n",
    "<strong>Caractéristiques principales :</strong>\n",
    "<ul>\n",
    "  <li>Données multimodales : texte + image</li>\n",
    "  <li>Données bruitées nécessitant un prétraitement approfondi</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Composition du jeu de données d'origine :</strong>\n",
    "\n",
    "<strong>X_train_update.csv</strong> - Données d'entraînement (84 916 lignes)\n",
    "<ul>\n",
    "  <li><code>designation</code> : titre du produit</li>\n",
    "  <li><code>description</code> : description détaillée du produit</li>\n",
    "  <li><code>productid</code> : identifiant unique du produit</li>\n",
    "  <li><code>imageid</code> : identifiant de l'image associée</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Y_train_CVw08PX.csv</strong> - Labels d'entraînement\n",
    "<ul>\n",
    "  <li><code>prdtypecode</code> : code de catégorie du produit</li>\n",
    "</ul>\n",
    "\n",
    "<strong>X_test_update.csv</strong> - Données de test (13 812 produits)\n",
    "<ul>\n",
    "  <li>Même structure que X_train.csv</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Images</strong> - Fichiers visuels associés\n",
    "<ul>\n",
    "  <li>Dossier <code>images/image_train</code> : images pour l'entraînement</li>\n",
    "  <li>Dossier <code>images/image_test</code> : images pour le test</li>\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "    \n",
    "\n",
    "\n",
    "<div style=\"background:#fff3cd; padding:12px; border-left:6px solid #ffdd57; border-radius:4px\">\n",
    "*Ce notebook est <b>exclusivement dédié au traitement du texte</b>.  \n",
    "L’analyse des images sera réalisée séparément dans un autre notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JpUf8zUPtMQ"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\"> <h2 align=\"left\">1.3 Chargement des données et observation générale du DataFrame</h2> </div> <div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "La première étape consiste à préparer l'environnement Python du notebook, explorer et nettoyer les données textuelles du challenge <b>Rakuten France Product Data Classification</b>.\n",
    "\n",
    "Notre objectif principal est de préparer un corpus textuel propre, cohérent et directement exploitable pour la modèlisation, mais aussi:\n",
    "\n",
    "- d' analyser la structure et la qualité des colonnes textuelles (<code>designation</code>, <code>description</code>) ;\n",
    "- d' identifier les valeurs manquantes et les incohérences ;\n",
    "- de nettoyer, normaliser et homogénéiser les textes ;\n",
    "- de comprendre la distribution lexicale globale et par catégorie ;\n",
    "- de construire un pré-traitement complet, reproductible et exploitable ;\n",
    "- de produire un dataset textuel propre, prêt pour la modélisation.\n",
    "\n",
    "\n",
    "\n",
    "Commençons par charger le dataset et effectuer un premier aperçu du DataFrame afin de mieux comprendre sa structure et son contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oiN4NFlCHIer"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/usuario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import regex as reg\n",
    "import unicodedata\n",
    "import os, re, json, html, base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from ftfy import fix_text\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UrP59F5O_BAa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84916 entries, 0 to 84915\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   designation  84916 non-null  object\n",
      " 1   description  55116 non-null  object\n",
      " 2   productid    84916 non-null  int64 \n",
      " 3   imageid      84916 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 2.6+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \n",
       "0                                                NaN  3804725264  1263597046  \n",
       "1                                                NaN   436067568  1008141237  \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978  \n",
       "3                                                NaN    50418756   457047496  \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"../data/raw/\"\n",
    "df = pd.read_csv(data_path + \"X_train_update.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "print(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "    \n",
    "Nous pouvons voir que seule la colonne <code>description</code> a des valeurs manquantes. Regardons le taux de NaN qu'a cette colonne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage de descriptions manquantes: 35.09%\n"
     ]
    }
   ],
   "source": [
    "no_description = df[\"description\"].isna().mean()\n",
    "print(f\"Pourcentage de descriptions manquantes: {no_description:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "35 % des produits ne disposent pas de description, ce qui représente une part importante du dataset. Dans le deuxième notebook, nous analyserons si certaines catégories sont davantage concernées que d’autres. Mais dans un premier temps, nous allons nous concentrer sur la qualité du texte de manière générale.\n",
    "\n",
    "Affichons la première entrée avec une description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PILOT STYLE Touch Pen de marque Speedlink est 1 stylet ergonomique pour GamePad Nintendo Wii U.<br> Pour un confort optimal et une précision maximale sur le GamePad de la Wii U: ce grand stylet hautement ergonomique est non seulement parfaitement adapté à votre main mais aussi très élégant.<br> Il est livré avec un support qui se fixe sans adhésif à l\\'arrière du GamePad<br> <br> Caractéristiques:<br> Modèle: Speedlink PILOT STYLE Touch Pen<br> Couleur: Bleu<br> Ref. Fabricant: SL-3468-BE<br> Compatibilité: GamePad Nintendo Wii U<br> Forme particulièrement ergonomique excellente tenue en main<br> Pointe à revêtement longue durée conçue pour ne pas abîmer l\\'écran tactile<br> En bonus : Support inclu pour GamePad<br> <span class=\"vga_style2\"><b></b><br>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2, 'description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "En observant les données, nous remarquons la présence de <strong>balises HTML</strong>, d’<strong>entités HTML</strong> (par exemple <code>\\&#39;</code>) ainsi que des <strong>caractères encodés</strong> sous différents formats.\n",
    "Avant de poursuivre l'exploration, il est donc nécessaire d’effectuer un <strong>nettoyage du texte</strong> afin d’obtenir des descriptions plus cohérentes et exploitables.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QOIIz0eBRXJ"
   },
   "source": [
    "# II. Premier nettoyage du texte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">Pour faciliter le nettoyage du texte, nous fusionnons designation et description dans une colonne text.\n",
    "Cela permet de traiter tout le contenu textuel d’un produit en une seule fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = (df[\"designation\"].fillna(\"\") + \" \" + df[\"description\"].fillna(\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">En parcourant les mots les plus fréquents dans le texte, on a constaté que beaucoup de termes sont des mots vides (comme « de », « et », « la ») ou des caractères isolés et symboles issus du HTML. Cette observation justifie l’étape de nettoyage, qui va standardiser le texte pour ensuite détecter plus précisément les doublons, créer des features textuelles fiables et assurer que l’association texte-image soit cohérente pour la suite du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "n755R-etUsIH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Top words BEFORE cleaning ===\n",
      "[('de', 372883), ('et', 145462), ('la', 136102), ('à', 111695), ('pour', 84528), ('-', 80819), (':', 79201), ('en', 78584), ('x', 69640), ('le', 69101), ('les', 66725), ('des', 51758), ('un', 46013), ('est', 44531), ('une', 41334), ('De', 39925), ('du', 39399), ('vous', 38690), ('avec', 38603), ('/>', 33990), ('1', 33813), ('votre', 31283), ('/', 30020), ('<br', 26820), ('dans', 25452), ('ou', 24868), ('sur', 24445), ('cm', 22995), ('pas', 21696), ('Le', 20464), ('La', 20364), ('plus', 20086), ('peut', 20084), ('2', 19762), ('que', 18200), ('au', 17353), ('haute', 15900), ('piscine', 15818), ('être', 15259), ('*', 15059)]\n",
      "['designation', 'description', 'productid', 'imageid', 'text']\n"
     ]
    }
   ],
   "source": [
    "top_n = 40\n",
    "\n",
    "def get_word_freq(series, top_n):\n",
    "    all_words = []\n",
    "    for text in series:\n",
    "        if isinstance(text, str):\n",
    "            all_words.extend(text.split())\n",
    "    return Counter(all_words).most_common(top_n)\n",
    "\n",
    "freq_before = get_word_freq(df[\"text\"],       top_n=top_n)\n",
    "\n",
    "print(\"=== Top words BEFORE cleaning ===\")\n",
    "print(freq_before)\n",
    "\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qB294x9wVFw1"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<h3 align=\"left\">3.1 Nettoyage structurel et standardisation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iUECfpNTgtu"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "Nous nettoyons le texte en supprimant les balises HTML, entités, caractères mal encodés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "301KZ3ROBc6s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- productid 36138 [texte original] ---\n",
      "Matelas Mémoire De Forme 180x200 X 20 Cm Très Ferme - Déhoussable Housse Lavable - 7 Zones De Confort - Noyau Poli Lattex Hr Dernière Génération - Très Respirant MATELAS:<br />Â· Accueil : Ferme .<br />Â· Soutien : Très Ferme .<br />Â· Technologie matelas : Face été &#43; à¢me en Mousse Poli Lattex Dernière Génération Indéformable Très Haute Résilience - Face Hiver 45 cm de Mousse à  Mémoire de Forme Très Haute Densité 60 Kg/m3 &#34; Massante&#34;<br />Â· Épaisseur du matelas : &#43;/- 20 cm.<br />Â· REPOS PLUS SAIN grà¢ce au Traitement Anti-acariens / anti-bactérien / Anti-moisissures.<br />Â· Très Bonne Indépendance de couchage.<br />Â· DORMEZ TRANQUILLE avec la Garantie 5 ans.  Il est Compatible avec les Sommiers Mécaniques et électriques<br />Coutil:<br />Â· Coutil stretch matelassé de 290 gr/m2 de Polyester avec traitement Sanitized. Faces de couchage Réversibles - Déhoussable sur 3 Cà¿tés et Housse Lavable à  30Â° position Lavage à  la main Délicat -<br />Â· SANITIZED : Traitement antibactérien et anti-moisissures diminuant également le développement des odeurs.<br />Structure et garnissage du matelas:<br />Â· Faces Dormeur : 15 cm de mousse profilée à  Haute Ventilation pour Maximiser le Confort &#43; coutil en tissu 3D pour la bonne Respiration du noyau matelassé Air System.<br />Â· Tissu Hyper Respirant réalisé en 3 dimensions qui permet une circulation libre de l&#39;air offrant ainsi une plus grande ventilation et dâ¿¿hygiène.<br />Â· Plate-bande: Tissu Hyper RESPIRANT matelassé avec de la ouate polyester <br />Â· à¿me noyau : bloc de 15 cm de Mousse HAUTE RESILIENCE Poli Lattex dernière génération Indéformable<br />Â·  La structure du matelas est divisée en 7 incroyables zones de confort qui s&#39;adaptent à  tous types de morphologies et aident à  libérer les tensions musculaires pour soulager les maux de dos provoqués par de mauvaises postures au cours de la journée.<br />Â· TISSUS CERTIFIES : Système de Qualité Certifié Oeko-tex 100  Certipur testé contre les substances nocives. Certification ISO 9001\n",
      "\n",
      "========================================\n",
      "\n",
      "--- productid 36138 [texte nettoyé] ---\n",
      "matelas mémoire de forme 180x200 x 20 cm très ferme déhoussable housse lavable 7 zones de confort noyau poli lattex hr dernière génération très respirant matelas: accueil ferme soutien très ferme technologie matelas face été à¢me en mousse poli lattex dernière génération indéformable très haute résilience face hiver 45 cm de mousse à mémoire de forme très haute densité 60 kg/m3 \" massante\" épaisseur du matelas +/- 20 cm repos plus sain grà¢ce au traitement anti-acariens anti-bactérien anti-moisissures très bonne indépendance de couchage dormez tranquille avec la garantie 5 ans il est compatible avec les sommiers mécaniques et électriques coutil: coutil stretch matelassé de 290 gr/m2 de polyester avec traitement sanitized faces de couchage réversibles déhoussable sur 3 cà¿tés et housse lavable à 30° position lavage à la main délicat sanitized traitement antibactérien et anti-moisissures diminuant également le développement des odeurs structure et garnissage du matelas: faces dormeur 15 cm de mousse profilée à haute ventilation pour maximiser le confort coutil en tissu 3d pour la bonne respiration du noyau matelassé air system tissu hyper respirant réalisé en 3 dimensions qui permet une circulation libre de l'air offrant ainsi une plus grande ventilation et dâ¿¿hygiène plate-bande: tissu hyper respirant matelassé avec de la ouate polyester à¿me noyau bloc de 15 cm de mousse haute resilience poli lattex dernière génération indéformable la structure du matelas est divisée en 7 incroyables zones de confort qui s'adaptent à tous types de morphologies et aident à libérer les tensions musculaires pour soulager les maux de dos provoqués par de mauvaises postures au cours de la journée tissus certifies système de qualité certifié oeko-tex 100 certipur testé contre les substances nocives certification iso 9001\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour nettoyer et standardiser les données textuelles:\n",
    "def nettoyer_texte(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    s = reg.sub(r\"<[^>]+>\", \" \", s)          # Supprime HTML\n",
    "    s = html.unescape(s)                     # Décode entités HTML\n",
    "    s = fix_text(s)                     # Corrige le texte cassé\n",
    "    s = unicodedata.normalize(\"NFC\", s)      # Normalise Unicode\n",
    "    s = reg.sub(r\"(?<!\\d)\\.(?!\\d)\", \" \", s)  # Supprime les points non numériques\n",
    "    s = reg.sub(r\"(?<!\\S)-(?!\\S)\", \" \", s)   # Supprime les tirets isolés\n",
    "    s = reg.sub(r\"(?<!\\S):(?!\\S)\", \" \", s)   # Supprime les deux-points isolés\n",
    "    s = reg.sub(r\"(?<!\\S)·(?!\\S)\", \" \", s)   # Supprime les points médians isolés\n",
    "    s = reg.sub(r\"(?<!\\S)/(?!\\S)\", \" \", s)   # Supprime le slash isolé\n",
    "    s = reg.sub(r\"(?<!\\S)\\+(?!\\S)\", \" \", s)  # Supprime le plus isolé\n",
    "    s = s.replace(\"////\", \" \")\n",
    "    s = reg.sub(r\"\\s+\", \" \", s).strip().lower()     # Nettoie espaces et casses\n",
    "    return s\n",
    "\n",
    "# Le texte de ce produit rencontre les problèmes décrits nous allons donc tester la fonction de nettoyage ci-dessus\n",
    "productid = 36138\n",
    "description = df.iloc[14]['text']\n",
    "\n",
    "print(f\"--- productid {productid} [texte original] ---\")\n",
    "print(description)\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "print(f\"--- productid {productid} [texte nettoyé] ---\")\n",
    "print(nettoyer_texte(description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "Cet example montre que le nettoyage a bien fonctionné."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peu-QyfyYJZn"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "Ensuite, la fonction global_text_cleaner reprend ce texte déjà nettoyé et applique des étapes supplémentaires : suppression des phrases récurrentes (boilerplate), retrait des mots vides (stopwords), et filtrage des caractères isolés ou ponctuations inutiles.\n",
    "Cette approche en deux temps garantit que le texte est à la fois lisible et prêt pour l’analyse ou la préparation du vocabulaire.\n",
    "Ainsi, même si les balises ont déjà été supprimées, leur nettoyage reste la fondation sur laquelle les traitements plus fins s’appuient.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "25-Pf-kBYNIe"
   },
   "outputs": [],
   "source": [
    "def global_text_cleaner(\n",
    "    text,\n",
    "    use_basic_cleaning: bool = True,\n",
    "    normalize_x_dimensions: bool = True,\n",
    "    remove_boilerplate: bool = True,\n",
    "    remove_nltk_stops: bool = True,\n",
    "    remove_custom_stops: bool = True,\n",
    "    remove_single_digit: bool = True,\n",
    "    remove_single_letter: bool = True\n",
    "):\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "\n",
    "    # ---- Étape 1 : Nettoyage de base (HTML, Regex, normalisation) ----\n",
    "    s = str(text)\n",
    "\n",
    "    if normalize_x_dimensions:\n",
    "        s = merge_x_dimensions(s)\n",
    "\n",
    "    if use_basic_cleaning:\n",
    "        s = reg.sub(r\"<[^>]+>\", \" \", s)          # Remove HTML\n",
    "        s = html.unescape(s)                     # Decode HTML entities\n",
    "        s = fix_text(s)                          # Fix broken text\n",
    "        s = unicodedata.normalize(\"NFC\", s)      # Normalize Unicode\n",
    "\n",
    "        # Keep decimal points in numbers, remove others\n",
    "        s = reg.sub(r\"(?<!\\d)\\.(?!\\d)\", \" \", s)\n",
    "\n",
    "        # Remove isolated hyphens / colons / middle dots / slashes / plus signs\n",
    "        s = reg.sub(r\"(?<!\\S)-(?!\\S)\", \" \", s)\n",
    "        s = reg.sub(r\"(?<!\\S):(?!\\S)\", \" \", s)\n",
    "        s = reg.sub(r\"(?<!\\S)·(?!\\S)\", \" \", s)\n",
    "        s = reg.sub(r\"(?<!\\S)/(?!\\S)\", \" \", s)\n",
    "        s = reg.sub(r\"(?<!\\S)\\+(?!\\S)\", \" \", s)\n",
    "        s = s.replace(\"////\", \" \")\n",
    "\n",
    "        # Lowercase at the end of basic cleaning\n",
    "        s = s.lower()\n",
    "\n",
    "    # ---- Étape 2 : Suppression des phrases \"boilerplate\" ----\n",
    "    if remove_boilerplate and \"BOILERPLATE_PHRASES\" in globals():\n",
    "        for phrase in BOILERPLATE_PHRASES:\n",
    "            s = s.replace(phrase, \" \")\n",
    "\n",
    "    # ---- Étape 3 : Stopwords + single-letter/digit + punctuation ----\n",
    "    if remove_nltk_stops or remove_custom_stops or remove_single_digit or remove_single_letter:\n",
    "        tokens = s.split()\n",
    "\n",
    "        # Build the ban list\n",
    "        stops_to_exclude = set()\n",
    "        if remove_nltk_stops and \"NLTK_STOPS\" in globals():\n",
    "            stops_to_exclude.update(NLTK_STOPS)\n",
    "        if remove_custom_stops and \"MY_STOPWORDS\" in globals():\n",
    "            stops_to_exclude.update(MY_STOPWORDS)\n",
    "\n",
    "        filtered = []\n",
    "        for w in tokens:\n",
    "            # skip if in stopword list\n",
    "            if w in stops_to_exclude:\n",
    "                continue\n",
    "            # skip pure punctuation tokens like \"*\", \"???\", \":\", \"(l\"\n",
    "            if is_pure_punctuation(w):\n",
    "                continue\n",
    "            # optional: skip single letters\n",
    "            if remove_single_letter and is_single_letter(w):\n",
    "                continue\n",
    "            # optional: skip single digits\n",
    "            if remove_single_digit and is_single_digit(w):\n",
    "                continue\n",
    "\n",
    "            filtered.append(w)\n",
    "\n",
    "        s = \" \".join(filtered)\n",
    "\n",
    "    # ---- Étape finale : nettoyer les espaces multiples ----\n",
    "    s = reg.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">Cette sous-section implémente un nettoyage caractère-par-caractère qui cible les artefacts résiduels les plus subtils. Les fonctions is_single_letter(), is_single_digit() et is_pure_punctuation() identifient respectivement les lettres isolées (\"a\", \"b\"), chiffres seuls (\"1\", \"2\"), et tokens composés exclusivement de ponctuation (\"...\", \"!!\"). Ces artefacts, souvent résidus de segmentations erronées, sont éliminés dans le pipeline principal via des conditions booléennes configurables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9rugbvkDYCfS"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "PUNCT_CHARS = set(string.punctuation) | {\"…\", \"’\", \"“\", \"”\", \"«\", \"»\"}\n",
    "\n",
    "def is_single_letter(token: str) -> bool:\n",
    "    return len(token) == 1 and token.isalpha()\n",
    "\n",
    "def is_single_digit(token: str) -> bool:\n",
    "    return len(token) == 1 and token.isdigit()\n",
    "\n",
    "def is_pure_punctuation(token: str) -> bool:\n",
    "    if not token:\n",
    "        return False\n",
    "    return all(ch in PUNCT_CHARS for ch in token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uxAcJ_7YGDR"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "<h3 align=\"left\">3.2 Raffinement lexical et filtrage du bruit</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsVr5rgZVc5t"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "La prochaine étape consiste à éliminer les stopwords, c’est-à-dire les mots vides du français et de l’anglais (via NLTK) ainsi que les termes trop fréquents ou peu informatifs propres au domaine (my_stopwords). Cela permet de concentrer l’analyse sur les mots réellement porteurs d’information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/usuario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "NLTK_STOPS = set(stopwords.words(\"french\")) | set(stopwords.words(\"english\"))\n",
    "\n",
    "def nltk_stopwords(text: str, stopwords_set=None):\n",
    "    if stopwords_set is None:\n",
    "        stopwords_set = set()\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    tokens = []\n",
    "    for w in text.split():\n",
    "        w = w.lower()\n",
    "        if w in stopwords_set:\n",
    "            continue\n",
    "        tokens.append(w)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_word_freq_with_nltk_stopwords(series, stopwords_set=None):\n",
    "    all_tokens = []\n",
    "    for text in series:\n",
    "        all_tokens.extend(nltk_stopwords(text, stopwords_set))\n",
    "    return Counter(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\"> Une fois ces mots vides retirés, il reste parfois du bruit résiduel : lettres isolées, chiffres seuls ou ponctuation inutile. Les fonctions utilitaires (is_single_letter, is_single_digit, is_pure_punctuation) interviennent alors pour nettoyer ces résidus, garantissant un texte plus standardisé et exploitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "PUNCT_CHARS = set(string.punctuation) | {\"…\", \"’\", \"“\", \"”\", \"«\", \"»\"}\n",
    "\n",
    "def is_single_letter(token: str) -> bool:\n",
    "    return len(token) == 1 and token.isalpha()\n",
    "\n",
    "def is_single_digit(token: str) -> bool:\n",
    "    return len(token) == 1 and token.isdigit()\n",
    "\n",
    "def is_pure_punctuation(token: str) -> bool:\n",
    "    if not token:\n",
    "        return False\n",
    "    return all(ch in PUNCT_CHARS for ch in token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uxAcJ_7YGDR"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "<h3 align=\"left\">3.3 Normalisation des dimensions physiques</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">Nous observons que certaines descriptions de produits contiennent des dimensions physiques dans des formats hétérogènes (\"22 x 11 x 2\", \"180 x 180\",\"L x H x L\"). Ces variations compliquent toute analyse quantitative ou catégorisation automatique, car le même type de dimension peut apparaître sous plusieurs formes textuelles. Pour résoudre ce problème, on applique une normalisation des dimensions: les chiffres et lettres sont regroupés avec le séparateur « x », produisant des formats uniformes (\"22x11x2\", \"180x180\"). Cette étape permet de standardiser l’information numérique et de réduire le bruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_x_dimensions(text):\n",
    "    \"\"\"\n",
    "    Normalize dimension patterns like:\n",
    "      - '22 x 11 x 2' -> '22x11x2'\n",
    "      - '180 x 180'   -> '180x180'\n",
    "      - 'L x H x L'   -> 'LxHxL'\n",
    "    \"\"\"\n",
    "    if text is None or (isinstance(text, float) and pd.isna(text)):\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "\n",
    "    # 1) numeric triplets: 22 x 11 x 2 → 22x11x2\n",
    "    s = re.sub(r\"\\b(\\d+)\\s*[xX]\\s*(\\d+)\\s*[xX]\\s*(\\d+)\\b\", r\"\\1x\\2x\\3\", s)\n",
    "\n",
    "    # 2) numeric pairs: 180 x 180 → 180x180\n",
    "    s = re.sub(r\"\\b(\\d+)\\s*[xX]\\s*(\\d+)\\b\", r\"\\1x\\2\", s)\n",
    "\n",
    "    # 3) letter triplets: L x H x L → LxHxL\n",
    "    s = re.sub(r\"\\b([LlHh])\\s*[xX]\\s*([LlHh])\\s*[xX]\\s*([LlHh])\\b\", r\"\\1x\\2x\\3\", s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnVrR5Y0VarE"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<h3 align=\"left\">3.4 Analyse des N-Grammes : Transition vers le Nettoyage Sémantique</h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1BnDddZpX7r"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">Cette section présente une analyse séquentielle du texte visant à identifier les combinaisons de mots récurrentes (bigrammes et trigrammes) qui structurent notre corpus. Nous utilisons une approche de comptage fréquentiel pour détecter non seulement les expressions génériques non discriminantes, mais aussi les formules métier caractéristiques. L'objectif est double : quantifier la présence des séquences phrastiques dominantes et établir une distinction claire entre le bruit linguistique à éliminer et les patterns sémantiquement pertinents à conserver.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "e9-tO_gJWYPk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 bigrams:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46789</th>\n",
       "      <td>de la</td>\n",
       "      <td>46986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91785</th>\n",
       "      <td>li li</td>\n",
       "      <td>45585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44974</th>\n",
       "      <td>de 39</td>\n",
       "      <td>24376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27214</th>\n",
       "      <td>br br</td>\n",
       "      <td>20373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64328</th>\n",
       "      <td>et de</td>\n",
       "      <td>15259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7019</th>\n",
       "      <td>39 eau</td>\n",
       "      <td>15114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46645</th>\n",
       "      <td>de haute</td>\n",
       "      <td>10851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>39 il</td>\n",
       "      <td>10497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120141</th>\n",
       "      <td>pour les</td>\n",
       "      <td>10314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76654</th>\n",
       "      <td>haute qualité</td>\n",
       "      <td>10153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ngram  count\n",
       "46789           de la  46986\n",
       "91785           li li  45585\n",
       "44974           de 39  24376\n",
       "27214           br br  20373\n",
       "64328           et de  15259\n",
       "7019           39 eau  15114\n",
       "46645        de haute  10851\n",
       "7332            39 il  10497\n",
       "120141       pour les  10314\n",
       "76654   haute qualité  10153"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 trigrams:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8247</th>\n",
       "      <td>39 il vous</td>\n",
       "      <td>9313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93278</th>\n",
       "      <td>il vous plaît</td>\n",
       "      <td>9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52169</th>\n",
       "      <td>de haute qualité</td>\n",
       "      <td>8965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71666</th>\n",
       "      <td>en raison de</td>\n",
       "      <td>6443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110693</th>\n",
       "      <td>li li strong</td>\n",
       "      <td>5371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104506</th>\n",
       "      <td>le forfait comprend</td>\n",
       "      <td>4889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>100 tout neuf</td>\n",
       "      <td>4636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169468</th>\n",
       "      <td>tout neuf et</td>\n",
       "      <td>4396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76916</th>\n",
       "      <td>et de haute</td>\n",
       "      <td>4233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148682</th>\n",
       "      <td>raison de la</td>\n",
       "      <td>4142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ngram  count\n",
       "8247             39 il vous   9313\n",
       "93278         il vous plaît   9270\n",
       "52169      de haute qualité   8965\n",
       "71666          en raison de   6443\n",
       "110693         li li strong   5371\n",
       "104506  le forfait comprend   4889\n",
       "821           100 tout neuf   4636\n",
       "169468         tout neuf et   4396\n",
       "76916           et de haute   4233\n",
       "148682         raison de la   4142"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_ngrams(corpus, ngram_range=(2,2), top_n=10):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, min_df=5)\n",
    "    X = vec.fit_transform(corpus)\n",
    "    freqs = zip(vec.get_feature_names_out(), X.sum(axis=0).tolist()[0])\n",
    "    df = pd.DataFrame(freqs, columns=[\"ngram\", \"count\"])\n",
    "    return df.sort_values(by=\"count\", ascending=False).head(top_n)\n",
    "\n",
    "corpus = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "print(\"Top 10 bigrams:\")\n",
    "display(get_top_ngrams(corpus, ngram_range=(2,2), top_n=10))\n",
    "\n",
    "print(\"\\nTop 10 trigrams:\")\n",
    "display(get_top_ngrams(corpus, ngram_range=(3,3), top_n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yi5zvfdDfjfP"
   },
   "source": [
    " <div style=\"background:#f0f8ff; padding:12px; border-radius:6px\"> Les résultats révèlent notamment la prédominance de formules commerciales standardisées telles que \"de haute qualité\" (10 153 occurrences) qui, bien que fréquentes, n'apportent aucune valeur discriminante pour la classification. Cette analyse diagnostique nous permet de construire des listes de filtrage ciblées (BOILERPLATE_PHRASES et MY_STOPWORDS) pour le nettoyage sémantique final, transitionnant ainsi du traitement structurel vers l'optimisation thématique du corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "evjtVM5MXXtv"
   },
   "outputs": [],
   "source": [
    "# boilerplate phrase list\n",
    "BOILERPLATE_PHRASES = [\"de haute qualite\", \"haute qualite\", \"il vous plait\", \"vous plait\", \"vous plait permettre\", \"peut etre\", \"peut etre legerement\", \"peut etre utilise\", \"etre legerement different\",\n",
    "    \"raison de la\", \"en raison\", \"en raison de\", \"raison de\", \"la couleur reelle\", \"couleur reelle de\", \"couleur reelle\", \"la couleur et\", \"la couleur de\", \"la mesure\", \"la mesure manuelle\",\n",
    "    \"de la mesure\", \"la difference\", \"de la difference\", \"la lumiere\", \"de la lumiere\", \"ne pas\", \"ne pas utiliser\", \"ne pas refleter\", \"ne pas reflecter\", \"ne sont pas\", \"pas refleter la\",\n",
    "        \"pas reflecter la\", \"plait permettre une\", \"la marque\", \"de la marque\", \"la main\", \"tout neuf\", \"100 tout neuf\", \"neuf et\", \"neuf et de\", \"tout neuf et\", \"forfait comprend\",\n",
    "    \"le forfait comprend\", \"le forfait\", \"contenu de emballage\", \"contenu du coffret\", \"le paquet contient\", \"de element peut\", \"element peut etre\", \"permettre une legere\", \"duree de vie\",\n",
    "    \"different des images\", \"en fonction de\",]\n",
    "\n",
    "def remove_boilerplate_phrases(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    out = s\n",
    "    for phrase in BOILERPLATE_PHRASES:\n",
    "        out = out.replace(phrase, \" \")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "2rpBGKERXnOI"
   },
   "outputs": [],
   "source": [
    "MY_STOPWORDS: set[str] = {\"qualite\", \"neuf\", \"nouveau\", \"s'il\", \"d'un\", \"d'une\", \"ect\", \"m?\", \"plus\", \"peut\", \"etre\", \"être\", \"comme\", \"cette\", \"tout\", \"tous\",\n",
    "    \"tres\", \"très\", \"si\", \"aussi\", \"encore\", \"peu\", \"egalement\", \"également\",\n",
    "    \"avant\", \"entre\", \"grace\", \"grâce\",\n",
    "    \"pour\", \"dans\", \"ce\", \"ces\", \"dont\", \"depuis\",\n",
    "    \"ainsi\", \"son\", \"leurs\", \"avec\",\"100%\",\"facile\"\n",
    "    \"vraiment\", \"simplement\", \"entierement\", \"completement\",\n",
    "    \"deja\", \"juste\",\"description:\", \"caractéristiques:\", \"caractéristiques\",\n",
    "    \"contenu\", \"type\", \"etc\",\"utiliser\", \"utilisé\", \"utilise\", \"utilisés\", \"utilises\",\n",
    "    \"fait\", \"faire\", \"permet\", \"permettre\", \"peuvent\",\n",
    "    \"parfait\", \"ideal\", \"idéal\", \"grand\", \"grande\",\n",
    "    \"different\", \"différent\", \"differents\", \"différents\",\n",
    "    \"durable\",\"facile\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">La création des listes BOILERPLATE_PHRASES (42 expressions phrastiques) et MY_STOPWORDS (45 termes métier) matérialise concrètement notre stratégie de nettoyage sémantique ciblé. Ces outils spécifiques complètent notre arsenal de prétraitement en s'attaquant au bruit commercial systémique qui résistait aux approches lexicales conventionnelles.\n",
    "Nous disposons désormais de l'ensemble des composants nécessaires pour implémenter le nettoyage final intégré, dernière étape avant la détection des doublons. Le chapitre suivant appliquera systématiquement ces filtres personnalisés dans un pipeline de nettoyage unifié."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QOIIz0eBRXJ"
   },
   "source": [
    "# III. Validation et Résultats du Pipeline de Nettoyage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SRgQYLDkDDx"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">Le nettoyage complet du corpus a été appliqué systématiquement à l'ensemble des données textuelles. Les résultats démontrent une transformation significative : la suppression des artefacts techniques et du bruit linguistique révèle désormais une distribution lexicale centrée sur des termes sémantiquement pertinents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "f3nAx5KwYU3p",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début du nettoyage complet du corpus...\n",
      "Nettoyage terminé.\n",
      "\n",
      "==================== COMPARAISON AVANT / APRÈS (Index 3804725264) ====================\n",
      "[ORIGINAL]:\n",
      "Olivia: Personalisiertes Notizbuch / 150 Seiten / Punktraster / Ca Din A5 / Rosen-Design \n",
      "\n",
      "[CLEANED]:\n",
      "olivia: personalisiertes notizbuch 150 seiten punktraster ca din a5 rosen-design\n",
      "\n",
      "==================== TOP 40 MOTS APRÈS NETTOYAGE COMPLET ====================\n",
      "cm              37189\n",
      "piscine         22512\n",
      "haute           18316\n",
      "qualité         16410\n",
      "couleur         15234\n",
      "taille          14137\n",
      "dimensions      13128\n",
      "enfants         12483\n",
      "sans            11006\n",
      "bois            10866\n",
      "taille:         10866\n",
      "jeu             10797\n",
      "matériel:       10519\n",
      "l'eau           10108\n",
      "acier           9999\n",
      "plaît           9666\n",
      "mm              9354\n",
      "coussin         9279\n",
      "lumière         8842\n",
      "produit         8594\n",
      "décoration      8354\n",
      "taie            8274\n",
      "led             7749\n",
      "bébé            7616\n",
      "blanc           7582\n",
      "sac             7335\n",
      "ans             7286\n",
      "protection      7216\n",
      "rc              7082\n",
      "hauteur         7037\n",
      "temps           6991\n",
      "pompe           6958\n",
      "design          6909\n",
      "mode            6905\n",
      "maison          6729\n",
      "batterie        6692\n",
      "non             6685\n",
      "couleur:        6628\n",
      "filtration      6535\n",
      "plastique       6358\n"
     ]
    }
   ],
   "source": [
    "# 1. Application du nettoyage complet sur tout le dataframe\n",
    "print(\"Début du nettoyage complet du corpus...\")\n",
    "df[\"text_cleaned\"] = df[\"text\"].apply(lambda x: global_text_cleaner(\n",
    "    x,\n",
    "    use_basic_cleaning=True,\n",
    "    remove_boilerplate=True,\n",
    "    remove_nltk_stops=True,\n",
    "    remove_custom_stops=True,\n",
    "    remove_single_digit=True,\n",
    "    remove_single_letter=True\n",
    "))\n",
    "print(\"Nettoyage terminé.\")\n",
    "\n",
    "# 2. Comparaison sur l'exemple de l'index 36138 (défini précédemment)\n",
    "idx_example = 3804725264\n",
    "print(f\"\\n{'='*20} COMPARAISON AVANT / APRÈS (Index {idx_example}) {'='*20}\")\n",
    "print(f\"[ORIGINAL]:\\n{df.loc[df['productid']== idx_example, 'text'][0]}\") # Affiche les 300 premiers chars\n",
    "print(f\"\\n[CLEANED]:\\n{df.loc[df['productid']== idx_example, \"text_cleaned\"][0]}\")\n",
    "\n",
    "# 3. Analyse des mots les plus fréquents APRES nettoyage complet\n",
    "print(f\"\\n{'='*20} TOP 40 MOTS APRÈS NETTOYAGE COMPLET {'='*20}\")\n",
    "freq_after = get_word_freq(df[\"text_cleaned\"], top_n=40)\n",
    "for mot, freq in freq_after:\n",
    "    print(f\"{mot:<15} {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzQ3K7CnlFRf"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">Observations clés :a) Lexique technique émergent, les mots les plus fréquents (\"piscine\", \"cm\", \"haute\", \"qualité\", \"couleur\") correspondent désormais à des concepts métier concrets ; b) Disparition du bruit structurel, absence totale de balises HTML, stopwords dominants et symboles isolés dans le top 40 mots plus fréquents; c) Conservation des features discriminantes, les dimensions (\"cm\", \"mm\"), matériaux (\"bois\", \"acier\") et catégories produits (\"enfants\", \"bébé\") occupent des positions significatives ; d) Validation qualitative,  l'exemple à l'index 3804725264 illustre la préservation du contenu informatif (\"Personalisiertes Notizbuch\", \"150 Seiten\", \"Din A5\") tandis que les artefacts sont éliminés. Le corpus atteint ainsi un niveau d'épuration optimal pour les étapes ultérieures de modélisation, avec une densité sémantique maximisée et un bruit résiduel minimisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "# IV. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "Nous allons sauvegarder le notebook contenant les textes nettoyés des colonnes <code>designation</code> et <code>description</code> afin de poursuivre l’exploration dans un second notebook, basé sur ces données prétraitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"designation_cleaned\"] = df[\"designation\"].fillna(\"\").apply(\n",
    "    lambda x: global_text_cleaner(\n",
    "        x,\n",
    "        use_basic_cleaning=True,\n",
    "        remove_boilerplate=True,\n",
    "        remove_nltk_stops=True,\n",
    "        remove_custom_stops=True,\n",
    "        remove_single_digit=True,\n",
    "        remove_single_letter=True\n",
    "    )\n",
    ")\n",
    "\n",
    "df[\"description_cleaned\"] = df[\"description\"].fillna(\"\").apply(\n",
    "    lambda x: global_text_cleaner(\n",
    "        x,\n",
    "        use_basic_cleaning=True,\n",
    "        remove_boilerplate=True,\n",
    "        remove_nltk_stops=True,\n",
    "        remove_custom_stops=True,\n",
    "        remove_single_digit=True,\n",
    "        remove_single_letter=True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['designation', 'description', 'productid', 'imageid', 'text',\n",
       "       'text_cleaned', 'designation_cleaned', 'description_cleaned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exploration_path = \"/Users/usuario/Documents/GitHub/DS_rakuten/data/raw/\"\n",
    "df.to_csv(data_exploration_path+'exploration_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">Ce notebook a permis de préparer et structurer le texte du dataset afin de disposer d’un corpus propre, cohérent et exploitable. À travers différentes étapes de nettoyage; harmonisation des champs, réduction du bruit inutile, préservation des informations pertinentes, nous avons construit une base textuelle beaucoup plus fiable.\n",
    "\n",
    "Dans le prochain notebook, nous passerons à une étape clé : évaluer systématiquement ces différentes versions. Pour cela, nous construirons un benchmark, en testant chaque stratégie de nettoyage sur nos modèles afin de mesurer laquelle améliore le plus la performance. Ce sera l’occasion de comparer objectivement les approches et d’identifier celles qui apportent un véritable gain et lesquelles non."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
