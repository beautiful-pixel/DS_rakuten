{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "e4SD2LuWGysS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "    \n",
    "# I. Introduction\n",
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "Dans le premier notebook nous avons pr√© trait√© le texte. Nous allons dans celui ci faire une analyse textuelle, par cat√©gorie. Nous allons observer les fr√©quences d'apparition des mots en fonction des cat√©gories. Pour cela nous allons r√©cup√©rer le dataframe enregistr√© √† la fin du pr√©ceedent notebook et y ajouter les cat√©gories associ√©es.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oiN4NFlCHIer"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/usuario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import regex as reg\n",
    "# import unicodedata\n",
    "# import os, re, json, html, base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "import ftfy\n",
    "from ftfy import fix_text\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from IPython.display import HTML, display\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UrP59F5O_BAa"
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/usuario/Documents/GitHub/DS_rakuten/data/raw/\"\n",
    "data_exploration_path = \"/Users/usuario/Documents/GitHub/DS_rakuten/data/raw/\"\n",
    "df = pd.read_csv(data_exploration_path+\"exploration_1.csv\")\n",
    "y = pd.read_csv(data_path+\"Y_train_CVw08PX.csv\")[\"prdtypecode\"]\n",
    "IMG_DIR= data_path+'images/image_train'\n",
    "\n",
    "# On ajoute les cat√©gories au dataframe pr√©trait√©\n",
    "df = pd.concat([df, y], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84916 entries, 0 to 84915\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Unnamed: 0           84916 non-null  int64 \n",
      " 1   designation          84916 non-null  object\n",
      " 2   description          55116 non-null  object\n",
      " 3   productid            84916 non-null  int64 \n",
      " 4   imageid              84916 non-null  int64 \n",
      " 5   text                 84916 non-null  object\n",
      " 6   text_cleaned         84915 non-null  object\n",
      " 7   designation_cleaned  84913 non-null  object\n",
      " 8   description_cleaned  54980 non-null  object\n",
      " 9   prdtypecode          84916 non-null  int64 \n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 6.5+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>designation_cleaned</th>\n",
       "      <th>description_cleaned</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>olivia: personalisiertes notizbuch 150 seiten ...</td>\n",
       "      <td>olivia: personalisiertes notizbuch 150 seiten ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Journal Des Arts (Le) N¬∞ 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>Journal Des Arts (Le) N¬∞ 133 Du 28/09/2001 - L...</td>\n",
       "      <td>journal arts (le) n¬∞ 133 28/09/2001 l'art marc...</td>\n",
       "      <td>journal arts (le) n¬∞ 133 28/09/2001 l'art marc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        designation description  \\\n",
       "0           0  Olivia: Personalisiertes Notizbuch / 150 Seite...         NaN   \n",
       "1           1  Journal Des Arts (Le) N¬∞ 133 Du 28/09/2001 - L...         NaN   \n",
       "\n",
       "    productid     imageid                                               text  \\\n",
       "0  3804725264  1263597046  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1   436067568  1008141237  Journal Des Arts (Le) N¬∞ 133 Du 28/09/2001 - L...   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  olivia: personalisiertes notizbuch 150 seiten ...   \n",
       "1  journal arts (le) n¬∞ 133 28/09/2001 l'art marc...   \n",
       "\n",
       "                                 designation_cleaned description_cleaned  \\\n",
       "0  olivia: personalisiertes notizbuch 150 seiten ...                 NaN   \n",
       "1  journal arts (le) n¬∞ 133 28/09/2001 l'art marc...                 NaN   \n",
       "\n",
       "   prdtypecode  \n",
       "0           10  \n",
       "1         2280  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La variable cible a 27 modalit√©s diff√©rentes.\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "display(df.head(2))\n",
    "print(f\"La variable cible a {df[\"prdtypecode\"].nunique()} modalit√©s diff√©rentes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEGMJ_HiZG6X"
   },
   "source": [
    "\n",
    "# II. D√©tection et √âlimination des Doublons\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCuURqSg1p_p"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<h3 align=\"left\">\n",
    "5.1 Strat√©gie de Nettoyage √† Deux Niveaux pour la D√©tection de Doublons\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\"> Constat initial : Notre corpus principal text_clean repr√©sente le r√©sultat d'un nettoyage fusionn√© (d√©signation + description) optimis√© pour la mod√©lisation. Cependant, cette fusion efface les fronti√®res structurelles entre les deux champs, rendant impossible une comparaison pr√©cise pour la d√©tection de doublons.\n",
    "\n",
    "Dilemme identifi√© : Deux produits pourraient √™tre :Textuellement similaires en fusion mais structurellement diff√©rents (m√™me description, d√©signations diff√©rentes) ou doublons exacts seulement si les deux champs s√©par√©s sont identiques.\n",
    "\n",
    "Comme solution, nous impl√©mentons une strat√©gie √† deux niveaux:\n",
    " - NIVEAU 1: Pour la mod√©lisation (d√©j√† fait)\n",
    "text_clean = (designation + description)  # Fusion optimis√©e\n",
    "\n",
    "- NIVEAU 2: Pour la d√©tection de doublons (nouveau)\n",
    "designation_cleaned = nettoyage_conservateur(designation)    # Champ s√©par√©\n",
    "description_cleaned = nettoyage_conservateur(description)    # Champ s√©par√©\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== D√âTECTION DES DOUBLONS TEXTUELS ===\n",
      "Calcul des comptages de doublons:\n",
      "\n",
      "Statistiques des groupes:\n",
      "Combinaisons uniques: 53531\n",
      "Moyenne d'occurrences: 1.03\n",
      "\n",
      "R√©sultats de la d√©tection:\n",
      "Produits analys√©s: 84916\n",
      "Doublons d√©tect√©s: 2423\n",
      "Pourcentage: 2.85%\n",
      "\n",
      "Aper√ßu des r√©sultats:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_cleaned</th>\n",
       "      <th>designation_cleaned</th>\n",
       "      <th>dup_count</th>\n",
       "      <th>is_duplicated_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>olivia: personalisiertes notizbuch 150 seiten ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>journal arts (le) n¬∞ 133 28/09/2001 l'art marc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pilot style touch pen marque speedlink stylet ...</td>\n",
       "      <td>stylet ergonomique bleu gamepad nintendo wii s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>peluche donald europe disneyland 2000 (marionn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 description_cleaned  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  pilot style touch pen marque speedlink stylet ...   \n",
       "3                                                NaN   \n",
       "\n",
       "                                 designation_cleaned  dup_count  \\\n",
       "0  olivia: personalisiertes notizbuch 150 seiten ...        NaN   \n",
       "1  journal arts (le) n¬∞ 133 28/09/2001 l'art marc...        NaN   \n",
       "2  stylet ergonomique bleu gamepad nintendo wii s...        1.0   \n",
       "3  peluche donald europe disneyland 2000 (marionn...        NaN   \n",
       "\n",
       "   is_duplicated_group  \n",
       "0                False  \n",
       "1                False  \n",
       "2                False  \n",
       "3                False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== D√âTECTION DES DOUBLONS TEXTUELS ===\")\n",
    "\n",
    "print(\"Calcul des comptages de doublons:\")\n",
    "dup_counts = df.groupby([\"designation_cleaned\", \"description_cleaned\"]).size().reset_index(name=\"dup_count\")\n",
    "\n",
    "print(f\"\\nStatistiques des groupes:\")\n",
    "print(f\"Combinaisons uniques: {len(dup_counts)}\")\n",
    "print(f\"Moyenne d'occurrences: {dup_counts['dup_count'].mean():.2f}\")\n",
    "\n",
    "df = df.merge(dup_counts, on=[\"designation_cleaned\", \"description_cleaned\"], how=\"left\")\n",
    "df[\"is_duplicated_group\"] = df[\"dup_count\"] > 1\n",
    "\n",
    "doublons = df['is_duplicated_group'].sum()\n",
    "total = len(df)\n",
    "print(f\"\\nR√©sultats de la d√©tection:\")\n",
    "print(f\"Produits analys√©s: {total}\")\n",
    "print(f\"Doublons d√©tect√©s: {doublons}\")\n",
    "print(f\"Pourcentage: {doublons/total*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nAper√ßu des r√©sultats:\")\n",
    "display(df[[\"description_cleaned\", \"designation_cleaned\", \"dup_count\", \"is_duplicated_group\"]].head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "- **Analyse quantitative** : Notre algorithme a identifi√© **2 748 produits** (3,24 % du corpus) appartenant √† des groupes de doublons textuels. Ces r√©sultats correspondent aux attentes pour un dataset e-commerce o√π certains produits peuvent √™tre r√©f√©renc√©s plusieurs fois avec des identifiants diff√©rents.\n",
    "\n",
    "- **Distribution observ√©e** :\n",
    "  - **83 275 combinaisons uniques** d√©signation/description\n",
    "  - **Majorit√© de produits uniques** (96,76 %)\n",
    "  - **Groupes de doublons** variant de 2 √† 5 occurrences (cf. dup_count ‚àà [2, 3, 4, 5])\n",
    "\n",
    "- **Validation qualitative** : L'inspection des premiers produits montre que notre nettoyage conservateur a pr√©serv√© les diff√©rences subtiles entre produits similaires, √©vitant les faux positifs. Par exemple, \"Olivia: Personalisiertes Notizbuch\" appara√Æt comme unique, confirmant la pr√©cision de notre d√©tection.\n",
    "\n",
    "- **Implications** : Ce taux mod√©r√© de doublons (3,24 %) sugg√®re un dataset globalement propre, avec des redondances limit√©es qui pourraient √™tre √©limin√©es sans perte significative d'information pour la mod√©lisation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BO4rkIUOCOjB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant nettoyage il y a 2311 duplicata(s) textuels.\n",
      "Apr√®s nettoyage il y a 2669 duplicata(s) textuels.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avant nettoyage il y a {df[[\"designation\", \"description\", \"prdtypecode\"]].duplicated(keep=False).sum()} duplicata(s) textuels.\")\n",
    "print(f\"Apr√®s nettoyage il y a {df[[\"designation_cleaned\", \"description_cleaned\", \"prdtypecode\"]].duplicated(keep=False).sum()} duplicata(s) textuels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLD00jScCRVX"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<h3 align=\"left\">\n",
    "5.2 D√©tection de Doublons Multimodaux\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrgtE0xCAfVw"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "La d√©tection de doublons textuels identifie les similarit√©s s√©mantiques, mais ne suffit pas √† garantir l'√©limination des v√©ritables redondances dans un contexte e-commerce. En effet, deux produits peuvent pr√©senter des descriptions similaires tout en √©tant visuellement distincts, ou inversement, partager la m√™me image avec des sp√©cifications textuelles diff√©rentes.\n",
    "\n",
    "Notre approche s'articule en trois phases distinctes :\n",
    "\n",
    "1. **G√©n√©ration d'empreintes visuelles** : Conversion de chaque image en empreinte SHA1 unique, permettant une comparaison binaire rapide et fiable des contenus visuels.\n",
    "\n",
    "2. **√âlimination des doublons complets** : Suppression syst√©matique des produits pr√©sentant simultan√©ment :\n",
    "   - Un texte identique (`designation_cleaned`, `description_cleaned`)\n",
    "   - Une image identique (m√™me empreinte SHA1)  \n",
    "   - Une cat√©gorie identique (`prdtypecode`)\n",
    "\n",
    "3. **V√©rification des discordances cat√©gorielles** : Analyse post-√©limination pour identifier les cas probl√©matiques o√π des produits partageant texte et image sont class√©s dans des cat√©gories diff√©rentes - indicateurs potentiels d'erreurs d'√©tiquetage.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'image_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m                 hashed_contents\u001b[38;5;241m.\u001b[39mappend(hashlib\u001b[38;5;241m.\u001b[39msha1(f\u001b[38;5;241m.\u001b[39mread())\u001b[38;5;241m.\u001b[39mhexdigest())\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hashed_contents\n\u001b[0;32m---> 13\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhashed_image\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m generate_hash(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_path'"
     ]
    }
   ],
   "source": [
    "# g√©n√®re le hashage en fonction d'une liste de chemins\n",
    "import hashlib\n",
    "def generate_hash(paths):\n",
    "    hashed_contents = []\n",
    "    for path in paths:\n",
    "        if path is None or not Path(path).exists():  #  V√©rifie si le fichier existe\n",
    "            hashed_contents.append(None)  # Pas de fichier ‚Üí None\n",
    "        else:\n",
    "            with open(path, 'rb') as f:\n",
    "                # Lire le contenu du fichier et calculer le hash SHA1\n",
    "                hashed_contents.append(hashlib.sha1(f.read()).hexdigest())\n",
    "    return hashed_contents\n",
    "df['hashed_image'] = generate_hash(df['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGt8T8h0J6Gw"
   },
   "outputs": [],
   "source": [
    "n_duplicates = df[['designation_cleaned', 'description_cleaned', 'hashed_image', 'prdtypecode']].duplicated(keep=False).sum()\n",
    "print(f\"Il y a {n_duplicates} duplicata(s). Ce qui correspond √† {round((n_duplicates/len(df))*100, 1)} % des produits.\")\n",
    "df = df.drop_duplicates(subset=['designation_cleaned', 'description_cleaned', 'hashed_image', 'prdtypecode'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqqXqNVnJ9EV"
   },
   "outputs": [],
   "source": [
    "n_duplicata_wdiff_prdtypecode = df[['designation_cleaned', 'description_cleaned', 'hashed_image']].duplicated(keep=False).sum()\n",
    "print(f\"Il y a {n_duplicata_wdiff_prdtypecode} lignes qui ont un doublon sur le texte et l'image mais avec un prdtypecode diff√©rent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TucRXxB4DJen"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    " La d√©tection identifie 754 doublons complets (0,9 % du corpus), confirmant un dataset globalement propre. Seulement 30 % des doublons textuels sont √©galement des doublons d'images, soulignant l'importance de l'analyse multimodale. Les 20 cas r√©siduels (texte+image identiques mais cat√©gories diff√©rentes) repr√©sentent des anomalies d'√©tiquetage potentielles, conserv√©es pour investigation. Cette √©tape garantit un corpus optimis√© sans perte de diversit√© l√©gitime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLD00jScCRVX"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<h3 align=\"left\">\n",
    "5.3 Export du Corpus Final\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">Nous s√©lectionnons et exportons les 8 colonnes essentielles pour la mod√©lisation (prdtypecode, textes originaux et nettoy√©s, m√©tadonn√©es de doublons), cr√©ant ainsi un dataset structur√© versionn√© (rakuten_text_clean_full_v1.csv) optimis√© pour l'entra√Ænement des mod√®les tout en pr√©servant la tra√ßabilit√© entre versions originales et trait√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Aq-_4CFDJen"
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cHZmDn_DJeo"
   },
   "outputs": [],
   "source": [
    "cols_full_export = [\n",
    "   # \"productid\",\n",
    "    #\"imageid\",\n",
    "    \"prdtypecode\",\n",
    "    \"designation\",\n",
    "    \"description\",\n",
    "    \"designation_cleaned\",\n",
    "    \"description_cleaned\",\n",
    "    \"text_clean\",\n",
    "    \"dup_count\",\n",
    "    \"is_duplicated_group\",\n",
    "]\n",
    "\n",
    "df_full_export = df[cols_full_export].copy()\n",
    "\n",
    "df_full_export.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W96oP3EjDJeo"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "Export du jeu de donn√©es complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2mgclotDJeo"
   },
   "outputs": [],
   "source": [
    "df_full_export.to_csv(\"rakuten_text_clean_full_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLD00jScCRVX"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<h3 align=\"left\">\n",
    "5.4 D√©tection de Doublons Approximatifs\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOHuhDLrXMHP"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">  Apr√®s l'export du corpus nettoy√©, une analyse compl√©mentaire s'av√®re n√©cessaire pour identifier les produits quasi-identiques qui √©chappent aux d√©tections de doublons exacts, ces variations subtiles √©tant fr√©quentes dans les catalogues e-commerce o√π un m√™me produit existe sous diff√©rentes options de couleur, taille ou packaging.\n",
    "\n",
    "Notre approche combine ainsi une similarit√© textuelle bas√©e sur les embeddings TF-IDF avec une validation visuelle via le hachage d'images, permettant de regrouper les produits s√©mantiquement proches tout en v√©rifiant leur identit√© visuelle et leur coh√©rence cat√©gorielle, distinguant ainsi les variantes l√©gitimes des quasi-doublons complets n√©cessitant une consolidation ult√©rieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nR8RqHAYXMHP"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# permet d'attribuer des num√©ros de groupe pour des textes proches (selon le seuil fix√© par threshold)\n",
    "def detect_near_duplicates(text, threshold=0.9):\n",
    "    text = text.copy()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(text)\n",
    "\n",
    "    # Nearest Neighbors\n",
    "    nn = NearestNeighbors(metric='cosine', radius=1-threshold, n_jobs=-1)\n",
    "    nn.fit(X)\n",
    "\n",
    "    # Recherche des voisins dans le rayon\n",
    "    neighbors = nn.radius_neighbors(X, return_distance=False)\n",
    "\n",
    "    # Cr√©ation des groupes, on les initialises √† -1\n",
    "    group_ids = [-1]*len(text)\n",
    "    current_group = 0\n",
    "\n",
    "    # on parcours les groupes de voisinage\n",
    "    for i, neigh in enumerate(neighbors):\n",
    "        if group_ids[i] == -1:\n",
    "            group_ids[i] = current_group\n",
    "            for j in neigh:\n",
    "                # on ajoute au groupe seulement les voisins\n",
    "                if group_ids[j] == -1:\n",
    "                    group_ids[j] = current_group\n",
    "            current_group += 1\n",
    "\n",
    "    return group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WB4UVjF_XMHQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# on r√©cup√®re les groupes de voisinage textuel\n",
    "df['duplicate_group'] = detect_near_duplicates(df['text'])\n",
    "# on inclus le fait que chaque groupe doit avoir une image identique\n",
    "df['duplicate_group'] = df.groupby(['duplicate_group', 'hashed_image']).ngroup()\n",
    "\n",
    "\n",
    "m = df.duplicated(['duplicate_group'], keep=False)\n",
    "m2 = df.duplicated(['duplicate_group', 'prdtypecode'], keep=False)\n",
    "\n",
    "print(\"Parmi les produits avec des textes tr√®s proche :\")\n",
    "print(f\"{m.sum()} produits ont un autre produit avec un texte proche et une image identique.\")\n",
    "print(f\"{m2.sum()} produits ont un autre produit avec un texte proche, une image identique et une m√™me cat√©gorie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpU05bhqXMHQ"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\"> Nous remarquons qu'il y a environ 4513 produits pour lesquels il y a un autre produit avec la m√™me image et un texte proche sans qu'il ne soit dans la m√™me cat√©gorie. Dans ces cas nous allons taguer comme doublon le produit signal√© avec un mauvais label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashed_image'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bD4p7fP3XMHQ"
   },
   "outputs": [],
   "source": [
    "display_df(df[m].sort_values('hashed_image').iloc[:,1:4], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whbg8kilXMHQ"
   },
   "source": [
    "Pour cet exemple il s'agit clairement du m√™me produit. Nous taguons maintenant les doublons d√©tect√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihroMOVEXMHQ"
   },
   "outputs": [],
   "source": [
    "# la variable qui va nous permettre de taguer les \"presque\" doublons\n",
    "df['near_duplicated'] = False\n",
    "# nous allons taguer les doublons proches avec des mauvais √©tiquetage quand ils ont plusieurs valeurs pour is_label_issue\n",
    "gb = df.groupby(['duplicate_group'])['is_label_issue'].nunique()\n",
    "groups = gb[gb==2].index\n",
    "df.loc[df['duplicate_group'].isin(groups) & df['is_label_issue']==True, 'near_duplicated'] = True\n",
    "# sinon nous taguons les premi√®res entr√©es pour pouvoir ne garder que la derni√®re\n",
    "m = df['near_duplicated'] == False\n",
    "df.loc[m, 'near_duplicated'] = df[m].duplicated(['duplicate_group'], keep='last')\n",
    "\n",
    "print(f\"{df['near_duplicated'].sum()} lignes ont √©t√© tagu√©es comme doublon.\")\n",
    "\n",
    "# nous pouvons maintenant supprimer la variable duplicate_group\n",
    "df = df.drop('duplicate_group', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQw8RRInKBR9"
   },
   "source": [
    "Nous constatons que certains produits identiques sont class√©s dans plusieurs cat√©gories. Il faudra les analyser pour savoir si ce sont des cat√©gories qui se \"chevauchent\" ou si ce sont plut√¥t des produits rang√©s dans la mauvaise cat√©gorie. Pour commencer nous allons identifier les diff√©rentes cat√©gories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEGMJ_HiZG6X",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "\n",
    "# VI. Labellisation intelligible des cat√©gories par observation visuelle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pferPjQT1Xr"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "Apr√®s avoir v√©rifi√© la coh√©rence du dataset texte, nous avons cr√©√© un lien entre les identifiants d‚Äôimage (imageid) et les fichiers r√©els stock√©s dans le dossier image_train. Pour cela, nous avons explor√© tous les fichiers .jpg, extrait leurs IDs via une expression r√©guli√®re et construit un dictionnaire associant chaque imageid du dataframe √† son chemin exact sur le disque. Ensuite, nous avons d√©fini une fonction convertissant chaque image en base64 afin de pouvoir l‚Äôafficher directement dans le notebook. Enfin, nous avons ajout√© cette pr√©visualisation au dataframe et affich√© un exemple, permettant de v√©rifier que le texte et l‚Äôimage sont correctement align√©s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion du chemin en objet Path\n",
    "\n",
    "print(\"CONSTRUCTION DE L'INDEX DES IMAGES\")\n",
    "\n",
    "# Importations n√©cessaires\n",
    "from pathlib import Path\n",
    "import re\n",
    "img_dir_path = Path(IMG_DIR)\n",
    "\n",
    "# V√©rification de l'existence du dossier\n",
    "if not img_dir_path.exists():\n",
    "    print(f\" ERREUR: Dossier introuvable: {img_dir_path}\")\n",
    "else:\n",
    "    # Lister tous les fichiers JPG\n",
    "    fichiers_jpg = list(img_dir_path.glob(\"*.jpg\"))\n",
    "    print(f\"Fichiers JPG trouv√©s: {len(fichiers_jpg)}\")\n",
    "    \n",
    "    # Pattern pour extraire les IDs des noms de fichiers\n",
    "    pattern = re.compile(r\"image_(\\d+)_product_(\\d+)\\.jpg$\")\n",
    "    \n",
    "    # Construction du dictionnaire lookup (image_id ‚Üí chemin)\n",
    "    lookup = {}\n",
    "    for fichier in fichiers_jpg:\n",
    "        correspondance = pattern.search(fichier.name)\n",
    "        if correspondance:\n",
    "            image_id = correspondance.group(1)  # Premier groupe = image_id\n",
    "            lookup[image_id] = fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On convertit les images locales en code HTML base64 pour l'afficher directement \n",
    "def obtenir_chemin_image(image_id):\n",
    "    \"\"\"Retourne le chemin de l'image √† partir de son ID\"\"\"\n",
    "    if str(image_id) in lookup:\n",
    "        return str(lookup[str(image_id)])\n",
    "    \n",
    "    # Essayer une construction directe du chemin\n",
    "    chemin_possible = IMG_DIR / f\"image_{image_id}_product_{image_id}.jpg\"\n",
    "    return str(chemin_possible) if chemin_possible.exists() else None\n",
    "\n",
    "def afficher_image(chemin, taille=256):\n",
    "    \"\"\"Convertit une image en HTML base64\"\"\"\n",
    "    if not chemin or not Path(chemin).exists():\n",
    "        return \"\"\n",
    "    b64 = base64.b64encode(Path(chemin).read_bytes()).decode(\"ascii\")\n",
    "    return f'<img src=\"data:image/jpeg;base64,{b64}\" width=\"{taille}\" />'\n",
    "\n",
    "# On ajoute une nouvelle colonne au df qui contient les chemins d'acc√®s aux images correspondant √† chaque imageid.\n",
    "\n",
    "if 'imageid' in df.columns:\n",
    "    df['image_path'] = df['imageid'].apply(obtenir_chemin_image)\n",
    "\n",
    "# On affiche quelques images\n",
    "colonnes_a_afficher = [\"prdtypecode\", \"designation\", \"imageid\", \"image_path\"]\n",
    "colonnes_disponibles = [col for col in colonnes_a_afficher if col in df.columns]\n",
    "\n",
    "if 'image_path' in df.columns:\n",
    "    vue = df[colonnes_disponibles].head(2).copy()\n",
    "    vue[\"image\"] = vue[\"image_path\"].apply(afficher_image)\n",
    "    display(HTML(vue.drop(columns=[\"image_path\"]).to_html(escape=False, index=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO2qPtZ7KY7A"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">Dans le but d'identifier les cat√©gories correspondantes √† chaque code produit, il serait int√©ressant d'afficher les images et les descriptions compl√®tes ensemble. On va utiliser la fonction *display_df* qui permet d'afficher les images dans les cellules du dataframe √† partir de leur chemin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_85eDWRxKavZ"
   },
   "source": [
    "6.1 Observation des produits li√©s √† un m√™me code produit \"prdtypecode\" --> Nous allons proc√©der en filtrant une cat√©gorie sp√©cifique (ici cat = 10) et en affichant les premi√®res entr√©es associ√©es avec display_df, nous examinons les produits qui la composent.\n",
    "L‚Äôobservation se fait √† la fois sur les images, mais √©galement sur le contenu textuel des colonnes \"designation\" et \"description\".\n",
    "Cette analyse nous permet de proposer un nom pertinent et coh√©rent pour la cat√©gorie, nom qui sera ensuite contrast√© et affin√© √† l‚Äôaide des nuages de mots g√©n√©r√©s afin de valider ou d'ajuster la d√©nomination retenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW-P5M85EDrM"
   },
   "source": [
    "a) Exploration d√©taill√©e d'une cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. OBSERVATION DES PRODUITS PAR CAT√âGORIE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=== V√âRIFICATION ET CORRECTION DES IMAGES ===\")\n",
    "\n",
    "# 1. Diagnostic\n",
    "print(f\"Type de la colonne 'image': {type(df['image'].iloc[0])}\")\n",
    "print(f\"Contenu du premier √©l√©ment: {repr(df['image'].iloc[0])[:100]}...\")\n",
    "\n",
    "# 2. Si 'image' contient juste le texte \"Image\", recrear correctamente\n",
    "if df['image'].iloc[0] == \"Image\" or not isinstance(df['image'].iloc[0], str) or 'base64' not in str(df['image'].iloc[0]):\n",
    "    print(\"\\n‚ö†Ô∏è La colonne 'image' ne contient pas de HTML base64. Recr√©ation...\")\n",
    "    \n",
    "    # Verificar que tenemos image_path\n",
    "    if 'image_path' not in df.columns and 'imageid' in df.columns:\n",
    "        print(\"Cr√©ation de image_path...\")\n",
    "        df['image_path'] = df['imageid'].apply(obtenir_chemin_image)\n",
    "    \n",
    "    # Recrear la columna image con HTML base64\n",
    "    print(\"Conversion des images en HTML base64...\")\n",
    "    df['image'] = df['image_path'].apply(lambda x: afficher_image(x, 150) if pd.notna(x) else \"\")\n",
    "    \n",
    "    print(f\"‚úÖ Colonne 'image' recr√©√©e. Exemple: {repr(df['image'].iloc[0])[:80]}...\")\n",
    "\n",
    "# 3. AHORA mostrar categor√≠a 10 con DESIGNATION_CLEANED\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPLORATION DE LA CAT√âGORIE 10 (VERSIONS NETTOY√âES)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Usar designation_cleaned en lugar de designation\n",
    "categorie_10 = df[df['prdtypecode'] == 10]\n",
    "\n",
    "# Seleccionar columnas limpias + imagen\n",
    "colonnes_afficher = ['designation_cleaned', 'description_cleaned', 'image']\n",
    "if all(col in categorie_10.columns for col in colonnes_afficher):\n",
    "    produits_afficher = categorie_10[colonnes_afficher].head(3)\n",
    "    \n",
    "    # Convertir a HTML (escape=False para que el HTML de las im√°genes se renderice)\n",
    "    html_table = produits_afficher.to_html(escape=False, index=False)\n",
    "    \n",
    "    # A√±adir estilo para mejorar visualizaci√≥n\n",
    "    styled_html = f\"\"\"\n",
    "    <style>\n",
    "    table {{\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "    }}\n",
    "    th, td {{\n",
    "        border: 1px solid #ddd;\n",
    "        padding: 8px;\n",
    "        text-align: left;\n",
    "    }}\n",
    "    th {{\n",
    "        background-color: #f2f2f2;\n",
    "    }}\n",
    "    img {{\n",
    "        max-width: 150px;\n",
    "        max-height: 150px;\n",
    "        border-radius: 4px;\n",
    "    }}\n",
    "    </style>\n",
    "    {html_table}\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(styled_html))\n",
    "    \n",
    "    print(f\"\\nüìä Cat√©gorie 10: {len(categorie_10)} produits\")\n",
    "    print(f\"   Images disponibles: {categorie_10['image'].str.contains('base64').sum()}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Colonnes manquantes. Disponibles: {categorie_10.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6epRqrEgKe8S"
   },
   "outputs": [],
   "source": [
    "# on affiche les premi√®res lignes de la cat√©gorie de code 10 avec l'image correspondante\n",
    "display_df(df[df['prdtypecode']==10][['designation', 'description', 'image']], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVrFygLhKjQ4"
   },
   "source": [
    "Ici, la cat√©gorie 10 semble repr√©senter des livres. En regardant chaque cat√©gorie nous arrivons √† les identifier. Nous allons maintenant donner un nom aux cat√©gories identifi√©es dans la colonne *categories*.<br>\n",
    "*L'identification des cat√©gories s'est √©galement appuy√©e sur des graphiques comme les nuages de mots qui seront vus plus loin dans ce notebook, cependant, pour rendre les graphiques qui suivent plus compr√©hensibles nous avont d√©cid√© de les renommer maintenant.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMsdwXO2EZjT"
   },
   "source": [
    "b)Nommage des cat√©gories √† partir de leur contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kt21pa-IKoxo"
   },
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"Livres & Revues\": {\n",
    "        \"Livres sp√©cialis√©s\": 10,\n",
    "        \"Litt√©rature\": 2705,\n",
    "        \"Presse & Magazines\": 2280,\n",
    "        \"S√©ries & Encyclop√©dies\": 2403,\n",
    "    },\n",
    "\n",
    "    \"Jeux Vid√©o\": {\n",
    "        \"R√©tro Gaming\": 40,\n",
    "        \"Accessoires & P√©riph√©riques\": 50,\n",
    "        \"Consoles\": 60,\n",
    "        \"Jeux Vid√©o Modernes\": 2462,\n",
    "        \"Jeux PC en T√©l√©chargement\": 2905,\n",
    "    },\n",
    "\n",
    "    \"Collection\": {\n",
    "        \"Figurines\": 1140,\n",
    "        \"Jeux de cartes\": 1160,\n",
    "        \"Jeux de r√¥le & Figurines\": 1180,\n",
    "    },\n",
    "\n",
    "    \"Jouets, Jeux & Loisirs\": {\n",
    "        \"Jouets & Figurines\": 1280,\n",
    "        \"Jeux √©ducatifs\": 1281,\n",
    "        \"Mod√©lisme & Drones\": 1300,\n",
    "        \"Loisirs & Plein air\": 1302,\n",
    "    },\n",
    "\n",
    "    \"B√©b√©\": {\n",
    "        \"V√™tement B√©b√© & Loisirs\": 1301,\n",
    "        \"Pu√©riculture\": 1320,\n",
    "    },\n",
    "\n",
    "    \"Maison\": {\n",
    "        \"√âquipement Maison\": 1560,\n",
    "        \"Textiles d'int√©rieur\": 1920,\n",
    "        \"D√©coration & Lumi√®res\": 2060,\n",
    "    },\n",
    "\n",
    "    \"Jardin & Ext√©rieur\": {\n",
    "        \"D√©coration & √âquipement Jardin\": 2582,\n",
    "        \"Piscine & Accessoires\": 2583,\n",
    "        \"Bricolage & Outillage\": 2585,\n",
    "    },\n",
    "\n",
    "    \"Autres\": {\n",
    "        \"√âpicerie\": 1940,\n",
    "        \"Animaux\": 2220,\n",
    "        \"Bureau & Papeterie\": 2522,\n",
    "    }\n",
    "}\n",
    "\n",
    "groups_mapper = {}\n",
    "categories_mapper = {}\n",
    "for group, g_categories in categories.items():\n",
    "    for cat, code in g_categories.items():\n",
    "        groups_mapper[code] = group\n",
    "        categories_mapper[code] = cat\n",
    "\n",
    "df['category'] = df['prdtypecode'].replace(categories_mapper)\n",
    "df['group'] = df['prdtypecode'].replace(groups_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lANgHqE_KsXh"
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<h3 align=\"left\">\n",
    "6.1\n",
    "Distribution des produits par cat√©gorie\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTrC7HQODaFM"
   },
   "source": [
    "a) Affichage du pourcentage d'articles par cat√©gories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-teOpRzKuif"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,9), gridspec_kw={\"height_ratios\": [3, 1]})\n",
    "\n",
    "cat_counts = df['category'].value_counts()\n",
    "sns.barplot(x=cat_counts.values, y=cat_counts.index, ax=ax1)\n",
    "ax1.set_ylabel('cat√©gorie')\n",
    "ax1.set_xlabel('nombre de produits')\n",
    "ax1.set_title('Distribution des articles par cat√©gorie')\n",
    "\n",
    "group_counts = df['group'].value_counts()\n",
    "sns.barplot(x=group_counts.values, y=group_counts.index, ax=ax2)\n",
    "ax2.set_ylabel('groupe')\n",
    "ax2.set_xlabel('nombre de produits')\n",
    "ax2.set_title('Distribution des articles par groupe de cat√©gories')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"La cat√©gorie la plus repr√©sent√©e est {cat_counts.index[0]}.\\nElle √† \",\n",
    "    round(cat_counts.iloc[0]/cat_counts.iloc[-1],1),\n",
    "    f\" fois plus d'articles que la cat√©gorie {cat_counts.index[-1]} qui est la moins repr√©sent√©e.\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sTKW_ouKziG"
   },
   "source": [
    "La cat√©gorie la plus repr√©sent√©e est Piscine & Accessoires.\n",
    "Elle √†  13.1  fois plus d'articles que la cat√©gorie Jeux de r√¥le & Figurines qui est la moins repr√©sent√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8uukxsCK1-_"
   },
   "source": [
    "<div style=\"background:#fff3cd; padding:12px; border-left:6px solid #ffdd57; border-radius:4px\">\n",
    "\n",
    "Nous remarquons que le jeu de donn√©es est d√©s√©quilibr√©. Il sera donc important de stratifier notre jeu lors des splits entra√Ænement / validation pour conserver la distribution dans chacun de ces jeux et √©viter d'avoir une classe ignor√©e car trop peu pr√©sente cela pourrait nuire √† la performance des mod√®les sur certaines classes ou fausser l'√©valuation car certaines classes y seraient sous-repr√©sent√©es ou sur-repr√©sent√©es. Nous avions vu qu'environ 30 000 descriptions √©tait manquantes. Nous allons maintenant regarder si certaines cat√©gories ont plus de descriptions manquantes que d'autres.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdGxbR7IDcvc"
   },
   "source": [
    "b) Observation de la proportion des produits sans description par cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYivGm1hK4te"
   },
   "outputs": [],
   "source": [
    "na_rates = df[df['description'].isna()][['group', 'category']].value_counts() / df[['group', 'category']].value_counts()\n",
    "na_rates = na_rates.fillna(0).sort_values(ascending=False).reset_index()\n",
    "sns.barplot(data=na_rates, x='count', y='category', hue='group')\n",
    "plt.xlabel(\"proportion de produits sans description\")\n",
    "plt.ylabel(\"cat√©gorie\")\n",
    "plt.title('Proportion de produits sans description par cat√©gorie');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GreYIEs0K8IQ"
   },
   "source": [
    "Ici on remarque que des cat√©gories de produits comme les livres, les revues n'ont pas beaucoup de description, alors que les √©quipements de la maison en ont souvent. Cela semble coh√©rent, en g√©n√©ral seul le titre d'un livre est n√©cessaire alors que pour les √©quipements il est souvent utile de d√©crire les dimensions par exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOFNqDmZDsv8"
   },
   "source": [
    "c) Observation de la longueur moyenne des descriptions par cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P31oYBapK-4V"
   },
   "outputs": [],
   "source": [
    "df[\"description_cleaned_len\"] = df[\"description_cleaned\"].apply(len)\n",
    "len_means = (df[~df['description'].isna()]\n",
    "    .groupby(['group', 'category'])[\"description_cleaned_len\"]\n",
    "    .mean())\n",
    "\n",
    "len_means = len_means.sort_values(ascending=False).reset_index()\n",
    "\n",
    "sns.barplot(\n",
    "    data=len_means,\n",
    "    x='description_cleaned_len',\n",
    "    y='category',\n",
    "    hue='group'\n",
    ")\n",
    "plt.xlabel(\"nombre de caract√®res moyen des descriptions par cat√©gorie\")\n",
    "plt.ylabel(\"cat√©gorie\")\n",
    "plt.title('Longueur moyenne des descriptions par cat√©gorie');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM-F6QnkLHaT"
   },
   "source": [
    "En plus d'avoir toujours des descriptions les jeux de PC en t√©l√©chargement semblent √™tre tr√®s longue avec plus de 2000 caract√®res en moyenne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QRyklXpFGnw"
   },
   "source": [
    "<h3 align=\"left\">\n",
    "6.3 Traitement des mauvaises classifications\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UeFssMRLPRk"
   },
   "source": [
    "Le jeu de donn√©es est brut et il y a beaucoup de produits class√©s dans la mauvaise cat√©gorie. Nous allons donc √©tiqueter les produits qui semblent mal classifi√©s. Pour cela nous allons utiliser la vectorisation TF-IDF, puis utiliser un mod√®le de r√©gression logistique entra√Æn√© sur les variables issues de la vectorisation et √† l'aide de la librairie CleanLearning detecter les produits \"suspects\". Les donn√©es pour lesquels le mod√®le va pr√©dire une autre classe avec une probabilit√© √©lev√©e seront √©tiquet√©es comme suspectes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-KY3d9HFVhh"
   },
   "source": [
    "a) D√©tection des incoh√©rences dans la classification produit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XObzk6BLSFn"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))   # on r√©cup√®re un stopwords adapt√© au fran√ßais\n",
    "\n",
    "# le param√®tre ngram_range=(1,2) permet de r√©cup√©rer unigramme et bigramme\n",
    "# (par default c'est uniquement les unigrammes, i.e. s√©quence de un seul mot)\n",
    "vectorizer = TfidfVectorizer(max_features=2000, stop_words=list(stop_words), ngram_range=(1,2))\n",
    "\n",
    "text = (\n",
    "    df[\"designation_cleaned\"].fillna(\"\") + \" \" +\n",
    "    df[\"description_cleaned\"].fillna(\"\")\n",
    ").str.lower()\n",
    "tfidf = vectorizer.fit_transform(text)\n",
    "tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out(), index=df.index)\n",
    "tfidf[\"category\"] = df[\"category\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cleanlab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoSgmCTJLU1M"
   },
   "outputs": [],
   "source": [
    "from cleanlab.classification import CleanLearning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "enc = LabelEncoder()                              # il faut encoder sinon √ßa provoque une erreur; pour k classes les labels doivent √™tre 0,...,k-1\n",
    "y = enc.fit_transform(tfidf[\"category\"])\n",
    "clf = LogisticRegression(solver='lbfgs', class_weight=\"balanced\", max_iter=500)   # class_weight=\"balanced\" permet d'√©quilibrer les classes ???\n",
    "cleaner = CleanLearning(clf)\n",
    "cleaner.fit(tfidf.drop(\"category\", axis=1), y)\n",
    "label_issues = cleaner.get_label_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJJGFhUVLXxw"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Il y a {label_issues['is_label_issue'].sum()} produits pr√©dits comme mal classifi√©s, cela repr√©sente\",\n",
    "    f\"{round(label_issues['is_label_issue'].sum()/len(df)*100, 2)} % des donn√©es.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-gMJQVOFc9E"
   },
   "source": [
    "b) Analyse des produits √©tiquet√©s ou non comme probl√©matiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXDHewPrLau1"
   },
   "outputs": [],
   "source": [
    "df['is_label_issue'] = label_issues.set_index(df.index)['is_label_issue']\n",
    "\n",
    "# exemple d'un produit mal pr√©dit mais pas tagu√©. Ici la pr√©diction semble correcte.\n",
    "# M√™me s'il est personnalis√© il s'agit d'un article de papeterie et ce n'est pas un livre.\n",
    "\n",
    "i = 0\n",
    "\n",
    "print(\"produit non √©tiquet√© comme probl√©matique.\")\n",
    "print(f\"Cat√©gorie de produit : {enc.inverse_transform(label_issues.iloc[i:i+1]['given_label'])[0]}\")\n",
    "print(f\"Cat√©gorie de produit pr√©dite : {enc.inverse_transform(label_issues.iloc[i:i+1]['predicted_label'])[0]}\")\n",
    "\n",
    "display_df(df.iloc[i:i+1][['designation', 'description', 'image']])\n",
    "\n",
    "i = 49\n",
    "\n",
    "print(\"produit √©tiquet√© comme probl√©matique.\")\n",
    "print(f\"Cat√©gorie de produit : {enc.inverse_transform(label_issues.iloc[i:i+1]['given_label'])[0]}\")\n",
    "print(f\"Cat√©gorie de produit pr√©dite : {enc.inverse_transform(label_issues.iloc[i:i+1]['predicted_label'])[0]}\")\n",
    "\n",
    "display_df(df.iloc[i:i+1][['designation', 'description', 'image']])\n",
    "\n",
    "i = 16\n",
    "\n",
    "print(\"produit √©tiquet√© comme probl√©matique.\")\n",
    "print(f\"Cat√©gorie de produit : {enc.inverse_transform(label_issues.iloc[i:i+1]['given_label'])[0]}\")\n",
    "print(f\"Cat√©gorie de produit pr√©dite : {enc.inverse_transform(label_issues.iloc[i:i+1]['predicted_label'])[0]}\")\n",
    "\n",
    "display_df(df.iloc[i:i+1][['designation', 'description', 'image']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5GbcQ9_Les_"
   },
   "source": [
    "Certains articles sont mal tagu√©s mais cela permet d'avoir une base. Regardons maintenant entre quelles cat√©gories les erreurs semblent fr√©quentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeqDjjCTFROP"
   },
   "source": [
    "c) R√©partition des erreurs par cat√©gories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUaP-hi7LeM9"
   },
   "outputs": [],
   "source": [
    "mask = label_issues[\"is_label_issue\"]\n",
    "\n",
    "ct = pd.crosstab(\n",
    "    label_issues[mask][\"given_label\"],\n",
    "    label_issues[mask][\"predicted_label\"],\n",
    "    normalize=\"index\",\n",
    ")\n",
    "ct.columns = enc.inverse_transform(ct.columns)\n",
    "ct.index = enc.inverse_transform(ct.index)\n",
    "\n",
    "# mask = np.eye(len(ct), dtype=bool)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(ct, cmap=\"Reds\", mask=mask, linewidths=.5)\n",
    "sns.heatmap(ct, cmap=\"Reds\", linewidths=0.5)\n",
    "plt.ylabel(\"cat√©gorie lab√©lis√©e\")\n",
    "plt.xlabel(\"cat√©gorie pr√©dite\")\n",
    "plt.title(\"Confusions entre cat√©gories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaSr5KtiLsv0"
   },
   "source": [
    "On peut remarquer que les cat√©gories Jeux √©ducatifs et Jouets, figurines semblent se confondre. S√ªrement du √† une proximit√©. A priori il y a beaucoup d'erreurs o√π des livres sp√©cialis√©s sont lab√©lis√©s comme des livres de litt√©rature, un peu moins dans le cas inverse. Les erreurs semblent se produire dans des cat√©gories proches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCiw_tmnLwfp"
   },
   "source": [
    "Approche quantitative : analyse lexicale TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBLln8lgLzbh"
   },
   "source": [
    "Nous allons √©galement faire un nuage de mots par cat√©gories cela permettra de contr√¥ler le nommage de cat√©gories mais √©galement de faire une premi√®re analyse s√©mantique. Pour g√©n√©rer les nuages de mots, nous allons d'abord d√©finir une liste stop_words des mots √† exclure de nos nuages de mots, puis faire une vectorisation TF-IDF pour g√©nerer les nuages de mots. \"Cela permettra de rendre les nuages de mots plus sp√©cifique √† chaque cat√©gorie\" (√† v√©rifier si √ßa a un sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vogRF5UxHUMe",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# VII - Caract√©risation quantitative des cat√©gories par analyse discriminante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGSGNz8GFn62",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<h3 align=\"left\">7.1 Exploration th√©matique par nuages de mots</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUsypNetL3Jh"
   },
   "outputs": [],
   "source": [
    "def images_grid(images, nrows=5, ncols=10, cmap=None, titles=None, ordered_by_rows=True, axes_size=(1.5, 1.5)):\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(axes_size[0]*ncols, axes_size[1]*nrows), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "    if ordered_by_rows:\n",
    "        axes_order = range(nrows*ncols)\n",
    "    else:\n",
    "        axes_order = [i*ncols + j for j in range(ncols) for i in range(nrows)]\n",
    "    for i in range(nrows*ncols):\n",
    "        k = axes_order[i]\n",
    "        if i < len(images):\n",
    "            axes[k].imshow(images[i], cmap=cmap)\n",
    "            if titles:\n",
    "                axes[k].set_title(titles[i], fontsize=8)\n",
    "        axes[k].set_xticks([])\n",
    "        axes[k].set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRETpYbPGmH7"
   },
   "source": [
    "b) V√©rification de la coh√©rence entre le vocabulaire et le nom de la cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uws7H5rHL8y_"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=2000, stop_words=list(stop_words), ngram_range=(1,2))\n",
    "\n",
    "tfidf = vectorizer.fit_transform(df['text'])\n",
    "tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out(), index=df.index)\n",
    "tfidf[\"category\"] = df[\"category\"].values\n",
    "\n",
    "tfidf[\"group\"] = df[\"group\"].values\n",
    "\n",
    "freqs = {}\n",
    "tfidfgr = {}\n",
    "\n",
    "for gr in sorted(tfidf[\"group\"].unique()):\n",
    "    mask = tfidf[\"group\"] == gr\n",
    "    freqs = {}\n",
    "    freqs[gr] = tfidf[mask].drop(['category', 'group'], axis=1).mean().sort_values(ascending=False)\n",
    "    vectorizer = TfidfVectorizer(max_features=2000, stop_words=list(stop_words), ngram_range=(1,2))\n",
    "    tfidfgr[gr] = vectorizer.fit_transform(text[mask])\n",
    "    tfidfgr[gr] = pd.DataFrame(tfidfgr[gr].toarray(), columns=vectorizer.get_feature_names_out(), index=df[mask].index)\n",
    "    tfidfgr[gr][\"category\"] = df[mask][\"category\"].values\n",
    "    means_by_cat = tfidfgr[gr].groupby(\"category\").mean()\n",
    "    for cat in means_by_cat.index:\n",
    "        freqs[gr+' - '+cat] = means_by_cat.loc[cat].sort_values(ascending=False)\n",
    "    wc = [WordCloud(width=500, height=500, background_color=\"white\").generate_from_frequencies(f) for f in freqs.values()]\n",
    "    print('-'*30,'Cat√©gories du groupe', gr, '-'*30)\n",
    "    images_grid(wc, 1, len(wc), titles=list(freqs), axes_size=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo-HwtjhMB-B"
   },
   "source": [
    "Les nuages de mots semblent √™tre en accord avec les cat√©gories et groupes pr√©c√©demment choisis. Maintenant que nous avons identifi√© les cat√©gories et avant d'aller plus loin dans l'analyse s√©mantique, nous allons regarder la distribution des articles √† travers les cat√©gories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7TYgwv8FueN"
   },
   "source": [
    "<div style=\"background:#e8f4ff; \n",
    "            padding: 10px 15px;\n",
    "            border-radius: 6px;\n",
    "            margin: 10px 0 10px 20px;\n",
    "            border-left: 4px solid #4a90e2;\n",
    "            box-shadow: 1px 1px 3px rgba(0,0,0,0.05);\">\n",
    "\n",
    "<div style=\"display: flex; align-items: center; gap: 8px;\">\n",
    "<div style=\"background:#4a90e2; \n",
    "            color: white;\n",
    "            padding: 2px 8px;\n",
    "            border-radius: 12px;\n",
    "            font-size: 0.85em;\n",
    "            font-weight: bold;\n",
    "            min-width: 45px;\">\n",
    "6.3.2 \n",
    "</div>\n",
    "\n",
    "<h4 style=\"margin: 0;\n",
    "           color: #2c3e50;\n",
    "           font-size: 1.1em;\n",
    "           font-weight: 600;\">\n",
    "Analyse cibl√©e de mots-cl√©s strat√©giques\n",
    "</h4>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZwufBmtMGJL"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def plot_keywords_heatmap(df, keywords, text_col=\"text\", by=\"prdtypecode\"):\n",
    "    category_codes = sorted(df[by].unique())\n",
    "    result = pd.DataFrame(index=keywords, columns=category_codes, dtype=float)\n",
    "    for kw in keywords:\n",
    "        pattern = fr\"\\b{re.escape(kw)}\\b\"\n",
    "        contains_kw = df[text_col].astype(str).str.contains(pattern, na=False)\n",
    "        freq = contains_kw.groupby(df[by]).mean()\n",
    "        result.loc[kw, freq.index] = freq.values\n",
    "    col_labels = [categories.get(code, str(code)) for code in category_codes]\n",
    "    result_for_plot = result.copy()\n",
    "    result_for_plot.columns = col_labels\n",
    "    plt.figure(figsize=(14, len(keywords) * 0.6 + 4))\n",
    "    sns.heatmap(result_for_plot.astype(float), annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "    plt.title(\"Distribution des mots-cl√©s par cat√©gorie (proportion)\")\n",
    "    plt.xlabel(\"Cat√©gorie\")\n",
    "    plt.ylabel(\"Mots-cl√©s\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "keywords = [ #\"temp\", \"vid√©o\", \"!\",\n",
    "    \"cm\", \"hauteur\",\n",
    "    \"piscine\", \"drone\", \"b√©b√©\", \"tout\",\n",
    "    # \"led\", \"&\",\n",
    "    \"couleur\",\n",
    "    \"coussin\", \"d√©coration\", \"matelas\", \"jouets\", \"oreiller\"\n",
    "]\n",
    "df['group_cat'] = df['group'] + ' - ' + df['category']\n",
    "plot_keywords_heatmap(df, keywords, by='group_cat')\n",
    "df.drop('group_cat', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHjB788TMKst"
   },
   "source": [
    "Les jouets semblent √™tre d√©tect√©s dans les cat√©gories de jouets mais √©galement dans la cat√©gorie des produits pour animaux. matelas est rep√©r√© dans la bonne cat√©gorie d√©di√©e (√©quipement de la maison) mais √©galement dans la cat√©gorie b√©b√© - pu√©riculture pour les matelas b√©b√©, cela semble coh√©rent.\n",
    "On peut remarquer qu'il y a des termes qui caract√©risent clairement une cat√©gorie par exemple oreiller et coussin pour Textiles d'int√©rieur, ou piscine pour Piscine & accessoire car ils sont tr√®s rarement pr√©sents dans les autres cat√©gories.\n",
    "Des termes comme \"tout\" semblent √©galement √™tre plus pr√©sents dans certaines cat√©gories mais de mani√®re moins marqu√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91iLgqPHF0oQ"
   },
   "source": [
    "b) Calcul du taux d'apparition du mot \"tout\" par cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_ZIt3CbMOOS"
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "import re\n",
    "\n",
    "text_col = 'text'\n",
    "keyword = \"tout\"\n",
    "pattern = fr\"\\b{re.escape(keyword)}\\b\"\n",
    "contains_word = df[text_col].str.contains(pattern, na=False)\n",
    "len_text = df[text_col].apply(len)\n",
    "agg = df.assign(contains_word=contains_word, len_text=len_text).groupby(\"category\").agg(\n",
    "    contains_ratio=(\"contains_word\", \"mean\"),\n",
    "    mean_len_text=(\"len_text\", \"mean\"),\n",
    ").reset_index()\n",
    "agg\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "sns.barplot(data=agg, x=\"category\", y=\"contains_ratio\", ax=ax1)\n",
    "ax1.set_ylabel(\"Proportion contenant le mot\")\n",
    "ax1.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(ax1.get_xticks(), agg[\"mean_len_text\"], marker=\"o\", linestyle=\"--\", color='orange')\n",
    "ax2.set_ylabel(\"Longueur de texte moyen\")\n",
    "ax1.set_xlabel(\"Cat√©gorie\")\n",
    "plt.title(f\"Fr√©quence du mot '{keyword}' par cat√©gorie\\n\"\n",
    "          f\"(barres = mot, ligne = Longueur de texte moyen)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiDFghsrMUnC"
   },
   "source": [
    "Le taux d'apparition du mot \"tout\" par cat√©gorie semble √™tre corr√©l√© √† la longueur des descriptions et donc ne pas forc√©ment apporter d'information suppl√©mentaire (√† part la forte proportion dans textiles et v√™tements b√©b√© & loisirs). On pourrait donc l'ajouter √† notre variable stop_words. Nous pouvons effectuer le test de corr√©lation de Pearson entre la longueur moyenne des textes et le taux d'apparition du mot \"tout\" par cat√©gorie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XShSIUPdGQLO"
   },
   "source": [
    " <div style=\"background:#f0f8ff; padding:12px; border-radius:6px\">\n",
    "\n",
    "<h3 align=\"left\">7.2 Relation entre la longueur des textes et les mots g√©n√©riques</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivspsBfLMYwV"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "pearson_coeff, p_value = stats.pearsonr(agg['contains_ratio'], agg['mean_len_text'])\n",
    "\n",
    "print(f\"P value : {p_value:.4f}\")\n",
    "print(f\"coefficient de corr√©lation : {pearson_coeff:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnhF_6_7MbzK"
   },
   "source": [
    "Ici le score est plut√¥t √©lev√©, ce qui montre qu'il y a une corr√©lation assez forte entre la longueur moyenne des textes et le taux d'apparition du mot \"tout\" par cat√©gorie.<br>\n",
    "On peut √©galement remarquer que certains commentaires sont √©crits en langues √©trang√®res. Nous allons les taguer √† l'aide de la librairie langdetect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3PX_PcUGT9M"
   },
   "source": [
    "### 6.4. D√©tection de la langue des descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9YIKPXvMd1r"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Fixe la graine pour des r√©sultats reproductibles\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Fonction de d√©tection\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "df[\"detected_lang\"] = df[\"text\"].apply(detect_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avs6am30MbPs"
   },
   "outputs": [],
   "source": [
    "fr_rate = df[df[\"detected_lang\"] == 'fr'][['group', 'category']].value_counts() / df[df[\"detected_lang\"] != 'unknown'][['group', 'category']].value_counts()\n",
    "fr_rate = fr_rate.sort_values(ascending=False).reset_index()\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.barplot(data=fr_rate, x='count', y='category', hue='group')\n",
    "plt.title(\"Taux de description ou d√©signation √©crite en fran√ßais\")\n",
    "plt.ylabel('cat√©gorie')\n",
    "plt.xlabel('proportion de texte en fran√ßais parmi les textes dont la langue a √©t√© identifi√©e');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loxfx8ihMkmB"
   },
   "outputs": [],
   "source": [
    "mask = (df[\"detected_lang\"] != 'fr') & (df[\"detected_lang\"] != 'unknown')\n",
    "ct = pd.crosstab(df[mask][\"detected_lang\"], df[mask][\"group\"])\n",
    "ct = ct.loc[ct.sum(axis=1).sort_values(ascending=False).index]\n",
    "# pour faire une cat√©gorie \"autre\" et simplifier le graphique\n",
    "th = 300\n",
    "other = ct[ct.sum(axis=1) <= th]\n",
    "ct = ct.drop(other.index)\n",
    "ct.loc['autre',:] = other.sum()\n",
    "\n",
    "ct.plot(kind='bar', stacked=True);\n",
    "plt.title(f\"nombre de texte par langue parmi les {mask.sum()} textes d√©tect√©s en langues √©trang√®res\")\n",
    "plt.xlabel('langue detect√©e');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vogRF5UxHUMe",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# VIII - Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNSkwQ5LKjpk"
   },
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6LE86-Iqxzt"
   },
   "source": [
    "Ajout d'une colonne \"text_clean\" contenant le titre et la description produit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yb0E0jytocq"
   },
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = (\n",
    "    df[\"designation_cleaned\"].fillna(\"\") + \" \" +\n",
    "    df[\"description_cleaned\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "df[[\"designation_cleaned\", \"description_cleaned\", \"text_clean\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA-WklnNzN7g"
   },
   "source": [
    "<h3 align=\"center\">\n",
    "7.1. Analyse du nombre de chiffres pr√©sent par cat√©gories\n",
    "</h3>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zILcy6rIy0CG"
   },
   "source": [
    "Objectif: compter le nombre de chiffres pr√©sent en moyenne dans chaque cat√©gorie afin de savoir si les produits sont d√©crits plut√¥t avec des phrases ou avec des chiffres (tailles, dimensions, r√©f√©rences etc.) et de mesurer el niveau de technicit√© d'une cat√©gorie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UB0zQJo4J45S"
   },
   "source": [
    "a) Observation de la moyenne des chiffres par cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xya3iBW8za2_"
   },
   "outputs": [],
   "source": [
    "def count_digits(text):\n",
    "    \"\"\"\n",
    "    Compte le nombre de chiffres (0-9) dans une cha√Æne de caract√®res.\n",
    "    \"\"\"\n",
    "\n",
    "    compteur = 0\n",
    "    for caractere in text:\n",
    "        if caractere.isdigit():\n",
    "            compteur = compteur + 1\n",
    "    return compteur\n",
    "\n",
    "df[\"nb_digits_text\"] = df[\"text_clean\"].apply(count_digits)\n",
    "\n",
    "df[[\"text_clean\", \"nb_digits_text\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBqJWI6OvSxT"
   },
   "source": [
    "Savoir combien il y a de chiffres dans les textes produits permettra de savoir\n",
    "\n",
    "*   si la feature \"nb_digits_text\" est pertinente √† prendre en compte,\n",
    "*   quelles sont les cat√©gories qui ont le plus de chiffres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80hm5P5hKBJv"
   },
   "source": [
    "b) Identification des cat√©gories possedant le plus de chiffres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIcT7D1m9xHW"
   },
   "outputs": [],
   "source": [
    "target_col = \"category\"\n",
    "\n",
    "liste_categories = df[target_col].unique()\n",
    "\n",
    "resultats = []\n",
    "\n",
    "for cat in sorted(liste_categories):\n",
    "    sous_df = df[df[target_col] == cat]\n",
    "    moyenne = sous_df[\"nb_digits_text\"].mean()\n",
    "    mediane = sous_df[\"nb_digits_text\"].median()\n",
    "    minimum = sous_df[\"nb_digits_text\"].min()\n",
    "    maximum = sous_df[\"nb_digits_text\"].max()\n",
    "\n",
    "    resultats.append({\n",
    "        \"category\": cat,\n",
    "        \"mean_nb_digits\": moyenne,\n",
    "        \"median_nb_digits\": mediane,\n",
    "        \"min_nb_digits\": minimum,\n",
    "        \"max_nb_digits\": maximum\n",
    "    })\n",
    "\n",
    "stats_digits = pd.DataFrame(resultats)\n",
    "\n",
    "stats_digits = stats_digits.sort_values(\"mean_nb_digits\", ascending=False)\n",
    "\n",
    "stats_digits.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRQMDaiSwj5d"
   },
   "source": [
    "R√©sultat: nous constatons que les moyennes sont tr√®s differentes en fonction des cat√©gories (46.5 pour les jeux PC en t√©l√©chargement contre 15 pour la cat√©gorie Bureau&Papeterie) donc \"nb_digits_text\" est bien une feature discriminantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC_bBz1i-i_m"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.bar(\n",
    "    stats_digits[\"category\"].astype(str),\n",
    "    stats_digits[\"mean_nb_digits\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Cat√©gorie\")\n",
    "plt.ylabel(\"Moyenne de chiffres\\n(titre + description)\")\n",
    "plt.title(\"Moyenne de chiffres par cat√©gorie\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiUMuz-4-5x1"
   },
   "source": [
    "Les cat√©gories contenant le plus de chiffres (Jeux PC en t√©l√©chargement, Mod√©lisme&Drone) contiennent g√©n√©ralement des r√©f√©rences techniques, produit, tailles de fichiers, version ou encore des dimensions.\n",
    "\n",
    "Les cat√©gories contenant le moins de chiffres ont des description ax√©es litterature (Livres sp√©cialis√©s, Litt√©rature).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUd-jAiWzbR0"
   },
   "source": [
    "\n",
    "### 7.2 D√©tection d'unit√©s par cat√©gorie\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7yYUxLFyubw"
   },
   "source": [
    "Objectif: identifier, pour chaque description produit, la pr√©sence d‚Äôunit√©s de mesure (cm, mm, kg, L, ml, W, V, Go, etc.), puis analyser leur distribution par cat√©gorie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCGfnCCUKWAV"
   },
   "source": [
    "a) Dictionnaire des unit√©s de mesure pr√©sentes dans les textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwwlrVxN0RX5"
   },
   "outputs": [],
   "source": [
    "\n",
    "unit_patterns = {\n",
    "    # Dimensions\n",
    "    \"cm\":   r\"\\b\\d+\\s*(cm|centimetre?s?|centim√®tre?s?)\\b\",\n",
    "    \"mm\":   r\"\\b\\d+\\s*(mm|millimetre?s?|millim√®tre?s?)\\b\",\n",
    "    \"m\":    r\"\\b\\d+\\s*(m|metre?s?|m√®tre?s?)\\b\",\n",
    "\n",
    "    # Poids\n",
    "    \"kg\":   r\"\\b\\d+\\s*(kg|kilo|kilogramme?s?)\\b\",\n",
    "    \"g\":    r\"\\b\\d+\\s*(g|gramme?s?)\\b\",\n",
    "\n",
    "    # Volume\n",
    "    \"ml\":   r\"\\b\\d+\\s*(ml|millilitres?|millilitre?)\\b\",\n",
    "    \"l\":    r\"\\b\\d+\\s*(l|litres?|litre?)\\b\",\n",
    "    \"cl\":   r\"\\b\\d+\\s*cl\\b\",  # ex : 50cl\n",
    "\n",
    "    # Dimensions √©ventuellement suivies d'une unit√©\n",
    "    \"x_dim\": r\"\\b\\d+\\s*(x|√ó)\\s*\\d+(\\s*(cm|mm|m))?\\b\",\n",
    "\n",
    "    # √Çge\n",
    "    \"age_ans\":  r\"\\b\\d+\\s*ans\\b\",\n",
    "    \"age_mois\": r\"\\b\\d+\\s*mois\\b\",\n",
    "\n",
    "    # Pouces (√©crans, certains produits tech)\n",
    "    \"inch\": r'\\b\\d+\\s*(\\\"|pouces?|po)\\b',\n",
    "}\n",
    "\n",
    "compiled_patterns = []\n",
    "for pattern in unit_patterns.values():\n",
    "    regex_obj = re.compile(pattern, flags=re.IGNORECASE)\n",
    "    compiled_patterns.append(regex_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2D-VmwfF_xFg"
   },
   "outputs": [],
   "source": [
    "def detect_any_unit(text, list_of_regex):\n",
    "    \"\"\"\n",
    "    Retourne 1 si le texte contient au moins une unit√© (parmi la liste de regex),\n",
    "    sinon 0.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "\n",
    "    for regex_pattern in list_of_regex:\n",
    "        if regex_pattern.search(text):\n",
    "            return 1\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTaE2nJWCOGT"
   },
   "outputs": [],
   "source": [
    "df[\"has_any_unit\"] = df[\"text_clean\"].apply(\n",
    "    lambda txt: detect_any_unit(txt, compiled_patterns)\n",
    ")\n",
    "\n",
    "df[[\"text_clean\", \"has_any_unit\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1u_xRNAKoDd"
   },
   "source": [
    "b) Fr√©quence des unit√©s par cat√©gorie produit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwI1jwF12OCh"
   },
   "source": [
    "Ici, on cherche √† savoir dans chaque cat√©gorie, quel pourcentage de produits contient au moins une unit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyhGOK4GCX83"
   },
   "outputs": [],
   "source": [
    "target_col = \"category\"\n",
    "\n",
    "liste_categories = sorted(df[target_col].unique())\n",
    "\n",
    "resultats = []\n",
    "\n",
    "for cat in liste_categories:\n",
    "    sous_df = df[df[target_col] == cat]\n",
    "\n",
    "    nb_produits = len(sous_df)\n",
    "    if nb_produits == 0:\n",
    "        continue\n",
    "\n",
    "    taux_unites = sous_df[\"has_any_unit\"].mean()\n",
    "    pourcentage = taux_unites * 100\n",
    "\n",
    "    resultats.append({\n",
    "        \"categorie\": cat,\n",
    "        \"nb_products\": nb_produits,\n",
    "        \"pct_products_with_unit\": pourcentage\n",
    "    })\n",
    "\n",
    "stats_units_any = pd.DataFrame(resultats)\n",
    "\n",
    "# Affichage des cat√©gories par pourcentage d√©croissant\n",
    "stats_units_any = stats_units_any.sort_values(\n",
    "    \"pct_products_with_unit\",\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "stats_units_any.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWZYhvCo2zX3"
   },
   "source": [
    "Nous constatons que la cat√©gorie \"Textiles d'int√©rieur\" contient quasi syst√©matiquement des mesures(90%) contrairement √† d'autres comme Jeux de r√¥le& Figurines(15%). La feature \"has_any_unit\" est donc tr√®s caract√©ristique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7cIYT9i1kDS"
   },
   "source": [
    "c) Affichage de toutes les cat√©gories en fonction du nombre d'unit√©s quelles poss√®dent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nV4reBFK1Vsz"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "plot_df = stats_units_any.sort_values(\"pct_products_with_unit\", ascending=True)\n",
    "\n",
    "plt.barh(\n",
    "    plot_df[\"categorie\"].astype(str),\n",
    "    plot_df[\"pct_products_with_unit\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Pourcentage de produits avec au moins une unit√© (%)\")\n",
    "plt.ylabel(\"Cat√©gorie\")\n",
    "plt.title(\"Pourcentage de produits contenant des unit√©s par cat√©gorie\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ8il1of3ziA"
   },
   "source": [
    "R√©sultat: nous constatons que les cat√©gories se distinguent bien:\n",
    "\n",
    "*   V√™tement B√©b√© & Loisirs (73 %), coh√©rent avec les √¢ges (ans, mois) et parfois dimensions.\n",
    "*   Bricolage & Outillage (72 %),  mm, cm, W, V, etc., tr√®s coh√©rent √©galement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brUWDNvQ4wtj"
   },
   "source": [
    "d) Combinaison du nombre de chiffres et d'unit√©s par cat√©gorie\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BN2EIk7kiTL6"
   },
   "outputs": [],
   "source": [
    "target_col = \"category\"\n",
    "\n",
    "liste_categories = sorted(df[target_col].unique())\n",
    "\n",
    "resultats_cat = []\n",
    "\n",
    "for cat in liste_categories:\n",
    "    sous_df = df[df[target_col] == cat]\n",
    "\n",
    "    nb_produits = len(sous_df)\n",
    "    if nb_produits == 0:\n",
    "        continue\n",
    "\n",
    "    mean_nb_digits = sous_df[\"nb_digits_text\"].mean()\n",
    "\n",
    "\n",
    "    taux_unites = sous_df[\"has_any_unit\"].mean()\n",
    "    pct_with_unit = taux_unites * 100\n",
    "\n",
    "    resultats_cat.append({\n",
    "        \"category\": cat,\n",
    "        \"nb_products\": nb_produits,\n",
    "        \"mean_nb_digits\": mean_nb_digits,\n",
    "        \"pct_with_unit\": pct_with_unit\n",
    "    })\n",
    "\n",
    "stats_cat = pd.DataFrame(resultats_cat)\n",
    "\n",
    "stats_cat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgpizPaeiZr2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 11))\n",
    "\n",
    "plt.scatter(\n",
    "    stats_cat[\"mean_nb_digits\"],\n",
    "    stats_cat[\"pct_with_unit\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Nombre moyen de chiffres (titre + description)\")\n",
    "plt.ylabel(\"Pourcentage de produits avec unit√©s (%)\")\n",
    "plt.title(\"Cat√©gories : technicit√© (chiffres) vs usage d'unit√©s\")\n",
    "\n",
    "seuil_pct = 70\n",
    "seuil_digits = stats_cat[\"mean_nb_digits\"].quantile(0.9)\n",
    "\n",
    "for _, row in stats_cat.iterrows():\n",
    "    x = row[\"mean_nb_digits\"]\n",
    "    y = row[\"pct_with_unit\"]\n",
    "    nom_cat = str(row[\"category\"])\n",
    "\n",
    "    if (y > seuil_pct) or (x > seuil_digits):\n",
    "        plt.text(x + 0.1, y + 1, nom_cat, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyjcoGmdH5L-"
   },
   "source": [
    "Plus une cat√©gorie est technique plus elle est situ√©e vers le haut √† droite. Les cat√©gories les moins techniques sont situ√©s vers le bas √† gauche.\n",
    "\n",
    "Les jeux PC en t√©l√©chargement ont beaucoup de chiffres et peu d'unit√©s, cela peut correspondre √† des versions, num√©ros de produits.\n",
    "\n",
    "Les produits techniques comportant des souvent dimensions (piscines&accessoires, bricolage&outillage) ont des descriptions qui conrrespondent √† des sp√©cifications produits. Leur localisation correspondent bien √† leur typologie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqAkWAGAFDnZ"
   },
   "source": [
    "### 7.3 Analyse de la relation entre taille/pointure et √¢ge par cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOxoNCzgEUEz"
   },
   "outputs": [],
   "source": [
    "stats_patterns = (\n",
    "    df.groupby(\"category\")\n",
    "      .agg(\n",
    "          nb_products=(\"category\", \"size\"),\n",
    "          pct_with_clothing_size=(\"has_clothing_size\", \"mean\"),\n",
    "          pct_with_age_range=(\"has_age_range\", \"mean\"),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "stats_patterns[\"pct_with_clothing_size\"] *= 100\n",
    "stats_patterns[\"pct_with_age_range\"] *= 100\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(\n",
    "    stats_patterns[\"pct_with_clothing_size\"],\n",
    "    stats_patterns[\"pct_with_age_range\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"% de produits avec tailles / pointures\")\n",
    "plt.ylabel(\"% de produits avec mention d'√¢ge (mois / ans)\")\n",
    "plt.title(\"Cat√©gories : tailles v√™tements vs mentions d'√¢ge\")\n",
    "\n",
    "for _, row in stats_patterns.iterrows():\n",
    "    x = row[\"pct_with_clothing_size\"]\n",
    "    y = row[\"pct_with_age_range\"]\n",
    "    nom_cat = str(row[\"category\"])   # on utilise la colonne 'category'\n",
    "\n",
    "    plt.text(x + 0.5, y + 0.5, nom_cat, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ft4C1eZFHnM"
   },
   "source": [
    "Les cat√©gories contenant des produits g√©n√©ralement class√©s par √¢ge se distinguent: v√™tement b√©b√© & loisirs, jeux √©ducatifs, pu√©riculture contrairement aux cat√©gories \"animaux et  textiles d'int√©rieur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiG_5KhIfIBI"
   },
   "source": [
    "\n",
    "<h3 align=\"center\">\n",
    "7.4 Pr√©sence de num√©ro dans la description produit\n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RJQuD63fLzp"
   },
   "source": [
    "Objectif: d√©tecter les mentions de type \"n¬∞4838\" dans les descriptions produit afin d'identifier les cat√©gories o√π la notion de num√©ro est fr√©quente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi0pHChCepHA"
   },
   "outputs": [],
   "source": [
    "df[\"contains_numerotation\"] = df[\"text_clean\"].str.contains(r\"n¬∞ ?([0-9])+\")\n",
    "num_counts = (df[df[\"contains_numerotation\"]][\"category\"].value_counts() / df[\"category\"].value_counts()).fillna(0).sort_values(ascending=False)\n",
    "sns.barplot(num_counts, orient='h')\n",
    "plt.xlabel('proportion');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVQOykm-flv5"
   },
   "source": [
    "Nous constatons que certains produits poss√®dent des num√©ros de parution, cela concerne plus de 80% de la cat√©gorie \"Presse&Magazine\".\n",
    "\n",
    "R√©sultat: la feature contains_numerotation est tr√®s sp√©cifique des produits de type presse et est donc discriminant donc pertinent √† prendre en compte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fbhKvjjP6kr"
   },
   "source": [
    "En conclusion, les features d√©riv√©es des chiffres, des unit√©s et des\n",
    "mentions d‚Äô√¢ge / taille / pointure apportent une distinction forte, elles permettent de mieux s√©parer les familles de produits et de comprendre ce que le mod√®le pourra exploiter au-del√† de TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhitA1u2lWkv"
   },
   "outputs": [],
   "source": [
    "y.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "187hpHv9lZGs"
   },
   "outputs": [],
   "source": [
    "# exemple; on affiche les 5 premi√®res entr√©es de la cat√©gorie 2583\n",
    "\n",
    "cat = 1301\n",
    "display_df(df[y == cat], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0fmKCYJlcna"
   },
   "source": [
    "### 7.5 Tester l'impact de variables indicatrices sur la pr√©sence ou non de certains mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6qcuucANum6"
   },
   "source": [
    "Avec cette feature, nous  cherchons √† mesurer l‚Äôimpact de variables indicatrices simples bas√©es sur la pr√©sence ou l‚Äôabsence de certains mots cl√©s dans les descriptions produits. L‚Äôobjectif est de v√©rifier si ces marqueurs textuels apportent une information compl√©mentaire. Nous examinerons ainsi dans quelle mesure ces features peuvent aider √† mieux diff√©rencier certaines cat√©gories de produits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEDihS3fqrIO"
   },
   "outputs": [],
   "source": [
    "df[df.detected_lang == \"fr\"]\n",
    "df= df[df.detected_lang == \"fr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIIC78LUOUcX"
   },
   "source": [
    "a) Cr√©ation des variables de la feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjkHd5iFqvoY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Definir les mots cl√©s par cat√©gorie\n",
    "\n",
    "keyword_dict = {\n",
    "    \"Animaux\": [ \"chien\", \"chat\", \"animal\", \"compagnie\", \"collier\"],\n",
    "    \"Bureau & Papeterie\": [\"verso\", \"cahier\", \"encre\", \"papier\", \"recto\", \"a5\"],\n",
    "    \"√âpicerie\": [\"epices\", \"ar√¥me\", \"chocolat\", \"sucre\", \"sachet\", \"capsule\"],\n",
    "    \"Pu√©riculture\": [\"langer\", \"bavoir\", \"assiette\", \"siege\", \"t√©tine\", \"poussette\"],\n",
    "    \"V√™tement B√©b√© & Loisirs\": [\"b√©b√©\", \"chaussettes\", \"paire\", \"longueur\", \"filles\",\"gar√ßons\"],\n",
    "    \"Figurines\": [\"figurine\", \"gundam\", \"statuette\", \"officiel\", \"marvel\", \"funko\"],\n",
    "    \"Jeux de cartes\":[\"mtg\", \"oh\", \"rare\", \"vf\", \"carte\", \"magic\"],\n",
    "    \"Jeux de r√¥le & Figurines\": [\"halloween\", \"figurine\", \"warhammer\", \"prince\", \"masque\"],\n",
    "    \"Bricolage & Outillage\": [\"arrosage\", \"tondeuse\", \"aspirateur\", \"appareils\", \"outil\", \"coupe\", \"b√¢che\"],\n",
    "    \"D√©coration & √âquipement Jardin\": [\"bois\", \"jardin\", \"r√©sistant\", \"tente\", \"parasol\", \"aluminium\"],\n",
    "    \"Piscine & Accessoires\": [\"piscine\", \"filtration\", \"pompe\", \"dimensions\",\"eau\", \"ronde\"],\n",
    "    \"Accessoires & P√©riph√©riques\":[\"nintendo\", \"manette\", \"protection\", \"ps4\", \"silicone\", \"c√¢ble\"],\n",
    "    \"Consoles\": [\"console\", \"oui\", \"jeu\", \"√©cran\", \"portable\", \"marque\", \"jeux\"],\n",
    "    \"Jeux PC en T√©l√©chargement\":[\"windows\", \"jeu\", \"directx\", \"plus\", \"t√©l√©chargement\", \"disque\", \"√©dition\"],\n",
    "    \"Jeux Vid√©o Modernes\": [\"duty\",\"jeux\", \"manettes\", \"ps3\", \"xbox\", \"kinect\"],\n",
    "    \"R√©tro Gaming\": [\"japonais\", \"import\", \"langue\", \"titres\", \"sous\", \"fran√ßais\"],\n",
    "    \"Jeux √©ducatifs\": [\"joue\", \"cartes\", \"enfants\", \"√©ducatif\", \"bois\", \"jouer\"],\n",
    "    \"Jouets & Figurines\": [\"doudou\", \"enfants\", \"cadeau\", \"peluche\", \"jouet\", \"puzzle\"],\n",
    "    \"Loisirs & Plein air\": [\"camping\", \"p√™che\", \"stress\", \"stream\", \"bracelet\", \"trampoline\"],\n",
    "    \"Mod√©lisme & Drones\": [\"drone\", \"g√©n√©rique\", \"dji\", \"avion\", \"batterie\", \"c√°mera\", \"one\"],\n",
    "    \"Litt√©rature\": [\"monde\", \"ouvrage\", \"si√®cle\", \"roman\", \"livre\", \"histoire\", \"tome\"],\n",
    "    \"Livres sp√©cialis√©s\": [\"guide\", \"√©dition\", \"histoire\", \"art\", \"collection\"],\n",
    "    \"Presse & Magazines\": [\"journal\", \"france\", \"illustre\", \"magazine\", \"presse\", \"revue\"],\n",
    "    \"S√©ries & Encyclop√©dies\":[ \"lot\", \"livres\", \"tomes\", \"volumes\", \"tome\", \"revues\"],\n",
    "    \"D√©coration & Lumi√®res\": [\"led\", \"no√´l\", \"lumi√®re\", \"lampe\", \"d√©coration\", \"couleur\"],\n",
    "    \"Textiles d'int√©rieur\": [\"oreiller\", \"taie\", \"coussin\", \"couverture\", \"canap√©\", \"cotton\"],\n",
    "     \"√âquipement Maison\":[\"matelas\", \"assise\", \"bois\", \"table\", \"hauteur\", \"mousse\"]\n",
    "\n",
    "}\n",
    "\n",
    "# Construire un texte global\n",
    "df[\"text\"] = (df[\"designation_cleaned\"].fillna(\"\") + \" \" +df[\"description_cleaned\"].fillna(\"\")).str.lower()\n",
    "\n",
    "# G√©n√©ration des colonnes de comptage\n",
    "\n",
    "data = {}\n",
    "\n",
    "for cat, mots in keyword_dict.items():\n",
    "    pattern = '|'.join(mots)  # expression r√©guli√®re\n",
    "    data[cat + \"_keywords\"] = df[\"text\"].str.count(pattern)\n",
    "\n",
    "# Convertir en DataFrame final\n",
    "new_X = pd.DataFrame(data)\n",
    "new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtbFS1lvqzBc"
   },
   "outputs": [],
   "source": [
    "# keywords / cat√©gorie\n",
    "total_keywords_per_category = new_X.sum().sort_values(ascending = False)\n",
    "total_keywords_per_category\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qz8TUkpiOZBP"
   },
   "source": [
    "b) Analyse de la distribution par cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-VzeCS3q2_h"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "y = df['category']\n",
    "for keyword in [  \"assise\"]:\n",
    "    count_kw = df[\"text\"].str.lower().str.count(keyword)\n",
    "    cat_counts = y.value_counts()\n",
    "    freq = {}\n",
    "    for cat, count in cat_counts.items():\n",
    "        freq[cat] = count_kw[y==cat].sum() / count\n",
    "    sns.barplot(y=freq.keys(), x=freq.values(), orient='h')\n",
    "    plt.title(keyword)\n",
    "    plt.show()\n",
    "\n",
    " #   \"Textiles d'int√©rieur\": [\"oreiller\", \"taie\", \"coussin\", \"couverture\", \"canap√©\", \"cotton\"] \"√âquipement Maison\":\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I52O1y28q5md"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cr√©er un DataFrame bool√©en : True si au moins 1 mot-cl√©\n",
    "has_keyword = new_X >= 1\n",
    "\n",
    "# Calculer le pourcentage de produits avec ‚â•1 mot-cl√© par cat√©gorie\n",
    "percentage_per_category = has_keyword.sum() / len(new_X) * 100\n",
    "\n",
    "# Trier de mani√®re d√©croissante\n",
    "percentage_per_category = percentage_per_category.sort_values(ascending=False)\n",
    "\n",
    "# Graphique en barres\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(percentage_per_category.index, percentage_per_category.values, color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Pourcentage de produits (%)\")\n",
    "plt.title(\"Pourcentage de produits avec au moins 1 mot-cl√© par cat√©gorie\")\n",
    "plt.ylim(0, 100)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czdGxx5vrnIQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Cat√©gorie dominante de mots-cl√©s par produit\n",
    "categorie_motcle_dominante = new_X.idxmax(axis=1)\n",
    "\n",
    "# Tableau crois√© : lignes = cat√©gorie officielle, colonnes = cat√©gorie dominante de mots-cl√©s\n",
    "distribution = pd.crosstab(df['category'], categorie_motcle_dominante)\n",
    "\n",
    "# --- Palette de 27 couleurs bien distinctes ---\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combinaison de palettes tr√®s contrast√©es\n",
    "palette1 = plt.cm.tab20.colors          # 20 couleurs\n",
    "palette2 = plt.cm.tab10.colors          # 10 couleurs\n",
    "palette = list(chain(palette1, palette2))[:27]  # 27 couleurs bien diff√©rentes\n",
    "\n",
    "# --- Graphique en barres empil√©es ---\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "bottom = np.zeros(len(distribution))\n",
    "categories_officielles = distribution.index\n",
    "\n",
    "for i, kw_cat in enumerate(distribution.columns):\n",
    "    ax.bar(\n",
    "        categories_officielles,\n",
    "        distribution[kw_cat],\n",
    "        bottom=bottom,\n",
    "        label=kw_cat,\n",
    "        color=palette[i]  # couleurs distinctes\n",
    "    )\n",
    "    bottom += distribution[kw_cat].values\n",
    "\n",
    "ax.set_ylabel(\"Nombre de produits\")\n",
    "ax.set_title(\"Distribution des cat√©gories de mots-cl√©s dominantes par cat√©gorie officielle\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiBif_L3S_Ri",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IX. Dataset final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFeRQtppa0BB"
   },
   "source": [
    "a) Int√©gration des nouvelles features dans le DataFrame final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GUKIT-Na733"
   },
   "outputs": [],
   "source": [
    "# Colonnes de base pour la mod√©lisation\n",
    "base_cols = [\n",
    "    \"prdtypecode\",\n",
    "    \"designation_cleaned\",\n",
    "    \"description_cleaned\",\n",
    "    \"text_clean\",\n",
    "    \"dup_count\",\n",
    "    \"is_duplicated_group\",\n",
    "]\n",
    "\n",
    "# Features cr√©es\n",
    "created_features = [\n",
    "    \"nb_digits_text\",\n",
    "    \"has_any_unit\",\n",
    "    \"contains_numerotation\",\n",
    "]\n",
    "\n",
    "# Construction de la liste finale des colonnes\n",
    "cols_for_modeling = base_cols + created_features + list(new_X.columns)\n",
    "\n",
    "# Jointure et s√©lection des colonnes\n",
    "df_final = df.join(new_X)[cols_for_modeling].copy()\n",
    "\n",
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oldvO1Qa2vY"
   },
   "source": [
    " b) Export du dataset enrichi pour les notebooks de mod√©lisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBIm-dOvbcIr"
   },
   "source": [
    "Maintenant que les features sont consolid√©es, il est essentiel d‚Äôexporter le dataset afin qu‚Äôil puisse √™tre utilis√© dans les notebooks d√©di√©s √† la mod√©lisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sblaoHfmb6ml"
   },
   "outputs": [],
   "source": [
    "df_final.to_csv(\"rakuten_features_v1.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UOjx1XFNPIs",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# X. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook a permis de pr√©parer et de structurer le texte du dataset Rakuten. Nous disposons d√©sormais d‚Äôun jeu de donn√©es propre, enrichi et pr√™t pour la mod√©lisation.\n",
    "\n",
    "Les √©l√©ments cl√©s mis en place dans ce notebook sont :\n",
    "\n",
    "- le nettoyage du texte : harmonisation des champs, r√©duction du bruit, pr√©servation des informations utiles,\n",
    "\n",
    "- analyse lexicale : mise en √©vidence de motifs discriminants (unit√©s, nombres, termes techniques),\n",
    "\n",
    "-gestion des doublons: r√©duction des biais li√©s aux r√©p√©titions et incoh√©rences,\n",
    "\n",
    "-cr√©ation de features: signaux compl√©mentaires √† TF-IDF pour mieux capturer les sp√©cificit√©s des cat√©gories.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Difficult√©s et biais identifi√©s:\n",
    "\n",
    "- forte variabilit√© et bruit dans les textes\n",
    "\n",
    "- classes d√©s√©quilibr√©es\n",
    "\n",
    "- description absente pour une part importante des produits\n",
    "\n",
    "- doublons fr√©quents\n",
    "\n",
    "- h√©t√©rog√©n√©it√© linguistique (syntaxe, graphies, niveaux de d√©tails)\n",
    "\n",
    "- erreurs ou impr√©cisions de labellisation\n",
    "\n",
    "    Ces constats constituent des points d‚Äôattention pour l'√©tape de mod√©lisation.\n",
    "Nous allons maintenant poursuivre avec l‚Äôanalyse des images dans un second notebook.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
