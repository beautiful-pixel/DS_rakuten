{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Rakuten - ConvNeXt (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q timm gdown pandas scikit-learn torch torchvision tqdm\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import GradScaler\n",
    "from timm.utils import ModelEmaV2\n",
    "import json\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Téléchargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargement des CSVs avec gdown\n",
    "print(\"Downloading CSV data...\")\n",
    "\n",
    "# Download X_train\n",
    "X_TRAIN_FILE_ID = \"1geSiJTTjamysiSbJ8-W9gR1kv-x6HyEd\"\n",
    "gdown.download(id=X_TRAIN_FILE_ID, output='/content/X_train.csv', quiet=False)\n",
    "X_train_full = pd.read_csv('/content/X_train.csv')\n",
    "\n",
    "# Download y_train\n",
    "Y_TRAIN_FILE_ID = \"16czWmLR5Ff0s5aYIqy1rHT7hc6Gcpfw3\"\n",
    "gdown.download(id=Y_TRAIN_FILE_ID, output='/content/y_train.csv', quiet=False)\n",
    "y_train_full = pd.read_csv('/content/y_train.csv')\n",
    "\n",
    "print(f\"Total data loaded: {len(X_train_full):,} samples\")\n",
    "\n",
    "# Constantes pour les splits unifiés (identiques au projet)\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPLITTING DATA - Unified Project Splits\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Génération des splits unifiés (même logique que generate_splits)\n",
    "y = y_train_full['prdtypecode'].to_numpy()\n",
    "indices = np.arange(len(y))\n",
    "\n",
    "# Première division: full_train (85%) / test (15%)\n",
    "full_train_idx, test_idx = train_test_split(\n",
    "    indices, test_size=TEST_SIZE, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "# Deuxième division: train (85% de full_train) / val (15% de full_train)\n",
    "train_idx, val_idx = train_test_split(\n",
    "    full_train_idx, test_size=VAL_SIZE, random_state=SEED, stratify=y[full_train_idx]\n",
    ")\n",
    "\n",
    "# Créer les DataFrames\n",
    "train_df = X_train_full.iloc[train_idx].copy()\n",
    "train_df['prdtypecode'] = y[train_idx]\n",
    "\n",
    "val_df = X_train_full.iloc[val_idx].copy()\n",
    "val_df['prdtypecode'] = y[val_idx]\n",
    "\n",
    "df_test = X_train_full.iloc[test_idx].copy()\n",
    "df_test['prdtypecode'] = y[test_idx]\n",
    "\n",
    "print(f\"✓ Train: {len(train_df):,} samples ({len(train_df)/len(y)*100:.2f}%)\")\n",
    "print(f\"✓ Val: {len(val_df):,} samples ({len(val_df)/len(y)*100:.2f}%)\")\n",
    "print(f\"✓ Test: {len(df_test):,} samples ({len(df_test)/len(y)*100:.2f}%)\")\n",
    "print(f\"✓ Classes: {train_df['prdtypecode'].nunique()}\")\n",
    "print(\"\\n⚠️  CRITICAL: Test set will ONLY be used for final evaluation!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FILE_ID = \"15ZkS0iTQ7j3mHpxil4mABlXwP-jAN_zi\"\n",
    "\n",
    "if not os.path.exists(\"/content/images\"):\n",
    "    os.makedirs(\"/content/tmp\", exist_ok=True)\n",
    "    os.makedirs(\"/content/images\", exist_ok=True)\n",
    "    !gdown --id $IMAGE_FILE_ID -O /content/tmp/images.zip\n",
    "    !unzip -q -o /content/tmp/images.zip -d /content/images\n",
    "\n",
    "IMG_ROOT = \"/content/images/images/image_train\"\n",
    "print(f\"Images: {IMG_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des labels (sur train uniquement)\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['prdtypecode'])\n",
    "\n",
    "train_df['encoded_label'] = le.transform(train_df['prdtypecode'])\n",
    "val_df['encoded_label'] = le.transform(val_df['prdtypecode'])\n",
    "df_test['encoded_label'] = le.transform(df_test['prdtypecode'])\n",
    "\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "\n",
    "print(f\"Nombre de classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RakutenImageDataset(Dataset):\n",
    "    def __init__(self, df, img_root, transform=None):\n",
    "        self.image_ids = df['imageid'].tolist()\n",
    "        self.product_ids = df['productid'].tolist()\n",
    "        self.labels = df['encoded_label'].tolist()\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = f\"image_{self.image_ids[idx]}_product_{self.product_ids[idx]}.jpg\"\n",
    "        img_path = os.path.join(self.img_root, img_name)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            image = Image.new('RGB', (384, 384), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modèle ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RakutenConvNeXt(nn.Module):\n",
    "    def __init__(self, model_name='convnext_base', num_classes=27, \n",
    "                 pretrained=True, drop_path_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=0,\n",
    "            global_pool='avg', drop_path_rate=drop_path_rate\n",
    "        )\n",
    "        \n",
    "        feature_dim = self.backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "print(\"Modèle ConvNeXt défini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datetime import datetime\n\n# Timestamp pour versioning\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\nCONFIG = {\n    \"model_name\": \"convnext_base\",\n    \"img_size\": 384,\n    \"num_classes\": NUM_CLASSES,\n    \"batch_size\": 64,  # Optimisé pour A100 + 384x384\n    \"num_epochs\": 30,\n    \"learning_rate\": 1e-4,\n    \"weight_decay\": 0.05,\n    \"drop_path_rate\": 0.3,\n    \"mixup_alpha\": 0.8,\n    \"cutmix_alpha\": 1.0,\n    \"label_smoothing\": 0.1,\n    \"use_ema\": True,\n    \"ema_decay\": 0.9999,\n    \"early_stopping_patience\": 5,\n    \"use_amp\": True,\n    \"num_workers\": 2,\n    \"timestamp\": timestamp\n}\n\nprint(\"=\"*80)\nprint(\"CONVNEXT TRAINING CONFIGURATION (Colab A100)\")\nprint(\"=\"*80)\nprint(f\"Model: {CONFIG['model_name']}\")\nprint(f\"Image Size: {CONFIG['img_size']}x{CONFIG['img_size']}\")\nprint(f\"Batch Size: {CONFIG['batch_size']} (optimized for A100 + 384x384)\")\nprint(f\"Epochs: {CONFIG['num_epochs']}\")\nprint(f\"Learning Rate: {CONFIG['learning_rate']}\")\nprint(f\"EMA: {CONFIG['use_ema']} (decay={CONFIG['ema_decay']})\")\nprint(f\"AMP: {CONFIG['use_amp']}\")\nprint(f\"Timestamp: {timestamp}\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(384, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(438),\n",
    "    transforms.CenterCrop(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = RakutenImageDataset(train_df, IMG_ROOT, train_transform)\n",
    "val_dataset = RakutenImageDataset(val_df, IMG_ROOT, val_transform)\n",
    "test_dataset = RakutenImageDataset(df_test, IMG_ROOT, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True,\n",
    "                          num_workers=CONFIG[\"num_workers\"], pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                        num_workers=CONFIG[\"num_workers\"], pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                         num_workers=CONFIG[\"num_workers\"], pin_memory=True)\n",
    "\n",
    "print(f\"Batches - Train: {len(train_loader)} | Val: {len(val_loader)} | Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "\n",
    "model = RakutenConvNeXt(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    num_classes=NUM_CLASSES,\n",
    "    drop_path_rate=CONFIG[\"drop_path_rate\"]\n",
    ").to(device)\n",
    "\n",
    "model_ema = ModelEmaV2(model, decay=CONFIG[\"ema_decay\"]) if CONFIG[\"use_ema\"] else None\n",
    "\n",
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=CONFIG[\"mixup_alpha\"], cutmix_alpha=CONFIG[\"cutmix_alpha\"],\n",
    "    prob=1.0, switch_prob=0.5, mode='batch',\n",
    "    label_smoothing=CONFIG[\"label_smoothing\"], num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "criterion_train = SoftTargetCrossEntropy()\n",
    "criterion_val = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"],\n",
    "                              weight_decay=CONFIG[\"weight_decay\"])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG[\"num_epochs\"],\n",
    "                                                        eta_min=1e-6)\n",
    "scaler = GradScaler() if CONFIG[\"use_amp\"] else None\n",
    "\n",
    "print(\"Initialisation terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Évaluation\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            if CONFIG[\"use_amp\"]:\n",
    "                with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            all_preds.extend(torch.argmax(outputs, dim=-1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = val_loss / len(loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return avg_loss, acc * 100, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": [], \"ema_val_acc\": []}\n",
    "\n",
    "# Nom du fichier avec timestamp\n",
    "model_filename = f\"convnext_best_{CONFIG['timestamp']}.pth\"\n",
    "\n",
    "for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=\"Entrainement\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images, labels = mixup_fn(images, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if CONFIG[\"use_amp\"]:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "                loss = criterion_train(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion_train(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion_val)\n",
    "    ema_val_acc = 0.0\n",
    "    if model_ema is not None:\n",
    "        _, ema_val_acc, _ = evaluate(model_ema.module, val_loader, criterion_val)\n",
    "\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"ema_val_acc\"].append(ema_val_acc)\n",
    "\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Val Acc: {val_acc:.2f}% | EMA: {ema_val_acc:.2f}%\")\n",
    "\n",
    "    current_best_acc = max(val_acc, ema_val_acc)\n",
    "    if current_best_acc > best_val_acc:\n",
    "        best_val_acc = current_best_acc\n",
    "        patience_counter = 0\n",
    "\n",
    "        save_model = model_ema.module if ema_val_acc > val_acc else model\n",
    "        is_ema = ema_val_acc > val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': save_model.state_dict(),\n",
    "            'val_acc': current_best_acc,\n",
    "            'is_ema': is_ema,\n",
    "            'timestamp': CONFIG['timestamp']\n",
    "        }, model_filename)\n",
    "        print(f\"Meilleur modele sauvegarde: {model_filename} (EMA: {is_ema})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CONFIG[\"early_stopping_patience\"]:\n",
    "            print(f\"Arret precoce apres {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(f\"\\nEntrainement termine. Meilleure Val Acc: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Évaluation finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "# Charger le meilleur modele\n",
    "model_filename = f\"convnext_best_{CONFIG['timestamp']}.pth\"\n",
    "checkpoint = torch.load(model_filename, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "is_ema = checkpoint.get('is_ema', False)\n",
    "print(f\"Modele charge: {model_filename} (EMA: {is_ema})\")\n",
    "print(f\"Epoch: {checkpoint['epoch']} | Val Acc: {checkpoint['val_acc']:.2f}%\")\n",
    "\n",
    "# =========================================================================\n",
    "# Evaluation sur VALIDATION set et export des predictions\n",
    "# =========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION SUR VALIDATION SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "val_probs_list = []\n",
    "val_labels_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        if CONFIG[\"use_amp\"]:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=-1).cpu().numpy()\n",
    "        val_probs_list.append(probs)\n",
    "        val_labels_list.append(labels.numpy())\n",
    "\n",
    "val_probs = np.vstack(val_probs_list)\n",
    "val_labels = np.concatenate(val_labels_list)\n",
    "val_preds = val_probs.argmax(axis=1)\n",
    "\n",
    "val_acc = 100.0 * accuracy_score(val_labels, val_preds)\n",
    "val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "print(f\"Val Acc: {val_acc:.2f}% | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "# Export validation predictions\n",
    "np.save(f\"convnext_probs_val_{CONFIG['timestamp']}.npy\", val_probs)\n",
    "np.save(f\"convnext_labels_val_{CONFIG['timestamp']}.npy\", val_labels)\n",
    "np.save(f\"convnext_preds_val_{CONFIG['timestamp']}.npy\", val_preds)\n",
    "print(\"Predictions validation exportees (.npy)\")\n",
    "\n",
    "# =========================================================================\n",
    "# Evaluation sur TEST set et export des predictions\n",
    "# =========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION SUR TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_probs_list = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Test\"):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        if CONFIG[\"use_amp\"]:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=-1).cpu().numpy()\n",
    "        test_probs_list.append(probs)\n",
    "        test_labels_list.append(labels.numpy())\n",
    "\n",
    "test_probs = np.vstack(test_probs_list)\n",
    "test_labels = np.concatenate(test_labels_list)\n",
    "test_preds = test_probs.argmax(axis=1)\n",
    "\n",
    "test_acc = 100.0 * accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "\n",
    "print(f\"Test Acc: {test_acc:.2f}% | Test F1: {test_f1:.4f}\")\n",
    "print(\"\\nRapport de classification (Test):\")\n",
    "print(classification_report(test_labels, test_preds, digits=4))\n",
    "\n",
    "# Export test predictions\n",
    "np.save(f\"convnext_probs_test_{CONFIG['timestamp']}.npy\", test_probs)\n",
    "np.save(f\"convnext_labels_test_{CONFIG['timestamp']}.npy\", test_labels)\n",
    "np.save(f\"convnext_preds_test_{CONFIG['timestamp']}.npy\", test_preds)\n",
    "print(\"Predictions test exportees (.npy)\")\n",
    "\n",
    "# =========================================================================\n",
    "# Sauvegarde des resultats (JSON)\n",
    "# =========================================================================\n",
    "results = {\n",
    "    \"timestamp\": CONFIG[\"timestamp\"],\n",
    "    \"model_name\": CONFIG[\"model_name\"],\n",
    "    \"best_epoch\": int(checkpoint['epoch']),\n",
    "    \"val_acc\": float(val_acc),\n",
    "    \"val_f1\": float(val_f1),\n",
    "    \"test_acc\": float(test_acc),\n",
    "    \"test_f1\": float(test_f1),\n",
    "    \"is_ema\": is_ema,\n",
    "    \"num_classes\": int(CONFIG[\"num_classes\"]),\n",
    "    \"train_samples\": len(train_df),\n",
    "    \"val_samples\": len(val_df),\n",
    "    \"test_samples\": len(df_test)\n",
    "}\n",
    "\n",
    "results_filename = f\"convnext_results_{CONFIG['timestamp']}.json\"\n",
    "with open(results_filename, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResultats sauvegardes: {results_filename}\")\n",
    "\n",
    "# =========================================================================\n",
    "# Sauvegarde sur Google Drive\n",
    "# =========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAUVEGARDE SUR GOOGLE DRIVE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "target_dir = \"/content/drive/MyDrive/Rakuten_models\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Copier le modele\n",
    "model_target = os.path.join(target_dir, model_filename)\n",
    "shutil.copy(model_filename, model_target)\n",
    "print(f\"Modele copie: {model_target}\")\n",
    "\n",
    "# Copier les fichiers npy (validation)\n",
    "for suffix in [\"probs\", \"labels\", \"preds\"]:\n",
    "    src = f\"convnext_{suffix}_val_{CONFIG['timestamp']}.npy\"\n",
    "    dst = os.path.join(target_dir, src)\n",
    "    shutil.copy(src, dst)\n",
    "    print(f\"NPY copie: {dst}\")\n",
    "\n",
    "# Copier les fichiers npy (test)\n",
    "for suffix in [\"probs\", \"labels\", \"preds\"]:\n",
    "    src = f\"convnext_{suffix}_test_{CONFIG['timestamp']}.npy\"\n",
    "    dst = os.path.join(target_dir, src)\n",
    "    shutil.copy(src, dst)\n",
    "    print(f\"NPY copie: {dst}\")\n",
    "\n",
    "# Copier le JSON\n",
    "results_target = os.path.join(target_dir, results_filename)\n",
    "shutil.copy(results_filename, results_target)\n",
    "print(f\"JSON copie: {results_target}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAUVEGARDE TERMINEE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}