{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion Swin + ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "\n",
    "print(\"Environnement initialisé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "swin_files = sorted(glob.glob(str(MODELS_DIR / \"swin\" / \"img_swin_probs_val_*.npy\")))\n",
    "convnext_files = sorted(glob.glob(str(MODELS_DIR / \"convnext\" / \"convnext_probs_val_*.npy\")))\n",
    "\n",
    "if not swin_files:\n",
    "    raise FileNotFoundError(\"Aucun fichier Swin trouve. Verifiez que le modele Swin a bien exporte les predictions.\")\n",
    "if not convnext_files:\n",
    "    raise FileNotFoundError(\"Aucun fichier ConvNeXt trouve. Verifiez que le modele ConvNeXt a bien exporte les predictions.\")\n",
    "\n",
    "# Utiliser les fichiers les plus recents (dernier dans la liste triee)\n",
    "swin_probs_file = swin_files[-1]\n",
    "convnext_probs_file = convnext_files[-1]\n",
    "\n",
    "# Extraire les timestamps des noms de fichiers\n",
    "swin_timestamp = swin_probs_file.split(\"_val_\")[-1].replace(\".npy\", \"\")\n",
    "convnext_timestamp = convnext_probs_file.split(\"_val_\")[-1].replace(\".npy\", \"\")\n",
    "\n",
    "print(f\"\\nFichiers selectionnes:\")\n",
    "print(f\"  Swin timestamp: {swin_timestamp}\")\n",
    "print(f\"  ConvNeXt timestamp: {convnext_timestamp}\")\n",
    "\n",
    "# =========================================================================\n",
    "# Chargement des predictions\n",
    "# =========================================================================\n",
    "print(\"\\nChargement des predictions...\")\n",
    "\n",
    "# Swin\n",
    "swin_probs = np.load(MODELS_DIR / \"swin\" / f\"img_swin_probs_val_{swin_timestamp}.npy\")\n",
    "swin_labels = np.load(MODELS_DIR / \"swin\" / f\"img_swin_labels_val_{swin_timestamp}.npy\")\n",
    "print(f\"Swin charge: {swin_probs.shape}\")\n",
    "\n",
    "# ConvNeXt\n",
    "convnext_probs = np.load(MODELS_DIR / \"convnext\" / f\"convnext_probs_val_{convnext_timestamp}.npy\")\n",
    "convnext_labels = np.load(MODELS_DIR / \"convnext\" / f\"convnext_labels_val_{convnext_timestamp}.npy\")\n",
    "print(f\"ConvNeXt charge: {convnext_probs.shape}\")\n",
    "\n",
    "# Verifications\n",
    "assert swin_probs.shape == convnext_probs.shape, f\"Shapes incompatibles: {swin_probs.shape} vs {convnext_probs.shape}\"\n",
    "assert np.array_equal(swin_labels, convnext_labels), \"Les labels de validation ne correspondent pas!\"\n",
    "labels = swin_labels\n",
    "\n",
    "NUM_CLASSES = swin_probs.shape[1]\n",
    "NUM_SAMPLES = swin_probs.shape[0]\n",
    "\n",
    "print(f\"\\nEchantillons de validation: {NUM_SAMPLES:,}\")\n",
    "print(f\"Classes: {NUM_CLASSES}\")\n",
    "print(\"Verification: Les deux modeles ont bien ete evalues sur les memes echantillons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance des modèles individuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swin v2\n",
    "swin_preds = swin_probs.argmax(axis=1)\n",
    "swin_acc = accuracy_score(labels, swin_preds)\n",
    "swin_f1 = f1_score(labels, swin_preds, average='weighted')\n",
    "\n",
    "print(\"Swin Transformer v2 (224×224):\")\n",
    "print(f\"  Accuracy: {swin_acc*100:.2f}%\")\n",
    "print(f\"  F1 Score: {swin_f1:.4f}\")\n",
    "\n",
    "# ConvNeXt\n",
    "convnext_preds = convnext_probs.argmax(axis=1)\n",
    "convnext_acc = accuracy_score(labels, convnext_preds)\n",
    "convnext_f1 = f1_score(labels, convnext_preds, average='weighted')\n",
    "\n",
    "print(\"\\nConvNeXt (384×384):\")\n",
    "print(f\"  Accuracy: {convnext_acc*100:.2f}%\")\n",
    "print(f\"  F1 Score: {convnext_f1:.4f}\")\n",
    "\n",
    "best_single_f1 = max(swin_f1, convnext_f1)\n",
    "best_single_name = \"ConvNeXt\" if convnext_f1 > swin_f1 else \"Swin v2\"\n",
    "print(f\"\\nMeilleur modèle: {best_single_name} (F1 = {best_single_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fusion: Moyenne Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_simple = (swin_probs + convnext_probs) / 2\n",
    "fusion_simple_preds = fusion_simple.argmax(axis=1)\n",
    "\n",
    "simple_acc = accuracy_score(labels, fusion_simple_preds)\n",
    "simple_f1 = f1_score(labels, fusion_simple_preds, average='weighted')\n",
    "\n",
    "print(\"Moyenne Simple:\")\n",
    "print(f\"  Accuracy: {simple_acc*100:.2f}%\")\n",
    "print(f\"  F1 Score: {simple_f1:.4f}\")\n",
    "print(f\"  Amélioration: {(simple_f1 - best_single_f1)*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fusion: Moyenne Pondérée (recherche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recherche du poids optimal (0 à 1, 101 étapes)...\")\n",
    "print(\"Formule: P_final = w * P_Swin + (1-w) * P_ConvNext\\n\")\n",
    "\n",
    "weights = np.linspace(0, 1, 101)\n",
    "best_weight = 0.0\n",
    "best_f1 = 0.0\n",
    "best_acc = 0.0\n",
    "\n",
    "results = []\n",
    "for w in weights:\n",
    "    fusion_probs = w * swin_probs + (1 - w) * convnext_probs\n",
    "    fusion_preds = fusion_probs.argmax(axis=1)\n",
    "    \n",
    "    acc = accuracy_score(labels, fusion_preds)\n",
    "    f1 = f1_score(labels, fusion_preds, average='weighted')\n",
    "    \n",
    "    results.append({\n",
    "        'weight': w,\n",
    "        'swin_weight': w,\n",
    "        'convnext_weight': 1 - w,\n",
    "        'accuracy': acc,\n",
    "        'f1_weighted': f1\n",
    "    })\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_acc = acc\n",
    "        best_weight = w\n",
    "\n",
    "print(f\"Poids optimal trouvé: {best_weight:.2f}\")\n",
    "print(f\"  - Contribution Swin: {best_weight:.1%}\")\n",
    "print(f\"  - Contribution ConvNeXt: {(1-best_weight):.1%}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Accuracy: {best_acc*100:.2f}%\")\n",
    "print(f\"  F1 Score: {best_f1:.4f}\")\n",
    "print(f\"  Amélioration: {(best_f1 - best_single_f1)*100:+.2f}%\")\n",
    "\n",
    "# Top 5\n",
    "results_df = pd.DataFrame(results)\n",
    "top5 = results_df.nlargest(5, 'f1_weighted')\n",
    "print(\"\\nTop 5 configurations:\")\n",
    "print(top5[['swin_weight', 'convnext_weight', 'accuracy', 'f1_weighted']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fusion: Moyenne Géométrique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_geometric = np.sqrt(swin_probs * convnext_probs)\n",
    "fusion_geometric_preds = fusion_geometric.argmax(axis=1)\n",
    "\n",
    "geometric_acc = accuracy_score(labels, fusion_geometric_preds)\n",
    "geometric_f1 = f1_score(labels, fusion_geometric_preds, average='weighted')\n",
    "\n",
    "print(\"Moyenne Géométrique:\")\n",
    "print(f\"  Accuracy: {geometric_acc*100:.2f}%\")\n",
    "print(f\"  F1 Score: {geometric_f1:.4f}\")\n",
    "print(f\"  Amélioration: {(geometric_f1 - best_single_f1)*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fusion: Moyenne Harmonique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10\n",
    "fusion_harmonic = 2 / ((1 / (swin_probs + eps)) + (1 / (convnext_probs + eps)))\n",
    "fusion_harmonic_preds = fusion_harmonic.argmax(axis=1)\n",
    "\n",
    "harmonic_acc = accuracy_score(labels, fusion_harmonic_preds)\n",
    "harmonic_f1 = f1_score(labels, fusion_harmonic_preds, average='weighted')\n",
    "\n",
    "print(\"Moyenne Harmonique:\")\n",
    "print(f\"  Accuracy: {harmonic_acc*100:.2f}%\")\n",
    "print(f\"  F1 Score: {harmonic_f1:.4f}\")\n",
    "print(f\"  Amélioration: {(harmonic_f1 - best_single_f1)*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparaison de toutes les méthodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPARAISON DES MÉTHODES DE FUSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = [\n",
    "    {'Méthode': 'Swin v2 (baseline)', 'Accuracy': f\"{swin_acc*100:.2f}%\", \n",
    "     'F1 Score': f\"{swin_f1:.4f}\", 'Amélioration': '-'},\n",
    "    {'Méthode': 'ConvNeXt (baseline)', 'Accuracy': f\"{convnext_acc*100:.2f}%\", \n",
    "     'F1 Score': f\"{convnext_f1:.4f}\", 'Amélioration': '-'},\n",
    "    {'Méthode': 'Moyenne Simple', 'Accuracy': f\"{simple_acc*100:.2f}%\", \n",
    "     'F1 Score': f\"{simple_f1:.4f}\", 'Amélioration': f\"{(simple_f1 - best_single_f1)*100:+.2f}%\"},\n",
    "    {'Méthode': f'Moyenne Pondérée (w={best_weight:.2f})', 'Accuracy': f\"{best_acc*100:.2f}%\", \n",
    "     'F1 Score': f\"{best_f1:.4f}\", 'Amélioration': f\"{(best_f1 - best_single_f1)*100:+.2f}%\"},\n",
    "    {'Méthode': 'Moyenne Géométrique', 'Accuracy': f\"{geometric_acc*100:.2f}%\", \n",
    "     'F1 Score': f\"{geometric_f1:.4f}\", 'Amélioration': f\"{(geometric_f1 - best_single_f1)*100:+.2f}%\"},\n",
    "    {'Méthode': 'Moyenne Harmonique', 'Accuracy': f\"{harmonic_acc*100:.2f}%\", \n",
    "     'F1 Score': f\"{harmonic_f1:.4f}\", 'Amélioration': f\"{(harmonic_f1 - best_single_f1)*100:+.2f}%\"}\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Meilleure méthode\n",
    "all_fusion_f1s = [simple_f1, best_f1, geometric_f1, harmonic_f1]\n",
    "best_fusion_f1 = max(all_fusion_f1s)\n",
    "best_method_idx = all_fusion_f1s.index(best_fusion_f1)\n",
    "best_methods = ['Moyenne Simple', f'Moyenne Pondérée (w={best_weight:.2f})', \n",
    "                'Moyenne Géométrique', 'Moyenne Harmonique']\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Meilleure méthode: {best_methods[best_method_idx]}\")\n",
    "print(f\"  F1 Score: {best_fusion_f1:.4f}\")\n",
    "print(f\"  Accuracy: {comparison_data[best_method_idx + 2]['Accuracy']}\")\n",
    "print(f\"  Amélioration vs meilleur modèle: {(best_fusion_f1 - best_single_f1)*100:+.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Rapport de classification détaillé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser la meilleure méthode\n",
    "if best_method_idx == 0:\n",
    "    final_probs = fusion_simple\n",
    "elif best_method_idx == 1:\n",
    "    final_probs = best_weight * swin_probs + (1 - best_weight) * convnext_probs\n",
    "elif best_method_idx == 2:\n",
    "    final_probs = fusion_geometric\n",
    "else:\n",
    "    final_probs = fusion_harmonic\n",
    "\n",
    "final_preds = final_probs.argmax(axis=1)\n",
    "\n",
    "print(f\"Rapport de classification - {best_methods[best_method_idx]}\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(labels, final_preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sauvegarde des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Timestamp pour cette execution de fusion\n",
    "fusion_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "FUSION_DIR = MODELS_DIR / \"Swin_ConvNext_Fusion\"\n",
    "FUSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sauvegarder les predictions avec timestamp\n",
    "np.save(FUSION_DIR / f\"img_fusion_probs_val_{fusion_timestamp}.npy\", final_probs)\n",
    "np.save(FUSION_DIR / f\"img_fusion_labels_val_{fusion_timestamp}.npy\", labels)\n",
    "np.save(FUSION_DIR / f\"img_fusion_preds_val_{fusion_timestamp}.npy\", final_preds)\n",
    "\n",
    "# Metadonnees avec timestamps des modeles sources\n",
    "metadata = {\n",
    "    \"fusion_timestamp\": fusion_timestamp,\n",
    "    \"swin_timestamp\": swin_timestamp,\n",
    "    \"convnext_timestamp\": convnext_timestamp,\n",
    "    \"model_1\": \"Swin Transformer v2 (224x224)\",\n",
    "    \"model_2\": \"ConvNeXt (384x384)\",\n",
    "    \"best_fusion_method\": best_methods[best_method_idx],\n",
    "    \"best_weight\": float(best_weight) if best_method_idx == 1 else None,\n",
    "    \"swin_f1\": float(swin_f1),\n",
    "    \"convnext_f1\": float(convnext_f1),\n",
    "    \"fusion_f1\": float(best_fusion_f1),\n",
    "    \"improvement_absolute\": float(best_fusion_f1 - best_single_f1),\n",
    "    \"improvement_relative_pct\": float((best_fusion_f1 / best_single_f1 - 1) * 100),\n",
    "    \"validation_samples\": int(NUM_SAMPLES),\n",
    "    \"num_classes\": int(NUM_CLASSES)\n",
    "}\n",
    "\n",
    "metadata_filename = f\"fusion_metadata_{fusion_timestamp}.json\"\n",
    "with open(FUSION_DIR / metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "comparison_filename = f\"fusion_comparison_{fusion_timestamp}.csv\"\n",
    "comparison_df.to_csv(FUSION_DIR / comparison_filename, index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FICHIERS SAUVEGARDES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Repertoire: {FUSION_DIR}\")\n",
    "print(f\"\\nPredictions:\")\n",
    "print(f\"  - img_fusion_probs_val_{fusion_timestamp}.npy\")\n",
    "print(f\"  - img_fusion_labels_val_{fusion_timestamp}.npy\")\n",
    "print(f\"  - img_fusion_preds_val_{fusion_timestamp}.npy\")\n",
    "print(f\"\\nMetadonnees:\")\n",
    "print(f\"  - {metadata_filename}\")\n",
    "print(f\"  - {comparison_filename}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Résumé final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RÉSUMÉ FINAL - FUSION SWIN + CONVNEXT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModèles fusionnés:\")\n",
    "print(f\"  1. Swin Transformer v2 (224×224) - F1 = {swin_f1:.4f}\")\n",
    "print(f\"  2. ConvNeXt (384×384)             - F1 = {convnext_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nMéthodes de fusion testées:\")\n",
    "print(f\"  1. Moyenne Simple:              F1 = {simple_f1:.4f}\")\n",
    "print(f\"  2. Moyenne Pondérée (w={best_weight:.2f}):  F1 = {best_f1:.4f}\")\n",
    "print(f\"  3. Moyenne Géométrique:         F1 = {geometric_f1:.4f}\")\n",
    "print(f\"  4. Moyenne Harmonique:          F1 = {harmonic_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nMeilleure méthode: {best_methods[best_method_idx]}\")\n",
    "print(f\"  F1 Score:  {best_fusion_f1:.4f}\")\n",
    "print(f\"  Accuracy:  {comparison_data[best_method_idx + 2]['Accuracy']}\")\n",
    "print(f\"  Amélioration:\")\n",
    "print(f\"    - Absolue: {(best_fusion_f1 - best_single_f1)*100:+.2f}%\")\n",
    "print(f\"    - Relative: {(best_fusion_f1 / best_single_f1 - 1)*100:+.2f}%\")\n",
    "\n",
    "print(f\"\\nDonnées:\")\n",
    "print(f\"  - Échantillons de validation: {NUM_SAMPLES:,}\")\n",
    "print(f\"  - Classes: {NUM_CLASSES}\")\n",
    "print(f\"  - Division: Splits unifiés du projet (data/splits/)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
