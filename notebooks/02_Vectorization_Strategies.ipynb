{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02 - Stratégies de Vectorisation\n",
    "\n",
    "**Objectif**: Trouver la meilleure méthode de représentation des features textuelles.\n",
    "\n",
    "**Méthodologie**:\n",
    "- **Modèle fixé**: Logistic Regression (contrôle de variable)\n",
    "- **Preprocessing**: `final_text_cleaner()` (from Notebook 01)\n",
    "- **Variables testées**: Vectorizer, Features, Strategy, Hyperparamètres\n",
    "\n",
    "**Output**: Configuration optimale → `configs/best_vectorization_config.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports réussis\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "sys.path.insert(0, '../scripts')\n",
    "\n",
    "from rakuten_text import (\n",
    "    final_text_cleaner,\n",
    "    extract_text_features,\n",
    "    save_vectorization_config,\n",
    "    run_single_experiment,\n",
    "    run_strategy_comparison,\n",
    "    run_hyperparameter_grid,\n",
    "    run_title_weighting_experiment,\n",
    "    analyze_experiment_results,\n",
    "    generate_vectorization_report,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import data loading with consistent split\n",
    "from load_data import split_data\n",
    "\n",
    "print(\"✓ Imports réussis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Chargement et Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 72178 samples loaded\n",
      "✓ Development: 72,178 samples (85%)\n",
      "✓ Hold-out:    12,738 samples (15%)\n",
      "⚠️  Hold-out will be evaluated at the end only!\n",
      "\n",
      "Applying preprocessing...\n",
      "✓ Texts cleaned (dev + holdout)\n"
     ]
    }
   ],
   "source": [
    "# Load data with consistent split (same as NB01)\n",
    "X_dev, X_holdout, y_dev, y_holdout = split_data()\n",
    "\n",
    "# Prepare dataframes\n",
    "df_dev = X_dev.copy()\n",
    "df_dev['prdtypecode'] = y_dev\n",
    "\n",
    "df_holdout = X_holdout.copy()\n",
    "df_holdout['prdtypecode'] = y_holdout\n",
    "\n",
    "print(f\"✓ {df_dev.shape[0]} samples loaded\")\n",
    "print(f\"✓ Development: {len(df_dev):,} samples (85%)\")\n",
    "print(f\"✓ Hold-out:    {len(df_holdout):,} samples (15%)\")\n",
    "print(\"⚠️  Hold-out will be evaluated at the end only!\")\n",
    "\n",
    "# Apply preprocessing (from Notebook 01)\n",
    "print(\"\\nApplying preprocessing...\")\n",
    "df_dev['title_clean'] = df_dev['designation'].fillna('').apply(final_text_cleaner)\n",
    "df_dev['desc_clean'] = df_dev['description'].fillna('').apply(final_text_cleaner)\n",
    "\n",
    "df_holdout['title_clean'] = df_holdout['designation'].fillna('').apply(final_text_cleaner)\n",
    "df_holdout['desc_clean'] = df_holdout['description'].fillna('').apply(final_text_cleaner)\n",
    "\n",
    "print(\"✓ Texts cleaned (dev + holdout)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Feature Engineering\n",
      "================================================================================\n",
      "Extraction des features manuelles...\n",
      "  Colonnes à analyser: ['designation', 'description']\n",
      "  → Traitement de 'designation'...\n",
      "  → Traitement de 'description'...\n",
      "✓ Extraction terminée: 12 features créées\n",
      "  Noms: ['designation_len_char', 'designation_len_words', 'designation_num_digits']...\n",
      "\n",
      "✓ 12 features extracted\n",
      "Features: ['designation_len_char', 'designation_len_words', 'designation_num_digits']...\n"
     ]
    }
   ],
   "source": [
    "# Extract manual features (on dev + holdout)\n",
    "print(\"=\"*80)\n",
    "print(\"Feature Engineering\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "features_dev = extract_text_features(\n",
    "    df_dev,\n",
    "    text_columns=['designation', 'description'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "features_holdout = extract_text_features(\n",
    "    df_holdout,\n",
    "    text_columns=['designation', 'description'],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Add to DataFrame\n",
    "for col in features_dev.columns:\n",
    "    df_dev[col] = features_dev[col]\n",
    "    df_holdout[col] = features_holdout[col]\n",
    "\n",
    "feature_columns = list(features_dev.columns)\n",
    "print(f\"\\n✓ {len(feature_columns)} features extracted\")\n",
    "print(f\"Features: {feature_columns[:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Development Split (Train/Val)\n",
      "================================================================================\n",
      "✓ Split complete:\n",
      "  Train:      61,351 (~72.2%)\n",
      "  Validation: 10,827 (~12.8%)\n",
      "  Hold-out:   12,738 (15.0%)\n",
      "\n",
      "⚠️  Model selection will use Train/Val only\n",
      "⚠️  Hold-out will be evaluated at the end\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Development Split (Train/Val)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Development Split (Train/Val)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "VAL_SIZE = 0.15\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df_dev,\n",
    "    df_dev['prdtypecode'],\n",
    "    test_size=VAL_SIZE,\n",
    "    stratify=df_dev['prdtypecode'],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "total_samples = len(df_dev) + len(df_holdout)\n",
    "print(f\"✓ Split complete:\")\n",
    "print(f\"  Train:      {len(X_train):,} (~{len(X_train)/total_samples*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(X_val):,} (~{len(X_val)/total_samples*100:.1f}%)\")\n",
    "print(f\"  Hold-out:   {len(df_holdout):,} (15.0%)\")\n",
    "\n",
    "print(\"\\n⚠️  Model selection will use Train/Val only\")\n",
    "print(\"⚠️  Hold-out will be evaluated at the end\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Expérience 1: Comparaison Vectorization Methods\n",
    "\n",
    "Test: Count vs TF-IDF (sans features manuelles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Count vs TF-IDF (Split, sans features)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "Entraînement du pipeline...\n",
      "✓ Entraînement terminé en 18.72s\n",
      "\n",
      "================================================================================\n",
      "RÉSULTATS DE L'ÉVALUATION\n",
      "================================================================================\n",
      "F1-Score (weighted): 0.8231\n",
      "Accuracy:            0.8225\n",
      "Temps prédiction:    1.51s\n",
      "\n",
      "Rapport de classification:\n",
      "--------------------------------------------------------------------------------\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "     Livres techniques       0.49      0.65      0.56       397\n",
      "          Rétro Gaming       0.74      0.68      0.71       320\n",
      "        Accessoires JV       0.82      0.82      0.82       214\n",
      "              Consoles       0.99      0.76      0.86       106\n",
      "              Figurine       0.78      0.78      0.78       341\n",
      "Cartes à collectionner       0.91      0.94      0.92       504\n",
      "          Jeux de rôle       0.92      0.59      0.72        97\n",
      "    Jouets & Figurines       0.72      0.76      0.74       621\n",
      "        Jeux éducatifs       0.72      0.54      0.62       264\n",
      "    Modélisme & Drones       0.97      0.94      0.95       643\n",
      "           Bébé & Jeux       0.99      0.88      0.93       103\n",
      "       Sport & Loisirs       0.85      0.77      0.81       318\n",
      "   Bébé & Puériculture       0.78      0.77      0.78       413\n",
      "     Équipement maison       0.86      0.83      0.84       647\n",
      "              Textiles       0.89      0.94      0.92       549\n",
      "          Alimentation       0.94      0.79      0.86       102\n",
      "      Déco & Éclairage       0.77      0.82      0.79       637\n",
      "               Animaux       0.92      0.77      0.84       105\n",
      "  Journaux & magazines       0.68      0.83      0.75       607\n",
      "Séries & encyclopédies       0.75      0.76      0.75       609\n",
      "            Jeux Vidéo       0.79      0.73      0.76       181\n",
      "    Fournitures bureau       0.91      0.94      0.92       636\n",
      "      Jardinage & déco       0.81      0.71      0.76       330\n",
      "               Piscine       0.96      0.97      0.96      1302\n",
      "    Jardin & Bricolage       0.81      0.75      0.78       318\n",
      "  Romans & littérature       0.80      0.67      0.73       352\n",
      "               Jeux PC       1.00      0.98      0.99       111\n",
      "\n",
      "              accuracy                           0.82     10827\n",
      "             macro avg       0.84      0.79      0.81     10827\n",
      "          weighted avg       0.83      0.82      0.82     10827\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Expérience: COUNT + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "Entraînement du pipeline...\n",
      "✓ Entraînement terminé en 67.83s\n",
      "\n",
      "================================================================================\n",
      "RÉSULTATS DE L'ÉVALUATION\n",
      "================================================================================\n",
      "F1-Score (weighted): 0.8159\n",
      "Accuracy:            0.8144\n",
      "Temps prédiction:    1.50s\n",
      "\n",
      "Rapport de classification:\n",
      "--------------------------------------------------------------------------------\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "     Livres techniques       0.46      0.69      0.56       397\n",
      "          Rétro Gaming       0.69      0.69      0.69       320\n",
      "        Accessoires JV       0.79      0.77      0.78       214\n",
      "              Consoles       0.94      0.83      0.88       106\n",
      "              Figurine       0.80      0.76      0.78       341\n",
      "Cartes à collectionner       0.92      0.93      0.92       504\n",
      "          Jeux de rôle       0.88      0.63      0.73        97\n",
      "    Jouets & Figurines       0.71      0.71      0.71       621\n",
      "        Jeux éducatifs       0.64      0.59      0.61       264\n",
      "    Modélisme & Drones       0.95      0.93      0.94       643\n",
      "           Bébé & Jeux       0.96      0.88      0.92       103\n",
      "       Sport & Loisirs       0.84      0.78      0.81       318\n",
      "   Bébé & Puériculture       0.76      0.78      0.77       413\n",
      "     Équipement maison       0.83      0.83      0.83       647\n",
      "              Textiles       0.89      0.93      0.91       549\n",
      "          Alimentation       0.94      0.82      0.88       102\n",
      "      Déco & Éclairage       0.80      0.78      0.79       637\n",
      "               Animaux       0.89      0.73      0.80       105\n",
      "  Journaux & magazines       0.75      0.80      0.78       607\n",
      "Séries & encyclopédies       0.76      0.73      0.75       609\n",
      "            Jeux Vidéo       0.80      0.75      0.77       181\n",
      "    Fournitures bureau       0.90      0.93      0.91       636\n",
      "      Jardinage & déco       0.77      0.68      0.72       330\n",
      "               Piscine       0.97      0.97      0.97      1302\n",
      "    Jardin & Bricolage       0.76      0.75      0.76       318\n",
      "  Romans & littérature       0.75      0.66      0.70       352\n",
      "               Jeux PC       0.99      0.99      0.99       111\n",
      "\n",
      "              accuracy                           0.81     10827\n",
      "             macro avg       0.82      0.79      0.80     10827\n",
      "          weighted avg       0.82      0.81      0.82     10827\n",
      "\n",
      "\n",
      "============================================================\n",
      "RÉSULTATS:\n",
      "      method  f1_score  accuracy\n",
      "TF-IDF Split  0.823086  0.822481\n",
      " Count Split  0.815917  0.814353\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# baseline_raw_f1 from Notebook 01\n",
    "baseline_f1 = 0.7858\n",
    "\n",
    "print(\"Test 1: Count vs TF-IDF (Split, sans features)\\n\")\n",
    "\n",
    "results_vec = []\n",
    "\n",
    "# TF-IDF Split\n",
    "result_tfidf = run_single_experiment(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    vectorizer_type='tfidf',\n",
    "    strategy='split',\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=None,\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    model_name='logreg',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "results_vec.append({\n",
    "    'method': 'TF-IDF Split',\n",
    "    'f1_score': result_tfidf['f1_score'],\n",
    "    'accuracy': result_tfidf['accuracy']\n",
    "})\n",
    "\n",
    "# Count Split\n",
    "result_count = run_single_experiment(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    vectorizer_type='count',\n",
    "    strategy='split',\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=None,\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    model_name='logreg',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "results_vec.append({\n",
    "    'method': 'Count Split',\n",
    "    'f1_score': result_count['f1_score'],\n",
    "    'accuracy': result_count['accuracy']\n",
    "})\n",
    "\n",
    "df_vec = pd.DataFrame(results_vec)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RÉSULTATS:\")\n",
    "print(df_vec.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Expérience 2: Split vs Merged Strategy\n",
    "\n",
    "Comparaison détaillée des deux stratégies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2: Split vs Merged (TF-IDF)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: merged\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "Entraînement du pipeline...\n",
      "✓ Entraînement terminé en 19.98s\n",
      "\n",
      "================================================================================\n",
      "RÉSULTATS DE L'ÉVALUATION\n",
      "================================================================================\n",
      "F1-Score (weighted): 0.7854\n",
      "Accuracy:            0.7859\n",
      "Temps prédiction:    1.59s\n",
      "\n",
      "Rapport de classification:\n",
      "--------------------------------------------------------------------------------\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "     Livres techniques       0.44      0.60      0.50       397\n",
      "          Rétro Gaming       0.71      0.61      0.66       320\n",
      "        Accessoires JV       0.76      0.80      0.78       214\n",
      "              Consoles       0.96      0.74      0.83       106\n",
      "              Figurine       0.73      0.74      0.73       341\n",
      "Cartes à collectionner       0.87      0.89      0.88       504\n",
      "          Jeux de rôle       0.82      0.38      0.52        97\n",
      "    Jouets & Figurines       0.65      0.64      0.64       621\n",
      "        Jeux éducatifs       0.66      0.48      0.56       264\n",
      "    Modélisme & Drones       0.85      0.91      0.88       643\n",
      "           Bébé & Jeux       0.92      0.81      0.86       103\n",
      "       Sport & Loisirs       0.83      0.72      0.77       318\n",
      "   Bébé & Puériculture       0.77      0.73      0.75       413\n",
      "     Équipement maison       0.82      0.81      0.82       647\n",
      "              Textiles       0.87      0.94      0.90       549\n",
      "          Alimentation       0.92      0.72      0.81       102\n",
      "      Déco & Éclairage       0.74      0.79      0.76       637\n",
      "               Animaux       0.88      0.76      0.82       105\n",
      "  Journaux & magazines       0.68      0.79      0.73       607\n",
      "Séries & encyclopédies       0.72      0.70      0.71       609\n",
      "            Jeux Vidéo       0.79      0.70      0.74       181\n",
      "    Fournitures bureau       0.87      0.91      0.89       636\n",
      "      Jardinage & déco       0.82      0.68      0.75       330\n",
      "               Piscine       0.94      0.97      0.95      1302\n",
      "    Jardin & Bricolage       0.78      0.71      0.75       318\n",
      "  Romans & littérature       0.71      0.70      0.71       352\n",
      "               Jeux PC       0.99      0.95      0.97       111\n",
      "\n",
      "              accuracy                           0.79     10827\n",
      "             macro avg       0.80      0.75      0.77     10827\n",
      "          weighted avg       0.79      0.79      0.79     10827\n",
      "\n",
      "\n",
      "============================================================\n",
      "COMPARAISON Split vs Merged:\n",
      "  Split:  F1 = 0.8231\n",
      "  Merged: F1 = 0.7854\n",
      "  → Split est +4.80% meilleur\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 2: Split vs Merged (TF-IDF)\\n\")\n",
    "\n",
    "# Créer colonne merged pour test\n",
    "X_train['text_merged'] = X_train['title_clean'] + ' ' + X_train['desc_clean']\n",
    "X_val['text_merged'] = X_val['title_clean'] + ' ' + X_val['desc_clean']\n",
    "\n",
    "# TF-IDF Merged\n",
    "result_merged = run_single_experiment(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    vectorizer_type='tfidf',\n",
    "    strategy='merged',\n",
    "    text_columns=['text_merged'],\n",
    "    feature_columns=None,\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    model_name='logreg',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARAISON Split vs Merged:\")\n",
    "print(f\"  Split:  F1 = {result_tfidf['f1_score']:.4f}\")\n",
    "print(f\"  Merged: F1 = {result_merged['f1_score']:.4f}\")\n",
    "improvement = (result_tfidf['f1_score'] - result_merged['f1_score']) / result_merged['f1_score'] * 100\n",
    "print(f\"  → Split est {improvement:+.2f}% meilleur\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PER-CLASS ANALYSIS: Split vs Merged Performance\n",
      "================================================================================\n",
      "\n",
      "Note: Comparing Split (20k total) vs Merged (10k total)\n",
      "For fair comparison with equal features, see Cell 15 below\n",
      "\n",
      "Top 10 categories benefiting most from Split strategy:\n",
      "              category  split_f1  merged_f1     diff  pct_change\n",
      "          Jeux de rôle  0.716981   0.521127 0.195854   37.582866\n",
      "    Jouets & Figurines  0.737910   0.644426 0.093483   14.506414\n",
      "           Bébé & Jeux  0.933333   0.860104 0.073230    8.514056\n",
      "    Modélisme & Drones  0.951855   0.880181 0.071674    8.143090\n",
      "        Jeux éducatifs  0.617391   0.560175 0.057216   10.213995\n",
      "     Livres techniques  0.561480   0.504772 0.056708   11.234352\n",
      "          Alimentation  0.861702   0.806630 0.055072    6.827456\n",
      "          Rétro Gaming  0.709887   0.658824 0.051063    7.750637\n",
      "Séries & encyclopédies  0.754902   0.709516 0.045386    6.396770\n",
      "              Figurine  0.780059   0.734993 0.045066    6.131486\n",
      "\n",
      "Bottom 10 categories (least benefit from Split):\n",
      "            category  split_f1  merged_f1     diff  pct_change\n",
      "            Consoles  0.861702   0.834225 0.027478    3.293781\n",
      "   Équipement maison  0.841857   0.815159 0.026698    3.275223\n",
      "             Animaux  0.839378   0.816327 0.023052    2.823834\n",
      "Romans & littérature  0.727554   0.707736 0.019818    2.800165\n",
      "          Jeux Vidéo  0.758621   0.739003 0.019618    2.654625\n",
      "             Jeux PC  0.990909   0.972477 0.018432    1.895369\n",
      "Journaux & magazines  0.748320   0.733080 0.015240    2.078872\n",
      "            Textiles  0.916667   0.904302 0.012365    1.367314\n",
      "             Piscine  0.963754   0.953356 0.010398    1.090695\n",
      "    Jardinage & déco  0.755267   0.745033 0.010234    1.373672\n",
      "\n",
      "Summary Statistics:\n",
      "  Categories improved by Split: 27 / 27\n",
      "  Average improvement: +0.0435\n",
      "  Median improvement: +0.0342\n",
      "  Std deviation: 0.0368\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from rakuten_text.categories import get_all_categories\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PER-CLASS ANALYSIS: Split vs Merged Performance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get predictions from both strategies (using existing result_merged from Cell 12)\n",
    "pipeline_split = result_tfidf['pipeline']\n",
    "pipeline_merged = result_merged['pipeline']\n",
    "\n",
    "y_pred_split = pipeline_split.predict(X_val)\n",
    "y_pred_merged = pipeline_merged.predict(X_val)\n",
    "\n",
    "# Classification reports\n",
    "report_split = classification_report(y_val, y_pred_split, output_dict=True, zero_division=0)\n",
    "report_merged = classification_report(y_val, y_pred_merged, output_dict=True, zero_division=0)\n",
    "\n",
    "# Load category mapping from categories.py\n",
    "categories = get_all_categories(short=True)\n",
    "\n",
    "# Compare per category\n",
    "comparison_data = []\n",
    "for code in sorted(categories.keys()):\n",
    "    cat_name = categories[code]\n",
    "    f1_split = report_split.get(str(code), {}).get('f1-score', 0)\n",
    "    f1_merged = report_merged.get(str(code), {}).get('f1-score', 0)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'code': code,\n",
    "        'category': cat_name,\n",
    "        'split_f1': f1_split,\n",
    "        'merged_f1': f1_merged,\n",
    "        'diff': f1_split - f1_merged,\n",
    "        'pct_change': (f1_split - f1_merged) / f1_merged * 100 if f1_merged > 0 else 0\n",
    "    })\n",
    "\n",
    "df_comp = pd.DataFrame(comparison_data)\n",
    "df_comp = df_comp.sort_values('diff', ascending=False)\n",
    "\n",
    "print(\"\\nNote: Comparing Split (20k total) vs Merged (10k total)\")\n",
    "print(\"For fair comparison with equal features, see Cell 15 below\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 categories benefiting most from Split strategy:\")\n",
    "print(df_comp.head(10)[['category', 'split_f1', 'merged_f1', 'diff', 'pct_change']].to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom 10 categories (least benefit from Split):\")\n",
    "print(df_comp.tail(10)[['category', 'split_f1', 'merged_f1', 'diff', 'pct_change']].to_string(index=False))\n",
    "\n",
    "# Statistical summary\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"  Categories improved by Split: {(df_comp['diff'] > 0).sum()} / {len(df_comp)}\")\n",
    "print(f\"  Average improvement: {df_comp['diff'].mean():+.4f}\")\n",
    "print(f\"  Median improvement: {df_comp['diff'].median():+.4f}\")\n",
    "print(f\"  Std deviation: {df_comp['diff'].std():.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6c. Per-Class Analysis: Which Categories Benefit from Split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FAIR COMPARISON: Split vs Merged (Equal Feature Capacity)\n",
      "================================================================================\n",
      "\n",
      "Scenario A: Total 10k features\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 5000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "Entraînement du pipeline...\n",
      "✓ Entraînement terminé en 17.47s\n",
      "\n",
      "================================================================================\n",
      "RÉSULTATS DE L'ÉVALUATION\n",
      "================================================================================\n",
      "F1-Score (weighted): 0.8124\n",
      "Accuracy:            0.8114\n",
      "Temps prédiction:    1.49s\n",
      "\n",
      "Rapport de classification:\n",
      "--------------------------------------------------------------------------------\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "     Livres techniques       0.47      0.65      0.55       397\n",
      "          Rétro Gaming       0.74      0.67      0.70       320\n",
      "        Accessoires JV       0.82      0.81      0.81       214\n",
      "              Consoles       0.96      0.77      0.86       106\n",
      "              Figurine       0.76      0.77      0.76       341\n",
      "Cartes à collectionner       0.89      0.92      0.90       504\n",
      "          Jeux de rôle       0.90      0.59      0.71        97\n",
      "    Jouets & Figurines       0.72      0.74      0.73       621\n",
      "        Jeux éducatifs       0.68      0.57      0.62       264\n",
      "    Modélisme & Drones       0.97      0.93      0.95       643\n",
      "           Bébé & Jeux       0.97      0.83      0.90       103\n",
      "       Sport & Loisirs       0.84      0.76      0.80       318\n",
      "   Bébé & Puériculture       0.76      0.74      0.75       413\n",
      "     Équipement maison       0.84      0.83      0.84       647\n",
      "              Textiles       0.88      0.94      0.91       549\n",
      "          Alimentation       0.90      0.75      0.82       102\n",
      "      Déco & Éclairage       0.77      0.81      0.79       637\n",
      "               Animaux       0.94      0.73      0.82       105\n",
      "  Journaux & magazines       0.69      0.81      0.74       607\n",
      "Séries & encyclopédies       0.75      0.74      0.74       609\n",
      "            Jeux Vidéo       0.78      0.71      0.75       181\n",
      "    Fournitures bureau       0.89      0.92      0.90       636\n",
      "      Jardinage & déco       0.80      0.67      0.73       330\n",
      "               Piscine       0.95      0.97      0.96      1302\n",
      "    Jardin & Bricolage       0.78      0.73      0.76       318\n",
      "  Romans & littérature       0.77      0.67      0.72       352\n",
      "               Jeux PC       1.00      0.98      0.99       111\n",
      "\n",
      "              accuracy                           0.81     10827\n",
      "             macro avg       0.82      0.78      0.80     10827\n",
      "          weighted avg       0.82      0.81      0.81     10827\n",
      "\n",
      "\n",
      "Results (10k total features):\n",
      "  Split (5k/col):  F1 = 0.8124\n",
      "  Merged (10k):    F1 = 0.7854\n",
      "  Difference:      +0.0270 (+3.43%)\n",
      "\n",
      "================================================================================\n",
      "Scenario B: Total 20k features\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: merged\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "Entraînement du pipeline...\n",
      "✓ Entraînement terminé en 22.75s\n",
      "\n",
      "================================================================================\n",
      "RÉSULTATS DE L'ÉVALUATION\n",
      "================================================================================\n",
      "F1-Score (weighted): 0.7994\n",
      "Accuracy:            0.7997\n",
      "Temps prédiction:    1.71s\n",
      "\n",
      "Rapport de classification:\n",
      "--------------------------------------------------------------------------------\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "     Livres techniques       0.47      0.63      0.54       397\n",
      "          Rétro Gaming       0.73      0.65      0.69       320\n",
      "        Accessoires JV       0.78      0.84      0.81       214\n",
      "              Consoles       0.95      0.74      0.83       106\n",
      "              Figurine       0.78      0.79      0.78       341\n",
      "Cartes à collectionner       0.89      0.90      0.90       504\n",
      "          Jeux de rôle       0.90      0.58      0.70        97\n",
      "    Jouets & Figurines       0.67      0.64      0.65       621\n",
      "        Jeux éducatifs       0.67      0.50      0.57       264\n",
      "    Modélisme & Drones       0.85      0.93      0.89       643\n",
      "           Bébé & Jeux       0.98      0.86      0.92       103\n",
      "       Sport & Loisirs       0.84      0.73      0.78       318\n",
      "   Bébé & Puériculture       0.78      0.75      0.77       413\n",
      "     Équipement maison       0.81      0.82      0.82       647\n",
      "              Textiles       0.88      0.94      0.91       549\n",
      "          Alimentation       0.92      0.75      0.82       102\n",
      "      Déco & Éclairage       0.74      0.79      0.76       637\n",
      "               Animaux       0.92      0.74      0.82       105\n",
      "  Journaux & magazines       0.68      0.80      0.73       607\n",
      "Séries & encyclopédies       0.74      0.71      0.73       609\n",
      "            Jeux Vidéo       0.82      0.68      0.74       181\n",
      "    Fournitures bureau       0.89      0.92      0.91       636\n",
      "      Jardinage & déco       0.82      0.68      0.75       330\n",
      "               Piscine       0.95      0.97      0.96      1302\n",
      "    Jardin & Bricolage       0.81      0.75      0.78       318\n",
      "  Romans & littérature       0.73      0.71      0.72       352\n",
      "               Jeux PC       0.99      0.95      0.97       111\n",
      "\n",
      "              accuracy                           0.80     10827\n",
      "             macro avg       0.81      0.77      0.79     10827\n",
      "          weighted avg       0.80      0.80      0.80     10827\n",
      "\n",
      "\n",
      "Results (20k total features):\n",
      "  Split (10k/col): F1 = 0.8231\n",
      "  Merged (20k):    F1 = 0.7994\n",
      "  Difference:      +0.0237 (+2.96%)\n",
      "\n",
      "================================================================================\n",
      "FAIR COMPARISON SUMMARY\n",
      "================================================================================\n",
      "10k total features: Split advantage = +0.0270\n",
      "20k total features: Split advantage = +0.0237\n",
      "\n",
      "Conclusion: Split strategy is genuinely better\n",
      "(advantage persists across both feature capacity levels)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FAIR COMPARISON: Split vs Merged (Equal Feature Capacity)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Scenario A: Control total features at 10,000\n",
    "print(\"\\nScenario A: Total 10k features\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Split: 5k per column\n",
    "result_split_5k = run_single_experiment(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    vectorizer_type='tfidf',\n",
    "    strategy='split',\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=None,\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    model_name='logreg',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nResults (10k total features):\")\n",
    "print(f\"  Split (5k/col):  F1 = {result_split_5k['f1_score']:.4f}\")\n",
    "print(f\"  Merged (10k):    F1 = {result_merged['f1_score']:.4f}\")\n",
    "diff_10k = result_split_5k['f1_score'] - result_merged['f1_score']\n",
    "print(f\"  Difference:      {diff_10k:+.4f} ({diff_10k/result_merged['f1_score']*100:+.2f}%)\")\n",
    "\n",
    "# Scenario B: Control total features at 20,000\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Scenario B: Total 20k features\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Merged: 20k total\n",
    "result_merged_20k = run_single_experiment(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    vectorizer_type='tfidf',\n",
    "    strategy='merged',\n",
    "    text_columns=['text_merged'],\n",
    "    feature_columns=None,\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2),\n",
    "    model_name='logreg',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nResults (20k total features):\")\n",
    "print(f\"  Split (10k/col): F1 = {result_tfidf['f1_score']:.4f}\")\n",
    "print(f\"  Merged (20k):    F1 = {result_merged_20k['f1_score']:.4f}\")\n",
    "diff_20k = result_tfidf['f1_score'] - result_merged_20k['f1_score']\n",
    "print(f\"  Difference:      {diff_20k:+.4f} ({diff_20k/result_merged_20k['f1_score']*100:+.2f}%)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FAIR COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"10k total features: Split advantage = {diff_10k:+.4f}\")\n",
    "print(f\"20k total features: Split advantage = {diff_20k:+.4f}\")\n",
    "\n",
    "if diff_10k > 0 and diff_20k > 0:\n",
    "    print(\"\\nConclusion: Split strategy is genuinely better\")\n",
    "    print(\"(advantage persists across both feature capacity levels)\")\n",
    "elif diff_10k > 0 and diff_20k <= 0:\n",
    "    print(\"\\nConclusion: Split advantage mainly from higher feature count\")\n",
    "    print(\"(not from strategy itself)\")\n",
    "else:\n",
    "    print(\"\\nConclusion: Results inconsistent, needs investigation\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b. FIX: Fair Split vs Merged Comparison\n",
    "\n",
    "**Issue**: Previous comparison was unfair\n",
    "- Split: 2 columns × 10k = 20k total features\n",
    "- Merged: 1 column × 10k = 10k total features\n",
    "\n",
    "**Fix**: Test both strategies with equal feature capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from rakuten_text.categories import get_all_categories\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PER-CLASS ANALYSIS: Split vs Merged Performance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get predictions from both strategies (using existing result_merged from Cell 12)\n",
    "pipeline_split = result_tfidf['pipeline']\n",
    "pipeline_merged = result_merged['pipeline']\n",
    "\n",
    "y_pred_split = pipeline_split.predict(X_val)\n",
    "y_pred_merged = pipeline_merged.predict(X_val)\n",
    "\n",
    "# Classification reports\n",
    "report_split = classification_report(y_val, y_pred_split, output_dict=True, zero_division=0)\n",
    "report_merged = classification_report(y_val, y_pred_merged, output_dict=True, zero_division=0)\n",
    "\n",
    "# Load category mapping from categories.py\n",
    "categories = get_all_categories(short=True)\n",
    "\n",
    "# Compare per category\n",
    "comparison_data = []\n",
    "for code in sorted(categories.keys()):\n",
    "    cat_name = categories[code]\n",
    "    f1_split = report_split.get(str(code), {}).get('f1-score', 0)\n",
    "    f1_merged = report_merged.get(str(code), {}).get('f1-score', 0)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'code': code,\n",
    "        'category': cat_name,\n",
    "        'split_f1': f1_split,\n",
    "        'merged_f1': f1_merged,\n",
    "        'diff': f1_split - f1_merged,\n",
    "        'pct_change': (f1_split - f1_merged) / f1_merged * 100 if f1_merged > 0 else 0\n",
    "    })\n",
    "\n",
    "df_comp = pd.DataFrame(comparison_data)\n",
    "df_comp = df_comp.sort_values('diff', ascending=False)\n",
    "\n",
    "print(\"\\nNote: Comparing Split (20k total) vs Merged (10k total)\")\n",
    "print(\"For fair comparison with equal features, see Cell 15 below\")\n",
    "print()\n",
    "\n",
    "print(\"Top 10 categories benefiting most from Split strategy:\")\n",
    "print(df_comp.head(10)[['category', 'split_f1', 'merged_f1', 'diff', 'pct_change']].to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom 10 categories (least benefit from Split):\")\n",
    "print(df_comp.tail(10)[['category', 'split_f1', 'merged_f1', 'diff', 'pct_change']].to_string(index=False))\n",
    "\n",
    "# Statistical summary\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"  Categories improved by Split: {(df_comp['diff'] > 0).sum()} / {len(df_comp)}\")\n",
    "print(f\"  Average improvement: {df_comp['diff'].mean():+.4f}\")\n",
    "print(f\"  Median improvement: {df_comp['diff'].median():+.4f}\")\n",
    "print(f\"  Std deviation: {df_comp['diff'].std():.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2b: Pondération du Titre (TF-IDF Split)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST DE PONDÉRATION DU TITRE\n",
      "================================================================================\n",
      "Poids à tester: [1.0, 1.5, 2.0, 2.5, 3.0]\n",
      "Vectorizer: TFIDF\n",
      "Max features: 10000 (par colonne)\n",
      "N-gram range: (1, 2)\n",
      "Modèle: LOGREG\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[1/5] Test avec title_weight = 1.0x\n",
      "--------------------------------------------------------------------------------\n",
      "  → F1-Score: 0.8231\n",
      "  → Accuracy: 0.8225\n",
      "  → Train time: 19.69s\n",
      "\n",
      "[2/5] Test avec title_weight = 1.5x\n",
      "--------------------------------------------------------------------------------\n",
      "  → F1-Score: 0.8248\n",
      "  → Accuracy: 0.8243\n",
      "  → Train time: 20.56s\n",
      "\n",
      "[3/5] Test avec title_weight = 2.0x\n",
      "--------------------------------------------------------------------------------\n",
      "  → F1-Score: 0.8236\n",
      "  → Accuracy: 0.8232\n",
      "  → Train time: 18.50s\n",
      "\n",
      "[4/5] Test avec title_weight = 2.5x\n",
      "--------------------------------------------------------------------------------\n",
      "  → F1-Score: 0.8214\n",
      "  → Accuracy: 0.8209\n",
      "  → Train time: 18.71s\n",
      "\n",
      "[5/5] Test avec title_weight = 3.0x\n",
      "--------------------------------------------------------------------------------\n",
      "  → F1-Score: 0.8170\n",
      "  → Accuracy: 0.8163\n",
      "  → Train time: 17.80s\n",
      "\n",
      "================================================================================\n",
      "RÉSULTATS DE LA PONDÉRATION DU TITRE\n",
      "================================================================================\n",
      "\n",
      " title_weight  f1_score  accuracy  train_time\n",
      "          1.5  0.824762  0.824328   20.560400\n",
      "          2.0  0.823637  0.823220   18.503972\n",
      "          1.0  0.823086  0.822481   19.694470\n",
      "          2.5  0.821372  0.820911   18.709546\n",
      "          3.0  0.817045  0.816293   17.797886\n",
      "\n",
      "================================================================================\n",
      "ANALYSE\n",
      "================================================================================\n",
      "Baseline (weight=1.0): F1 = 0.8231\n",
      "Meilleur (weight=1.5): F1 = 0.8248\n",
      "Amélioration: +0.17%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "✓ Test de pondération terminé\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 2b: Pondération du Titre (TF-IDF Split)\\n\")\n",
    "\n",
    "# Tester différents poids pour le titre: 1x, 1.5x, 2x, 2.5x, 3x\n",
    "results_weighting = run_title_weighting_experiment(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=None,  # Sans features manuelles pour isoler l'effet\n",
    "    title_weights=[1.0, 1.5, 2.0, 2.5, 3.0],\n",
    "    vectorizer_type='tfidf',\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    model_name='logreg',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Test de pondération terminé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARAMETER INTERACTION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Question: Is title_weight=1.5x optimal under different configurations?\n",
      "Testing under 4 scenarios with varying max_features and ngram_range\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Scenario: Baseline (10k, bigrams)\n",
      "  max_features: 10000, ngram_range: (1, 2)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 1.0x: F1 = 0.8231\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 1.5x: F1 = 0.8248\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 2.0x: F1 = 0.8236\n",
      "  → Best: 1.5x (F1=0.8248)\n",
      "\n",
      "================================================================================\n",
      "Scenario: Unigrams only (10k)\n",
      "  max_features: 10000, ngram_range: (1, 1)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 1.0x: F1 = 0.8241\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 1.5x: F1 = 0.8262\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 2.0x: F1 = 0.8241\n",
      "  → Best: 1.5x (F1=0.8262)\n",
      "\n",
      "================================================================================\n",
      "Scenario: Optimal from Grid (20k, unigrams)\n",
      "  max_features: 20000, ngram_range: (1, 1)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 1.0x: F1 = 0.8257\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 1.5x: F1 = 0.8323\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 2.0x: F1 = 0.8307\n",
      "  → Best: 1.5x (F1=0.8323)\n",
      "\n",
      "================================================================================\n",
      "Scenario: Low capacity (5k, bigrams)\n",
      "  max_features: 5000, ngram_range: (1, 2)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 5000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 1.0x: F1 = 0.8124\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 5000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 1.5x: F1 = 0.8152\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 5000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "  Weight 2.0x: F1 = 0.8120\n",
      "  → Best: 1.5x (F1=0.8152)\n",
      "\n",
      "================================================================================\n",
      "INTERACTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "F1 Scores by Scenario and Title Weight:\n",
      "title_weight                            1.0       1.5       2.0\n",
      "scenario                                                       \n",
      "Baseline (10k, bigrams)            0.823086  0.824762  0.823637\n",
      "Low capacity (5k, bigrams)         0.812366  0.815207  0.811959\n",
      "Optimal from Grid (20k, unigrams)  0.825700  0.832314  0.830716\n",
      "Unigrams only (10k)                0.824088  0.826247  0.824134\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Optimal title_weight per scenario:\n",
      "--------------------------------------------------------------------------------\n",
      "Baseline (10k, bigrams)                  → 1.5x (F1=0.8248)\n",
      "Unigrams only (10k)                      → 1.5x (F1=0.8262)\n",
      "Optimal from Grid (20k, unigrams)        → 1.5x (F1=0.8323)\n",
      "Low capacity (5k, bigrams)               → 1.5x (F1=0.8152)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Weight preference distribution:\n",
      "--------------------------------------------------------------------------------\n",
      "  Weight 1.5x: 4/4 scenarios\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "✓ Title weight 1.5x is robust across different configurations\n",
      "  (optimal or near-optimal in majority of scenarios)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PARAMETER INTERACTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nQuestion: Is title_weight=1.5x optimal under different configurations?\")\n",
    "print(\"Testing under 4 scenarios with varying max_features and ngram_range\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Define test scenarios\n",
    "scenarios = [\n",
    "    {'max_features': 10000, 'ngram_range': (1, 2), 'name': 'Baseline (10k, bigrams)'},\n",
    "    {'max_features': 10000, 'ngram_range': (1, 1), 'name': 'Unigrams only (10k)'},\n",
    "    {'max_features': 20000, 'ngram_range': (1, 1), 'name': 'Optimal from Grid (20k, unigrams)'},\n",
    "    {'max_features': 5000, 'ngram_range': (1, 2), 'name': 'Low capacity (5k, bigrams)'}\n",
    "]\n",
    "\n",
    "# Test weights for each scenario\n",
    "test_weights = [1.0, 1.5, 2.0]\n",
    "\n",
    "results_interaction = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Scenario: {scenario['name']}\")\n",
    "    print(f\"  max_features: {scenario['max_features']}, ngram_range: {scenario['ngram_range']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    scenario_results = []\n",
    "    \n",
    "    for weight in test_weights:\n",
    "        # Run experiment\n",
    "        result = run_single_experiment(\n",
    "            X_train, X_val, y_train, y_val,\n",
    "            vectorizer_type='tfidf',\n",
    "            strategy='split',\n",
    "            text_columns=['title_clean', 'desc_clean'],\n",
    "            feature_columns=None,\n",
    "            max_features=scenario['max_features'],\n",
    "            ngram_range=scenario['ngram_range'],\n",
    "            title_weight=weight,\n",
    "            model_name='logreg',\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        scenario_results.append({\n",
    "            'scenario': scenario['name'],\n",
    "            'max_features': scenario['max_features'],\n",
    "            'ngram_range': str(scenario['ngram_range']),\n",
    "            'title_weight': weight,\n",
    "            'f1_score': result['f1_score']\n",
    "        })\n",
    "        \n",
    "        print(f\"  Weight {weight}x: F1 = {result['f1_score']:.4f}\")\n",
    "    \n",
    "    # Find best weight for this scenario\n",
    "    best_in_scenario = max(scenario_results, key=lambda x: x['f1_score'])\n",
    "    print(f\"  → Best: {best_in_scenario['title_weight']}x (F1={best_in_scenario['f1_score']:.4f})\")\n",
    "    \n",
    "    results_interaction.extend(scenario_results)\n",
    "\n",
    "# Summary analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERACTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_interaction = pd.DataFrame(results_interaction)\n",
    "\n",
    "# Pivot table for visualization\n",
    "pivot = df_interaction.pivot_table(\n",
    "    values='f1_score',\n",
    "    index='scenario',\n",
    "    columns='title_weight',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "print(\"\\nF1 Scores by Scenario and Title Weight:\")\n",
    "print(pivot.to_string())\n",
    "\n",
    "# Find best weight for each scenario\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Optimal title_weight per scenario:\")\n",
    "print(\"-\"*80)\n",
    "for scenario_name in df_interaction['scenario'].unique():\n",
    "    scenario_data = df_interaction[df_interaction['scenario'] == scenario_name]\n",
    "    best_row = scenario_data.loc[scenario_data['f1_score'].idxmax()]\n",
    "    print(f\"{scenario_name:40s} → {best_row['title_weight']}x (F1={best_row['f1_score']:.4f})\")\n",
    "\n",
    "# Count how many scenarios prefer each weight\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Weight preference distribution:\")\n",
    "print(\"-\"*80)\n",
    "best_weights = []\n",
    "for scenario_name in df_interaction['scenario'].unique():\n",
    "    scenario_data = df_interaction[df_interaction['scenario'] == scenario_name]\n",
    "    best_weight = scenario_data.loc[scenario_data['f1_score'].idxmax()]['title_weight']\n",
    "    best_weights.append(best_weight)\n",
    "\n",
    "from collections import Counter\n",
    "weight_counts = Counter(best_weights)\n",
    "for weight in sorted(weight_counts.keys()):\n",
    "    count = weight_counts[weight]\n",
    "    print(f\"  Weight {weight}x: {count}/{len(scenarios)} scenarios\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "if weight_counts[1.5] >= len(scenarios) / 2:\n",
    "    print(\"✓ Title weight 1.5x is robust across different configurations\")\n",
    "    print(\"  (optimal or near-optimal in majority of scenarios)\")\n",
    "else:\n",
    "    majority_weight = max(weight_counts.items(), key=lambda x: x[1])[0]\n",
    "    print(f\"⚠ Parameter interaction detected!\")\n",
    "    print(f\"  Weight {majority_weight}x is preferred in most scenarios\")\n",
    "    print(f\"  May need to adjust title_weight based on other hyperparameters\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6e. Parameter Interaction: Title Weight Consistency\n",
    "\n",
    "Test whether optimal title_weight (1.5x) holds across different hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STATISTICAL VALIDATION: Cross-Validation + Significance Testing\n",
      "================================================================================\n",
      "\n",
      "Running 5-fold cross-validation for title weights: [1.0, 1.5, 2.0]\n",
      "Dataset: Development set (72178 samples)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title weight = 1.0x\n",
      "----------------------------------------\n",
      "  Fold 1: F1 = 0.8172\n",
      "  Fold 2: F1 = 0.8162\n",
      "  Fold 3: F1 = 0.8227\n",
      "  Fold 4: F1 = 0.8183\n",
      "  Fold 5: F1 = 0.8158\n",
      "  Mean F1: 0.8180 ± 0.0025\n",
      "  95% CI: [0.8158, 0.8202]\n",
      "\n",
      "Title weight = 1.5x\n",
      "----------------------------------------\n",
      "  Fold 1: F1 = 0.8214\n",
      "  Fold 2: F1 = 0.8180\n",
      "  Fold 3: F1 = 0.8273\n",
      "  Fold 4: F1 = 0.8233\n",
      "  Fold 5: F1 = 0.8208\n",
      "  Mean F1: 0.8222 ± 0.0031\n",
      "  95% CI: [0.8195, 0.8249]\n",
      "\n",
      "Title weight = 2.0x\n",
      "----------------------------------------\n",
      "  Fold 1: F1 = 0.8195\n",
      "  Fold 2: F1 = 0.8185\n",
      "  Fold 3: F1 = 0.8265\n",
      "  Fold 4: F1 = 0.8223\n",
      "  Fold 5: F1 = 0.8208\n",
      "  Mean F1: 0.8215 ± 0.0028\n",
      "  95% CI: [0.8191, 0.8240]\n",
      "\n",
      "================================================================================\n",
      "PAIRED T-TESTS (Comparing title weights)\n",
      "================================================================================\n",
      "\n",
      "Test 1: Weight 1.5x vs 1.0x (baseline)\n",
      "  Mean improvement: +0.0041\n",
      "  t-statistic: 6.9113\n",
      "  p-value: 0.0023\n",
      "  Significant at α=0.05? YES\n",
      "\n",
      "Test 2: Weight 2.0x vs 1.0x (baseline)\n",
      "  Mean improvement: +0.0035\n",
      "  t-statistic: 6.5497\n",
      "  p-value: 0.0028\n",
      "  Significant at α=0.05? YES\n",
      "\n",
      "Test 3: Weight 1.5x vs 2.0x\n",
      "  Mean difference: +0.0007\n",
      "  t-statistic: 1.5520\n",
      "  p-value: 0.1956\n",
      "  Significant at α=0.05? NO\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "Baseline (1.0x): F1 = 0.8180 ± 0.0025\n",
      "Optimal  (1.5x): F1 = 0.8222 ± 0.0031\n",
      "Alternative (2.0x): F1 = 0.8215 ± 0.0028\n",
      "\n",
      "✓ Title weight 1.5x provides statistically significant improvement over baseline\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STATISTICAL VALIDATION: Cross-Validation + Significance Testing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select weights to validate: baseline (1.0), optimal (1.5), and alternative (2.0)\n",
    "weights_to_validate = [1.0, 1.5, 2.0]\n",
    "n_folds = 5\n",
    "\n",
    "print(f\"\\nRunning {n_folds}-fold cross-validation for title weights: {weights_to_validate}\")\n",
    "print(f\"Dataset: Development set ({len(df_dev)} samples)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Store CV scores for each weight\n",
    "cv_scores_by_weight = {}\n",
    "\n",
    "for weight in weights_to_validate:\n",
    "    print(f\"\\nTitle weight = {weight}x\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Import pipeline components\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from rakuten_text.vectorization import FeatureWeighter\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from scipy.sparse import hstack\n",
    "    \n",
    "    # Build pipeline for this weight\n",
    "    tfidf_title = TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )\n",
    "    \n",
    "    tfidf_desc = TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )\n",
    "    \n",
    "    # Prepare cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df_dev, y_dev), 1):\n",
    "        X_fold_train = df_dev.iloc[train_idx]\n",
    "        X_fold_val = df_dev.iloc[val_idx]\n",
    "        y_fold_train = y_dev.iloc[train_idx]\n",
    "        y_fold_val = y_dev.iloc[val_idx]\n",
    "        \n",
    "        # Vectorize\n",
    "        title_train = tfidf_title.fit_transform(X_fold_train['title_clean'])\n",
    "        title_val = tfidf_title.transform(X_fold_val['title_clean'])\n",
    "        \n",
    "        desc_train = tfidf_desc.fit_transform(X_fold_train['desc_clean'])\n",
    "        desc_val = tfidf_desc.transform(X_fold_val['desc_clean'])\n",
    "        \n",
    "        # Apply weighting\n",
    "        if weight != 1.0:\n",
    "            title_train = title_train * weight\n",
    "            title_val = title_val * weight\n",
    "        \n",
    "        # Combine\n",
    "        X_fold_train_vec = hstack([title_train, desc_train])\n",
    "        X_fold_val_vec = hstack([title_val, desc_val])\n",
    "        \n",
    "        # Train and evaluate\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "        clf.fit(X_fold_train_vec, y_fold_train)\n",
    "        \n",
    "        from sklearn.metrics import f1_score\n",
    "        y_pred = clf.predict(X_fold_val_vec)\n",
    "        fold_f1 = f1_score(y_fold_val, y_pred, average='weighted')\n",
    "        fold_scores.append(fold_f1)\n",
    "        \n",
    "        print(f\"  Fold {fold_idx}: F1 = {fold_f1:.4f}\")\n",
    "    \n",
    "    cv_scores_by_weight[weight] = fold_scores\n",
    "    mean_f1 = np.mean(fold_scores)\n",
    "    std_f1 = np.std(fold_scores)\n",
    "    \n",
    "    print(f\"  Mean F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    print(f\"  95% CI: [{mean_f1 - 1.96*std_f1/np.sqrt(n_folds):.4f}, {mean_f1 + 1.96*std_f1/np.sqrt(n_folds):.4f}]\")\n",
    "\n",
    "# Statistical significance testing\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PAIRED T-TESTS (Comparing title weights)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_scores = cv_scores_by_weight[1.0]\n",
    "optimal_scores = cv_scores_by_weight[1.5]\n",
    "alternative_scores = cv_scores_by_weight[2.0]\n",
    "\n",
    "# Test 1: Optimal (1.5x) vs Baseline (1.0x)\n",
    "t_stat_1, p_value_1 = stats.ttest_rel(optimal_scores, baseline_scores)\n",
    "print(f\"\\nTest 1: Weight 1.5x vs 1.0x (baseline)\")\n",
    "print(f\"  Mean improvement: {np.mean(optimal_scores) - np.mean(baseline_scores):+.4f}\")\n",
    "print(f\"  t-statistic: {t_stat_1:.4f}\")\n",
    "print(f\"  p-value: {p_value_1:.4f}\")\n",
    "print(f\"  Significant at α=0.05? {'YES' if p_value_1 < 0.05 else 'NO'}\")\n",
    "\n",
    "# Test 2: Alternative (2.0x) vs Baseline (1.0x)\n",
    "t_stat_2, p_value_2 = stats.ttest_rel(alternative_scores, baseline_scores)\n",
    "print(f\"\\nTest 2: Weight 2.0x vs 1.0x (baseline)\")\n",
    "print(f\"  Mean improvement: {np.mean(alternative_scores) - np.mean(baseline_scores):+.4f}\")\n",
    "print(f\"  t-statistic: {t_stat_2:.4f}\")\n",
    "print(f\"  p-value: {p_value_2:.4f}\")\n",
    "print(f\"  Significant at α=0.05? {'YES' if p_value_2 < 0.05 else 'NO'}\")\n",
    "\n",
    "# Test 3: Optimal (1.5x) vs Alternative (2.0x)\n",
    "t_stat_3, p_value_3 = stats.ttest_rel(optimal_scores, alternative_scores)\n",
    "print(f\"\\nTest 3: Weight 1.5x vs 2.0x\")\n",
    "print(f\"  Mean difference: {np.mean(optimal_scores) - np.mean(alternative_scores):+.4f}\")\n",
    "print(f\"  t-statistic: {t_stat_3:.4f}\")\n",
    "print(f\"  p-value: {p_value_3:.4f}\")\n",
    "print(f\"  Significant at α=0.05? {'YES' if p_value_3 < 0.05 else 'NO'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Baseline (1.0x): F1 = {np.mean(baseline_scores):.4f} ± {np.std(baseline_scores):.4f}\")\n",
    "print(f\"Optimal  (1.5x): F1 = {np.mean(optimal_scores):.4f} ± {np.std(optimal_scores):.4f}\")\n",
    "print(f\"Alternative (2.0x): F1 = {np.mean(alternative_scores):.4f} ± {np.std(alternative_scores):.4f}\")\n",
    "\n",
    "if p_value_1 < 0.05:\n",
    "    print(f\"\\n✓ Title weight 1.5x provides statistically significant improvement over baseline\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Title weight 1.5x improvement is NOT statistically significant (p={p_value_1:.4f})\")\n",
    "    \n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Expérience 3: Impact des Features Manuelles\n",
    "\n",
    "Test des 5 stratégies complètes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: Impact des features manuelles\n",
      "\n",
      "\n",
      "================================================================================\n",
      "COMPARAISON COMPLÈTE DES STRATÉGIES DE VECTORISATION\n",
      "================================================================================\n",
      "Modèles à tester: ['logreg']\n",
      "Max features: 10000\n",
      "N-gram range: (1, 2)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "📊 STRATÉGIE 1: Indicateurs seuls\n",
      "--------------------------------------------------------------------------------\n",
      "  Modèle: LOGREG\n",
      "    F1: 0.2919, Acc: 0.3398\n",
      "\n",
      "\n",
      "📊 STRATÉGIE 2: CountVectorizer seul\n",
      "--------------------------------------------------------------------------------\n",
      "  Modèle: LOGREG\n",
      "\n",
      "================================================================================\n",
      "Expérience: COUNT + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "    F1: 0.8159, Acc: 0.8144\n",
      "\n",
      "\n",
      "📊 STRATÉGIE 3: TF-IDF seul\n",
      "--------------------------------------------------------------------------------\n",
      "  Modèle: LOGREG\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "    F1: 0.8231, Acc: 0.8225\n",
      "\n",
      "\n",
      "📊 STRATÉGIE 4: Indicateurs + CountVectorizer\n",
      "--------------------------------------------------------------------------------\n",
      "  Modèle: LOGREG\n",
      "\n",
      "================================================================================\n",
      "Expérience: COUNT + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "    F1: 0.8227, Acc: 0.8218\n",
      "\n",
      "\n",
      "📊 STRATÉGIE 5: Indicateurs + TF-IDF\n",
      "--------------------------------------------------------------------------------\n",
      "  Modèle: LOGREG\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "    F1: 0.8276, Acc: 0.8270\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RÉSUMÉ DES RÉSULTATS (triés par F1-Score)\n",
      "================================================================================\n",
      "                     strategy vectorizer  model  use_features  f1_score  accuracy  train_time  predict_time\n",
      "         Indicateurs + TF-IDF      tfidf logreg          True  0.827647  0.827007   29.794660      1.025685\n",
      "                  TF-IDF seul      tfidf logreg         False  0.823086  0.822481   18.151027      1.520383\n",
      "Indicateurs + CountVectorizer      count logreg          True  0.822655  0.821834   68.933068      1.453714\n",
      "         CountVectorizer seul      count logreg         False  0.815917  0.814353   65.763412      0.636656\n",
      "            Indicateurs seuls       None logreg          True  0.291866  0.339799    4.551644      0.002008\n",
      "================================================================================\n",
      "\n",
      "🏆 MEILLEURE STRATÉGIE: Indicateurs + TF-IDF\n",
      "   Modèle: LOGREG\n",
      "   F1-Score: 0.8276\n",
      "   Accuracy: 0.8270\n",
      "================================================================================\n",
      "\n",
      "\n",
      "✓ Comparaison terminée\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 3: Impact des features manuelles\\n\")\n",
    "\n",
    "# Utiliser la fonction run_strategy_comparison\n",
    "results_strategies = run_strategy_comparison(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=feature_columns,\n",
    "    models=['logreg'],  # Seul LogReg pour contrôle\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Comparaison terminée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Expérience 4: Grid Search Hyperparamètres\n",
    "\n",
    "Optimisation de max_features et ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search avec:\n",
      "  Vectorizer: tfidf\n",
      "  Features: True\n",
      "  Title weight: 1.5x (optimal de Exp 2b)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GRID SEARCH: TFIDF Hyperparameters\n",
      "================================================================================\n",
      "Total expériences: 12\n",
      "  max_features: [5000, 10000, 15000, 20000]\n",
      "  ngram_range: [(1, 1), (1, 2), (1, 3)]\n",
      "  title_weight: 1.5\n",
      "  model: logreg\n",
      "================================================================================\n",
      "\n",
      "[1/12] max_features=5000, ngram_range=(1, 1)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 5000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8242, Acc: 0.8239\n",
      "\n",
      "[2/12] max_features=5000, ngram_range=(1, 2)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 5000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8221, Acc: 0.8218\n",
      "\n",
      "[3/12] max_features=5000, ngram_range=(1, 3)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 5000\n",
      "  N-gram range: (1, 3)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8182, Acc: 0.8180\n",
      "\n",
      "[4/12] max_features=10000, ngram_range=(1, 1)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8321, Acc: 0.8315\n",
      "\n",
      "[5/12] max_features=10000, ngram_range=(1, 2)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8312, Acc: 0.8311\n",
      "\n",
      "[6/12] max_features=10000, ngram_range=(1, 3)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 10000\n",
      "  N-gram range: (1, 3)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8306, Acc: 0.8302\n",
      "\n",
      "[7/12] max_features=15000, ngram_range=(1, 1)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 15000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8340, Acc: 0.8335\n",
      "\n",
      "[8/12] max_features=15000, ngram_range=(1, 2)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 15000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8336, Acc: 0.8332\n",
      "\n",
      "[9/12] max_features=15000, ngram_range=(1, 3)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 15000\n",
      "  N-gram range: (1, 3)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8348, Acc: 0.8345\n",
      "\n",
      "[10/12] max_features=20000, ngram_range=(1, 1)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8351, Acc: 0.8345\n",
      "\n",
      "[11/12] max_features=20000, ngram_range=(1, 2)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8339, Acc: 0.8335\n",
      "\n",
      "[12/12] max_features=20000, ngram_range=(1, 3)\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 3)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "  → F1: 0.8343, Acc: 0.8338\n",
      "\n",
      "\n",
      "================================================================================\n",
      "MEILLEURE CONFIGURATION:\n",
      "================================================================================\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 1)\n",
      "  F1-Score: 0.8351\n",
      "  Accuracy: 0.8345\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Déterminer le meilleur vectorizer depuis Exp 3\n",
    "best_from_exp3 = results_strategies.iloc[0]\n",
    "best_vectorizer_type = best_from_exp3['vectorizer']\n",
    "use_features = best_from_exp3['use_features']\n",
    "\n",
    "# Extraire le meilleur title_weight depuis Exp 2b\n",
    "best_title_weight = results_weighting.iloc[0]['title_weight']\n",
    "\n",
    "print(f\"Grid search avec:\")\n",
    "print(f\"  Vectorizer: {best_vectorizer_type}\")\n",
    "print(f\"  Features: {use_features}\")\n",
    "print(f\"  Title weight: {best_title_weight}x (optimal de Exp 2b)\")\n",
    "print()\n",
    "\n",
    "# Grid search avec title_weight optimal\n",
    "grid_results = run_hyperparameter_grid(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    vectorizer_type=best_vectorizer_type if best_vectorizer_type != 'None' else 'tfidf',\n",
    "    max_features_list=[5000, 10000, 15000, 20000],\n",
    "    ngram_range_list=[(1, 1), (1, 2), (1, 3)],\n",
    "    model_name='logreg',\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=feature_columns if use_features else None,\n",
    "    title_weight=best_title_weight,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEEP ANALYSIS: Unigrams vs Bigrams Performance\n",
      "================================================================================\n",
      "\n",
      "Finding from Grid Search:\n",
      "  Unigrams (1,1): F1 = 0.8444 (20k features)\n",
      "  Bigrams  (1,2): F1 = 0.8417 (20k features)\n",
      "  → Unigrams are +0.27% better\n",
      "\n",
      "Hypotheses to test:\n",
      "  H1: Bigrams introduce noise (too sparse, less reliable)\n",
      "  H2: Bigrams cause overfitting (too specific to training data)\n",
      "  H3: Feature budget dilution (20k split between unigrams+bigrams)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "H3: Feature Budget Dilution Analysis\n",
      "================================================================================\n",
      "\n",
      "Vocabulary composition (max_features=20k):\n",
      "  (1,1) model: 20,000 unigrams\n",
      "  (1,2) model: 20,000 total features\n",
      "    ├─ Unigrams: 7,915 (39.6%)\n",
      "    └─ Bigrams:  12,085 (60.4%)\n",
      "\n",
      "================================================================================\n",
      "H1: Sparsity Analysis\n",
      "================================================================================\n",
      "\n",
      "Sparsity (title column):\n",
      "  (1,1) model: 99.95% sparse\n",
      "  (1,2) model: 99.66% sparse\n",
      "  → Bigrams add -0.30% more sparsity\n",
      "\n",
      "Average non-zero features per document:\n",
      "  (1,1) model: 9.1 features/doc\n",
      "  (1,2) model: 68.5 features/doc\n",
      "  → Bigrams use +652.2% more features per doc\n",
      "\n",
      "================================================================================\n",
      "H2: Overfitting Test (Train/Val Gap)\n",
      "================================================================================\n",
      "\n",
      "Retraining models to measure train/val gap...\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 2)\n",
      "  Features manuelles: Non\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Unigrams (1,1):\n",
      "  Train F1: 0.9147\n",
      "  Val F1:   0.8257\n",
      "  Gap:      0.0890\n",
      "\n",
      "Bigrams (1,2):\n",
      "  Train F1: 0.9122\n",
      "  Val F1:   0.8261\n",
      "  Gap:      0.0861\n",
      "\n",
      "Overfitting difference: -0.0028\n",
      "  → No significant difference in overfitting\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "\n",
      "Evidence summary:\n",
      "  H1 (Sparsity): Bigrams are -0.30% more sparse\n",
      "  H2 (Overfitting): Gap difference = -0.0028\n",
      "  H3 (Budget dilution): Only 7,915/20,000 unigrams retained\n",
      "\n",
      "Interpretation:\n",
      "  ✓ Primary factor: Feature budget dilution\n",
      "    Adding bigrams forces removal of important unigrams\n",
      "    Bigrams don't compensate for lost unigram information\n",
      "\n",
      "Recommendation:\n",
      "  → Use unigrams only (1,1) for optimal performance\n",
      "  → Consider higher max_features if bigrams are needed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DEEP ANALYSIS: Unigrams vs Bigrams Performance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFinding from Grid Search:\")\n",
    "print(\"  Unigrams (1,1): F1 = 0.8444 (20k features)\")\n",
    "print(\"  Bigrams  (1,2): F1 = 0.8417 (20k features)\")\n",
    "print(\"  → Unigrams are +0.27% better\")\n",
    "print(\"\\nHypotheses to test:\")\n",
    "print(\"  H1: Bigrams introduce noise (too sparse, less reliable)\")\n",
    "print(\"  H2: Bigrams cause overfitting (too specific to training data)\")\n",
    "print(\"  H3: Feature budget dilution (20k split between unigrams+bigrams)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Build vectorizers for comparison\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_uni = TfidfVectorizer(max_features=20000, ngram_range=(1, 1), min_df=2, max_df=0.95)\n",
    "tfidf_bi = TfidfVectorizer(max_features=20000, ngram_range=(1, 2), min_df=2, max_df=0.95)\n",
    "\n",
    "# Vectorize title (representative column)\n",
    "title_train_uni = tfidf_uni.fit_transform(X_train['title_clean'])\n",
    "title_train_bi = tfidf_bi.fit_transform(X_train['desc_clean'])\n",
    "\n",
    "# Analysis 1: Vocabulary composition\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"H3: Feature Budget Dilution Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vocab_uni = tfidf_uni.vocabulary_\n",
    "vocab_bi = tfidf_bi.vocabulary_\n",
    "\n",
    "# Count actual unigrams vs bigrams in (1,2) model\n",
    "unigrams_in_bi = [term for term in vocab_bi.keys() if ' ' not in term]\n",
    "bigrams_in_bi = [term for term in vocab_bi.keys() if ' ' in term]\n",
    "\n",
    "print(f\"\\nVocabulary composition (max_features=20k):\")\n",
    "print(f\"  (1,1) model: {len(vocab_uni):,} unigrams\")\n",
    "print(f\"  (1,2) model: {len(vocab_bi):,} total features\")\n",
    "print(f\"    ├─ Unigrams: {len(unigrams_in_bi):,} ({len(unigrams_in_bi)/len(vocab_bi)*100:.1f}%)\")\n",
    "print(f\"    └─ Bigrams:  {len(bigrams_in_bi):,} ({len(bigrams_in_bi)/len(vocab_bi)*100:.1f}%)\")\n",
    "\n",
    "# Analysis 2: Sparsity comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"H1: Sparsity Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sparsity_uni = 1.0 - (title_train_uni.nnz / (title_train_uni.shape[0] * title_train_uni.shape[1]))\n",
    "sparsity_bi = 1.0 - (title_train_bi.nnz / (title_train_bi.shape[0] * title_train_bi.shape[1]))\n",
    "\n",
    "print(f\"\\nSparsity (title column):\")\n",
    "print(f\"  (1,1) model: {sparsity_uni*100:.2f}% sparse\")\n",
    "print(f\"  (1,2) model: {sparsity_bi*100:.2f}% sparse\")\n",
    "print(f\"  → Bigrams add {(sparsity_bi - sparsity_uni)*100:+.2f}% more sparsity\")\n",
    "\n",
    "# Average non-zero features per document\n",
    "avg_nnz_uni = title_train_uni.nnz / title_train_uni.shape[0]\n",
    "avg_nnz_bi = title_train_bi.nnz / title_train_bi.shape[0]\n",
    "\n",
    "print(f\"\\nAverage non-zero features per document:\")\n",
    "print(f\"  (1,1) model: {avg_nnz_uni:.1f} features/doc\")\n",
    "print(f\"  (1,2) model: {avg_nnz_bi:.1f} features/doc\")\n",
    "print(f\"  → Bigrams use {(avg_nnz_bi - avg_nnz_uni)/avg_nnz_uni*100:+.1f}% more features per doc\")\n",
    "\n",
    "# Analysis 3: Overfitting test (train vs val performance gap)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"H2: Overfitting Test (Train/Val Gap)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nRetraining models to measure train/val gap...\")\n",
    "\n",
    "# Unigrams\n",
    "result_uni_train = run_single_experiment(\n",
    "    X_train, X_train, y_train, y_train,\n",
    "    vectorizer_type='tfidf',\n",
    "    strategy='split',\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=None,\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 1),\n",
    "    model_name='logreg',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "result_uni_val = run_single_experiment(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    vectorizer_type='tfidf',\n",
    "    strategy='split',\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=None,\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 1),\n",
    "    model_name='logreg',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Bigrams\n",
    "result_bi_train = run_single_experiment(\n",
    "    X_train, X_train, y_train, y_train,\n",
    "    vectorizer_type='tfidf',\n",
    "    strategy='split',\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=None,\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2),\n",
    "    model_name='logreg',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "result_bi_val = run_single_experiment(\n",
    "    X_train, X_val, y_train, y_val,\n",
    "    vectorizer_type='tfidf',\n",
    "    strategy='split',\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=None,\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2),\n",
    "    model_name='logreg',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "gap_uni = result_uni_train['f1_score'] - result_uni_val['f1_score']\n",
    "gap_bi = result_bi_train['f1_score'] - result_bi_val['f1_score']\n",
    "\n",
    "print(f\"\\nUnigrams (1,1):\")\n",
    "print(f\"  Train F1: {result_uni_train['f1_score']:.4f}\")\n",
    "print(f\"  Val F1:   {result_uni_val['f1_score']:.4f}\")\n",
    "print(f\"  Gap:      {gap_uni:.4f}\")\n",
    "\n",
    "print(f\"\\nBigrams (1,2):\")\n",
    "print(f\"  Train F1: {result_bi_train['f1_score']:.4f}\")\n",
    "print(f\"  Val F1:   {result_bi_val['f1_score']:.4f}\")\n",
    "print(f\"  Gap:      {gap_bi:.4f}\")\n",
    "\n",
    "print(f\"\\nOverfitting difference: {(gap_bi - gap_uni):.4f}\")\n",
    "if gap_bi > gap_uni + 0.01:\n",
    "    print(\"  → Bigrams show MORE overfitting\")\n",
    "elif gap_uni > gap_bi + 0.01:\n",
    "    print(\"  → Unigrams show MORE overfitting\")\n",
    "else:\n",
    "    print(\"  → No significant difference in overfitting\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nEvidence summary:\")\n",
    "print(f\"  H1 (Sparsity): Bigrams are {(sparsity_bi-sparsity_uni)*100:+.2f}% more sparse\")\n",
    "print(f\"  H2 (Overfitting): Gap difference = {(gap_bi-gap_uni):.4f}\")\n",
    "print(f\"  H3 (Budget dilution): Only {len(unigrams_in_bi):,}/{len(vocab_uni):,} unigrams retained\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if len(unigrams_in_bi) < len(vocab_uni) * 0.8:\n",
    "    print(\"  ✓ Primary factor: Feature budget dilution\")\n",
    "    print(\"    Adding bigrams forces removal of important unigrams\")\n",
    "    print(\"    Bigrams don't compensate for lost unigram information\")\n",
    "elif (gap_bi - gap_uni) > 0.02:\n",
    "    print(\"  ✓ Primary factor: Overfitting\")\n",
    "    print(\"    Bigrams are too specific and don't generalize well\")\n",
    "elif (sparsity_bi - sparsity_uni) > 0.10:\n",
    "    print(\"  ✓ Primary factor: Sparsity\")\n",
    "    print(\"    Bigrams are too sparse to be reliable\")\n",
    "else:\n",
    "    print(\"  ⚠ Unclear - multiple factors at play\")\n",
    "    \n",
    "print(\"\\nRecommendation:\")\n",
    "print(\"  → Use unigrams only (1,1) for optimal performance\")\n",
    "print(\"  → Consider higher max_features if bigrams are needed\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Analyse & Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANALYSE DES RÉSULTATS\n",
      "================================================================================\n",
      "\n",
      "Nombre d'expériences: 12\n",
      "F1-Score moyen: 0.8304\n",
      "F1-Score médian: 0.8329\n",
      "F1-Score min: 0.8182\n",
      "F1-Score max: 0.8351\n",
      "Écart-type: 0.0056\n",
      "\n",
      "Baseline F1: 0.7858\n",
      "Expériences supérieures à baseline: 12 / 12\n",
      "Meilleure amélioration: +6.28%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PERFORMANCE PAR MODÈLE:\n",
      "--------------------------------------------------------------------------------\n",
      "            mean       max  count\n",
      "model                            \n",
      "logreg  0.830352  0.835141     12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdEAAAHqCAYAAADrpwd3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAlYpJREFUeJzs3Xd4FNX+x/HPpCdLQktCgkBCr1IElKg0EZAgKqAXK82GojQRBVEIFpTiBRtYkIAIcpFyRRDxegmKolJFEEG8FMXE0CS97vz+yC8LS7IhCcnukrxfz7OPM2fPzPnOnATPfnP2jGGapikAAAAAAAAAAFCAh6sDAAAAAAAAAADAXZFEBwAAAAAAAADAAZLoAAAAAAAAAAA4QBIdAAAAAAAAAAAHSKIDAAAAAAAAAOAASXQAAAAAAAAAABwgiQ4AAAAAAAAAgAMk0QEAAAAAAAAAcIAkOgAAAAAAAAAADpBEB+BysbGxMgzD9vLz81NYWJi6d++u6dOnKzExscAxU6dOlWEYJWonLS1NU6dOVVxcXImOK6ytyMhI3XzzzSU6z8UsXbpUc+bMKfQ9wzA0derUMm3vUpTm/l+qoUOH2v2cnP/69NNPbfXmzJmjAQMGqH79+jIMQ926dStxW99//7369++vevXqydfXV7Vq1VJUVJSeeOKJMrwiAAAA93HhmNzLy0t16tTRsGHDdPz48TJtKysrSyNGjFB4eLg8PT3Vtm3bMj0/nKtdu3a64oorlJub67DOddddp+DgYGVlZZVZu6X9fFdc+b8TR44cKZfzA7i8eLk6AADIt3DhQjVr1kzZ2dlKTEzUli1b9Morr2jWrFlavny5brzxRlvdBx54QDfddFOJzp+WlqaYmBhJKlFitTRtlcbSpUu1d+9ejRkzpsB7W7duVZ06dco9Bnfn7++v//73vwXKmzVrZtueP3++LBaLbrjhBq1du7bEbaxbt0633HKLunXrphkzZig8PFzx8fHavn27PvroI82ePfuSrgEAAMCd5Y/J09PT9dVXX2n69OnavHmzfvrpJ1ksljJpY968eXr77bf1+uuvq3379qpSpUqZnBeucf/99+vxxx/X559/rujo6ALvHzx4UN9++63GjBkjHx+fMmu3tJ/viqtv377aunWrwsPDy/zcAC4/JNEBuI1WrVqpQ4cOtv2BAwdq7Nixuv766zVgwAD9+uuvqlWrliSpTp065Z5UTktLU0BAgFPauphOnTq5tH134eHhcdF78fPPP8vDI++LVq1atSpxGzNmzFD9+vX1+eefy8vr3P8m77zzTs2YMaPE57sU+T+DAAAAznL+mLx79+7Kzc3V888/rzVr1uiee+65pHPnj2327t0rf39/PfbYY2URsiQpPT1d/v7+ZXY+FORobHrPPffoySef1Pvvv19oEv3999+XJA0fPrzcYywL6enp8vPzU0hIiEJCQpzaNuN/wH2xnAsAt1avXj3Nnj1bycnJevvtt23lhS0n8t///lfdunVTzZo15e/vr3r16mngwIFKS0vTkSNHbAOgmJgY29dUhw4dane+nTt36vbbb1f16tXVsGFDh23lW716tVq3bi0/Pz81aNBAr732mt37jr4CGBcXJ8MwbF897Natm9atW6ejR4/afY02X2HLuezdu1e33nqrqlevLj8/P7Vt21aLFi0qtJ1ly5bpmWeeUe3atRUUFKQbb7xRBw4ccHzjz7Nu3Tq1bdtWvr6+ql+/vmbNmlVoPdM09dZbb6lt27by9/dX9erVdfvtt+t///ufXb1du3bp5ptvVmhoqHx9fVW7dm317dtXf/zxR7HiuZj8BHppnTp1SsHBwXYJ9KLOvXTpUkVFRalKlSqqUqWK2rZtqwULFtjVef/999WmTRv5+fmpRo0a6t+/v/bv329XZ+jQoapSpYp++ukn9erVS4GBgerRo4ekvK88v/DCC2rWrJl8fX0VEhKiYcOG6cSJE3bnKOp3AAAAoDTyJzAcPXpUUvHHfN26dVOrVq301Vdf6dprr1VAQICGDx8uwzD03nvvKT093TbmjY2NlSRlZGRo4sSJql+/vnx8fHTFFVdo5MiR+vvvv+3Onb+04qpVq9SuXTv5+fkpJibGNvZdunSpnnrqKYWHh6tKlSrq16+f/vrrLyUnJ+uhhx5ScHCwgoODNWzYMKWkpNid+80331SXLl0UGhoqi8WiK6+8UjNmzFB2dnah17dt2zZ17txZAQEBatCggV5++WVZrVa7un///beeeOIJNWjQQL6+vgoNDVV0dLR++eUXW53ijvcKkz+O3Ldvn3r06CGLxaKQkBA99thjBcaBJb2+C/uvMNWrV1f//v21du1anTp1yu693NxcffDBB+rYsaOuvPJKSdKvv/6qu+++2/Z5oHnz5nrzzTcLnLeo+3axz3eStGXLFvXo0UOBgYEKCAjQtddeq3Xr1tm1kf95bePGjRo+fLhCQkIUEBCgzMzMAp/l8n++CntFRkbanXf58uWKioqSxWJRlSpV1Lt3b+3atavQfits/A/A/TATHYDbi46Olqenp7766iuHdY4cOaK+ffuqc+fOev/991WtWjUdP35cGzZsUFZWlsLDw7VhwwbddNNNuv/++/XAAw9IUoGZBQMGDNCdd96pESNGKDU1tci4du/erTFjxmjq1KkKCwvThx9+qNGjRysrK0vjx48v0TW+9dZbeuihh/Tbb79p9erVF61/4MABXXvttQoNDdVrr72mmjVrasmSJRo6dKj++usvTZgwwa7+pEmTdN111+m9995TUlKSnnrqKfXr10/79++Xp6enw3a+/PJL3XrrrYqKitJHH32k3NxczZgxQ3/99VeBug8//LBiY2M1atQovfLKKzp9+rSmTZuma6+9Vj/++KNq1aql1NRU9ezZU/Xr19ebb76pWrVqKSEhQZs2bVJycnKx7lVOTo7dvmEYRV5DSUVFRem9997TqFGjdM899+iqq66St7d3oXWfe+45Pf/88xowYICeeOIJVa1aVXv37rV9yJSk6dOna9KkSbrrrrs0ffp0nTp1SlOnTlVUVJS2bdumxo0b2+pmZWXplltu0cMPP6ynn35aOTk5slqtuvXWW/X1119rwoQJuvbaa3X06FFNmTJF3bp10/bt2+Xv73/R3wFmtAAAgNI4dOiQpHPj5uKM+fLFx8fr3nvv1YQJE/TSSy/Jw8NDY8aM0fPPP69NmzbZlulr2LChTNPUbbfdpi+//FITJ05U586dtWfPHk2ZMkVbt27V1q1b5evrazv3zp07tX//fk2ePFn169eXxWKxjd8nTZqk7t27KzY2VkeOHNH48eN11113ycvLS23atNGyZcu0a9cuTZo0SYGBgXYTYX777TfdfffdtkT+jz/+qBdffFG//PKLbUZ1voSEBN1zzz164oknNGXKFK1evVoTJ05U7dq1NXjwYElScnKyrr/+eh05ckRPPfWUrrnmGqWkpOirr75SfHy8mjVrVuzxXlGys7MVHR1tG0d+++23euGFF3T06FG7JQ5Lcn2F9Z8j999/v5YtW6YlS5Zo9OjRtvLPP/9cf/75p5577jlJed8avfbaa22TpcLCwvT5559r1KhROnnypKZMmVKs+3bttdcW+flu8+bN6tmzp1q3bq0FCxbI19dXb731lvr166dly5Zp0KBBdvEPHz5cffv21QcffKDU1NRCx/9XXXWVtm7dalf266+/6v7771fLli1tZS+99JImT56sYcOGafLkycrKytLMmTPVuXNn/fDDD2rRooWtbmHjfwBuygQAF1u4cKEpydy2bZvDOrVq1TKbN29u258yZYp5/j9hH3/8sSnJ3L17t8NznDhxwpRkTpkypcB7+ed77rnnHL53voiICNMwjALt9ezZ0wwKCjJTU1Ptru3w4cN29TZt2mRKMjdt2mQr69u3rxkREVFo7BfGfeedd5q+vr7msWPH7Or16dPHDAgIMP/++2+7dqKjo+3q/etf/zIlmVu3bi20vXzXXHONWbt2bTM9Pd1WlpSUZNaoUcPunmzdutWUZM6ePdvu+N9//9309/c3J0yYYJqmaW7fvt2UZK5Zs6bIdgszZMgQU1KB13XXXefwmJYtW5pdu3YtUTsnT540r7/+etv5vb29zWuvvdacPn26mZycbKv3v//9z/T09DTvueceh+c6c+aM6e/vX+D+Hzt2zPT19TXvvvvuAtf3/vvv29VdtmyZKclcuXKlXfm2bdtMSeZbb71lmmbxfgcAAAAcyR+3fvfdd2Z2draZnJxsfvrpp2ZISIgZGBhoJiQkFHvMZ5qm2bVrV1OS+eWXXxZoa8iQIabFYrEr27BhgynJnDFjhl358uXLTUnmO++8YyuLiIgwPT09zQMHDtjVzR/79uvXz658zJgxpiRz1KhRduW33XabWaNGDYf3JDc318zOzjYXL15senp6mqdPny5wfd9//73dMS1atDB79+5t2582bZopyfziiy8ctlPc8Z4j+ePIuXPn2pW/+OKLpiRzy5Ytpb6+wvqvMFar1axfv77ZunVru/KBAweaAQEB5tmzZ03TNM3evXubderUse3ne+yxx0w/Pz9bDMW5b0V9vuvUqZMZGhpqN37PyckxW7VqZdapU8e0Wq2maZ77uR88eHCBczj6LJfvr7/+Mhs0aGC2bNnSPHPmjGmaeeN8Ly8v8/HHH7erm5ycbIaFhZn/+Mc/bGWOxv8A3BPLuQC4LJimWeT7bdu2lY+Pjx566CEtWrSowNdJi2vgwIHFrtuyZUu1adPGruzuu+9WUlKSdu7cWar2i+u///2vevToobp169qVDx06VGlpaQVmSNxyyy12+61bt5YkuxnTF0pNTdW2bds0YMAA+fn52coDAwPVr18/u7qffvqpDMPQvffeq5ycHNsrLCxMbdq0sS1b06hRI1WvXl1PPfWU5s+fr59//rlE1+3v769t27bZvS5cOqU4TNO0i/P8GR81a9bU119/rW3btunll1/WrbfeqoMHD2rixIm68sordfLkSUnSF198odzcXI0cOdJhO1u3blV6errd10olqW7durrhhhv05ZdfFjjmwp/BTz/9VNWqVVO/fv3s4m3btq3CwsJs97asfgcAAEDl1qlTJ3l7eyswMFA333yzwsLC9Nlnn6lWrVrFHvPlq169um644YZitZs/K/3CcdMdd9whi8VSYNzUunVrNWnSpNBz3XzzzXb7zZs3l5T3oMgLy0+fPm23pMuuXbt0yy23qGbNmvL09JS3t7cGDx6s3NxcHTx40O74sLAwXX311QXiOn+M/dlnn6lJkya68cYbHV16scd7F3PhmvV33323JGnTpk2lur6S9J9hGBo2bJj27NmjHTt2SMpbJnHt2rUaOHCggoKClJGRoS+//FL9+/dXQECA3bVGR0crIyND3333naTi3TdHUlNT9f333+v222+3e2itp6en7rvvPv3xxx8FlrYsyefA/Db69u2rjIwMffbZZ6pWrZqkvJn3OTk5Gjx4sN31+fn5qWvXroX2ZUnbBuAaJNEBuL3U1FSdOnVKtWvXdlinYcOG+s9//qPQ0FCNHDlSDRs2VMOGDTV37twStVWSJ6+HhYU5LLtwLcCydurUqUJjzb9HF7Zfs2ZNu/38r8Kmp6c7bOPMmTOyWq1FXme+v/76S6ZpqlatWvL29rZ7fffdd7bEc9WqVbV582a1bdtWkyZNUsuWLVW7dm1NmTKlwDqMhfHw8FCHDh3sXk2bNr3ocRfavHlzgTgvXLe+Q4cOeuqpp7RixQr9+eefGjt2rI4cOWJ7uGj++pRFPXQ2vx8c9dWF/RQQEKCgoCC7sr/++kt///23fHx8CsSckJBgu7dl9TsAAAAqt8WLF2vbtm3atWuX/vzzT+3Zs0fXXXedpOKP+fKVZGx96tQpeXl5FVhu0TAMhYWFFRg3FXXuGjVq2O37+PgUWZ6RkSFJOnbsmDp37qzjx49r7ty5tokV+et1Xzh2vnCMLeWNs8+vd+LEiSLHi1Lxx3tF8fLyKhDPhZ9NSnp9Jek/SRo2bJg8PDy0cOFCSdKHH36orKws3X///bY4cnJy9Prrrxe4zvwHkuZfa3HumyNnzpyRaZol+rxUkmvNycnR7bffroMHD2r9+vV2E5vyl73s2LFjgWtcvnx5gb4sbPwPwD2xJjoAt7du3Trl5uaqW7duRdbr3LmzOnfurNzcXG3fvl2vv/66xowZo1q1aunOO+8sVluOHiBamISEBIdl+QPY/BncmZmZdvWKMxAuSs2aNRUfH1+g/M8//5QkBQcHX9L5pbyZJ4ZhFHmd+YKDg2UYhr7++mu7tSrznV925ZVX6qOPPpJpmtqzZ49iY2M1bdo0+fv76+mnn77kuIujffv22rZtm11ZUX+k8fb21pQpU/TPf/5Te/fulXRuvcU//vijwDcC8uX/HDjqqwv7qbCfv+DgYNWsWVMbNmwotI3AwEDbdln8DgAAgMqtefPm6tChQ6HvlWTMJ5VsbF2zZk3l5OToxIkTdol00zSVkJCgjh07lvrcxbVmzRqlpqZq1apVioiIsJXv3r271OcMCQnRH3/8UWSdkoz3HMnJydGpU6fsEukXfjYp6fWV9B7XqVNHvXr10tKlSzV79mwtXLhQjRo1UpcuXSTlfb7Inw3u6Nuc9evXl1S8++ZI9erV5eHhUaLPSyW51oceekhffvml1q9fX+Cbyfnn/fjjj+3usSPl8XMMoHwwEx2AWzt27JjGjx+vqlWr6uGHHy7WMZ6enrrmmmtsMyryl1Ypzuzrkti3b59+/PFHu7KlS5cqMDBQV111lSTZntK+Z88eu3qffPJJgfNdOGulKD169NB///tf2yAw3+LFixUQEKBOnToV9zIcslgsuvrqq7Vq1Srb7Bwp7yE/5z+cSMr7yqxpmjp+/HiBmeIdOnTQlVdeWeD8hmGoTZs2+uc//6lq1aqV+xI45wsMDCwQY/5MpMIG25K0f/9+SeeS7b169ZKnp6fmzZvnsJ2oqCj5+/tryZIlduV//PGHbUmei7n55pt16tQp5ebmFnpvC5uJ7+h3AAAA4FKUZsxXXPnjogvHTStXrlRqamqxxk2XKj+hef4fA0zT1Lvvvlvqc/bp00cHDx60LVdTmNKM9wrz4Ycf2u0vXbpUkmyTkcrj+i50//3368yZM3ruuee0e/duDRs2zNZuQECAunfvrl27dql169aFXmt+wr84983R5zuLxaJrrrlGq1atsnvParVqyZIlqlOnjsOlgC5m8uTJWrhwod57771Cl5rp3bu3vLy89NtvvxV6fY7+QAXA/TETHYDb2Lt3r23NuMTERH399ddauHChPD09tXr16gJf7Tzf/Pnz9d///ld9+/ZVvXr1lJGRYXu6fP7gJjAwUBEREfr3v/+tHj16qEaNGgoODrYlukuqdu3auuWWWzR16lSFh4dryZIl+uKLL/TKK68oICBAUt7X+Jo2barx48crJydH1atX1+rVq7Vly5YC57vyyiu1atUqzZs3T+3bt7ctXVKYKVOm6NNPP1X37t313HPPqUaNGvrwww+1bt06zZgxQ1WrVi3VNV3o+eef10033aSePXvqiSeeUG5url555RVZLBadPn3aVu+6667TQw89pGHDhmn79u3q0qWLLBaL4uPjtWXLFl155ZV65JFH9Omnn+qtt97SbbfdpgYNGsg0Ta1atUp///23evbsWSYxb9++3bY0S1JSkkzT1Mcffywprz8uNiOkd+/eqlOnjvr166dmzZrJarVq9+7dmj17tqpUqaLRo0dLyvsDyaRJk/T8888rPT1dd911l6pWraqff/5ZJ0+eVExMjKpVq6Znn31WkyZN0uDBg3XXXXfp1KlTiomJkZ+fn6ZMmXLR67nzzjv14YcfKjo6WqNHj9bVV18tb29v/fHHH9q0aZNuvfVW9e/fv1i/AwAAAJeiuGO+0ujZs6d69+6tp556SklJSbruuuu0Z88eTZkyRe3atdN9991XxldTeAw+Pj666667NGHCBGVkZGjevHk6c+ZMqc85ZswYLV++XLfeequefvppXX311UpPT9fmzZt18803q3v37sUe7xXFx8dHs2fPVkpKijp27Khvv/1WL7zwgvr06aPrr7++3K7vQrfccouCg4M1c+ZMeXp6asiQIXbvz507V9dff706d+6sRx55RJGRkUpOTtahQ4e0du1aW9K8OPetqM9306dPV8+ePdW9e3eNHz9ePj4+euutt7R3714tW7asVDPAV6xYoRdffFG33367mjRpYlu/XcpL6Ldr106RkZGaNm2annnmGf3vf//TTTfdpOrVq+uvv/7SDz/8IIvFopiYmEu7yQBcwyWPMwWA8+Q/9Tz/5ePjY4aGhppdu3Y1X3rpJTMxMbHAMVOmTDHP/yds69atZv/+/c2IiAjT19fXrFmzptm1a1fzk08+sTvuP//5j9muXTvT19fXlGQOGTLE7nwnTpy4aFumaZoRERFm3759zY8//ths2bKl6ePjY0ZGRpqvvvpqgeMPHjxo9urVywwKCjJDQkLMxx9/3Fy3bp0pydy0aZOt3unTp83bb7/drFatmmkYhl2bKuSp8z/99JPZr18/s2rVqqaPj4/Zpk0bc+HChXZ1Nm3aZEoyV6xYYVd++PBhU1KB+oX55JNPzNatW5s+Pj5mvXr1zJdffrnQe2Kapvn++++b11xzjWmxWEx/f3+zYcOG5uDBg83t27ebpmmav/zyi3nXXXeZDRs2NP39/c2qVauaV199tRkbG3vROIYMGWJaLJZi1Tv/5+n8V3Gud/ny5ebdd99tNm7c2KxSpYrp7e1t1qtXz7zvvvvMn3/+uUD9xYsXmx07djT9/PzMKlWqmO3atSvQznvvvWe7h1WrVjVvvfVWc9++fcW+vuzsbHPWrFlmmzZtbO00a9bMfPjhh81ff/3VNM3i/w4AAAAUJn9Mvm3btovWvdiYzzRNs2vXrmbLli0LPd7RuCc9Pd186qmnzIiICNPb29sMDw83H3nkEfPMmTN29fLH4hdyNPZ1dG2FfQZYu3atbcx1xRVXmE8++aT52WefFRi7O7q+IUOGmBEREXZlZ86cMUePHm3Wq1fP9Pb2NkNDQ82+ffuav/zyi61OccZ7juTfzz179pjdunUz/f39zRo1apiPPPKImZKSYlf3Uq+vOMaOHWtKMqOjowt9//Dhw+bw4cPNK664wvT29jZDQkLMa6+91nzhhRfs6hXnvjn6fGeapvn111+bN9xwg+3ntFOnTubatWvt2ijq5z7/vcOHD5umee7npbDXhX2+Zs0as3v37mZQUJDp6+trRkREmLfffrv5n//8x1anuJ9vALgHwzRNs5zz9AAAAAAAACgHQ4cO1ccff6yUlBRXhwIAFRZrogMAAAAAAAAA4ABJdAAAAAAAAAAAHGA5FwAAAAAAAAAAHGAmOgAAAAAAAAAADpBEBwAAAAAAAADAAZLoAAAAAAAAAAA44OXqAFzBarXqzz//VGBgoAzDcHU4AAAAqARM01RycrJq164tDw/mslyIMToAAACcrbhj9EqZRP/zzz9Vt25dV4cBAACASuj3339XnTp1XB2G22GMDgAAAFe52Bi9UibRAwMDJeXdnKCgIKe0mWvN1e6E3ZKktmFt5enh6ZR2y5vVatWJEycUEhLCjKpKgP6uPOjryoX+rlwK9HdurrR7d96bbdtKnhVjjOKOkpKSVLduXdtYFPby78vRo0dVrVo11wZzmeDf79LhvpUc96zkuGclxz0rOe5ZyXHPSqci37fijtErZRI9/+uhQUFBTkuip2al6oblN0iSUiamyOJjcUq75c1qtSojI0NBQUEV7pcIBdHflQd9XbnQ35VLgf5OTZVuyBujKCVFslSMMYo7Y6mSwrlijH6549/v0uG+lRz3rOS4ZyXHPSs57lnJcc9KpzLct4uN0SvmVQMAAAAAAAAAUAZIogMAAAAAAAAA4ABJdAAAAAAAAAAAHCCJDgAAAAAAAACAAyTRAQAAAAAAAABwgCQ6AAAAAAAAAAAOeLk6gMrC29NbU7pOsW0DAAC4BW9vacqUc9sAAAAAADsk0Z3Ex9NHU7tNdXUYAAAA9nx8pKlTXR0FAAAAALgtlnMBAAAAAAAAAMABZqI7idW0av+J/ZKk5iHN5WHw9wsAAOAGrFZpf94YRc2bSx6MUQAAAADgfCTRnSQ9O12t5rWSJKVMTJHFx+LiiAAAACSlp0ut8sYoSkmRLIxRAAAAAOB8TDUCAAAAAAAAAMABt0qiT58+XYZhaMyYMUXW27x5s9q3by8/Pz81aNBA8+fPd06AAAAAAAAAAIBKxW2S6Nu2bdM777yj1q1bF1nv8OHDio6OVufOnbVr1y5NmjRJo0aN0sqVK50UKQAAAAAAAACgsnCLJHpKSoruuecevfvuu6pevXqRdefPn6969eppzpw5at68uR544AENHz5cs2bNclK0AAAAAAAAAIDKwi2S6CNHjlTfvn114403XrTu1q1b1atXL7uy3r17a/v27crOzi6vEAEAAAAAAAAAlZCXqwP46KOPtHPnTm3btq1Y9RMSElSrVi27slq1aiknJ0cnT55UeHh4gWMyMzOVmZlp209KSpIkWa1WWa3WS4i++M5vx5ntljer1SrTNCvM9aBo9HflQV9XLvR35VKgv61W26wKq9UqudnPwcmTJ21jt4sJCgpScHBwOUdUevyOAQAAAJcnlybRf//9d40ePVobN26Un59fsY8zDMNu3zTNQsvzTZ8+XTExMQXKT5w4oYyMjBJEXHpZuVl6pPUjkqQzp84o1TPVKe2WN6vVqrNnz8o0TXl4uMUXG1CO6O/Kg76uXOjvyqVAf2dlKfCRvDFK8pkzUqr7jFHOnj2r2XNfV0p65sUrS6ri76snRj+uqlWrlnNkpZOcnOzqEAAAAACUgkuT6Dt27FBiYqLat29vK8vNzdVXX32lN954Q5mZmfL09LQ7JiwsTAkJCXZliYmJ8vLyUs2aNQttZ+LEiRo3bpxtPykpSXXr1lVISIiCgoLK8IqK9satbzitLWexWq0yDEMhISEkXioB+rvyoK8rF/q7cim0v9/IG6P4uzCuwqSkpGj3zwcV0mmALDVqFVk39fRfOvTdKnl6eio0NNRJEZZMSSaNAAAAAHAfLk2i9+jRQz/99JNd2bBhw9SsWTM99dRTBRLokhQVFaW1a9falW3cuFEdOnSQt7d3oe34+vrK19e3QLmHhwfJgjJgGAb3shKhvysP+rpyob8rl8ulvw3DkGmaCqhRS4GhdYqsayrv24n51+aO3DUuAAAAAEVzaRI9MDBQrVq1siuzWCyqWbOmrXzixIk6fvy4Fi9eLEkaMWKE3njjDY0bN04PPvigtm7dqgULFmjZsmVOj78krKZVx84ekyTVq1pPHgYfogAAgBuwWqVjeWMU1asnkegFAAAAADsuf7DoxcTHx+tY/gc7SfXr19f69es1duxYvfnmm6pdu7Zee+01DRw40IVRXlx6drrqz60vSUqZmCKLj8XFEQEAAEhKT5fq541RlJIiWRijAAAAAMD53C6JHhcXZ7cfGxtboE7Xrl21c+dO5wQEAAAAAAAAAKi0+L4uAAAAAAAAAAAOkEQHAAAAAAAAAMABkugAAAAAAAAAADhAEh0AAAAAAAAAAAdIogMAAAAAAAAA4ICXqwOoLLw8vPRoh0dt2wAAAG7By0t69NFz2wAAAAAAO3xSchJfL1+92fdNV4cBAABgz9dXepMxCgAAAAA4wnIuAAAAAAAAAAA4wEx0JzFNUyfTTkqSggOCZRiGiyMCAACQZJrSybwxioKDJcYoAAAAAGCHJLqTpGWnKXRWqCQpZWKKLD4WF0cEAAAgKS1NCs0boyglRbIwRgEAAACA87GcCwAAAAAAAAAADpBEBwAAAAAAAADAAZLoAAAAAAAAAAA4QBIdAAAAAAAAAAAHSKIDAAAAAAAAAOAASXQAAAAAAAAAABzwcnUAlYWXh5eGtBli2wYAAHALXl7SkCHntgEAAAAAdvik5CS+Xr6KvS3W1WEAAADY8/WVYmNdHQUAAAAAuC2WcwEAAAAAAAAAwAFmojuJaZpKy06TJAV4B8gwDBdHBAAAIMk0pbS8MYoCAiTGKAAAAABgh5noTpKWnaYq06uoyvQqtmQ6AACAy6WlSVWq5L3SGKMAAAAAwIVIogMAAAAAAAAA4ABJdAAAAAAAAAAAHCCJDgAAAAAAAACAAyTRAQAAAAAAAABwgCQ6AAAAAAAAAAAOkEQHAAAAAAAAAMABL1cHUFl4enjq9ha327YBAADcgqendPvt57YBAAAAAHZIojuJn5efVtyxwtVhAAAA2PPzk1YwRgEAAAAAR1jOBQAAAAAAAAAAB0iiAwAAAAAAAADgAEl0J0nNSpURY8iIMZSalerqcAAAAPKkpkqGkfdKZYwCAAAAABciiQ4AAAAAAAAAgAMk0QEAAAAAAAAAcIAkOgAAAAAAAAAADpBEBwAAAAAAAADAAZLoAAAAAAAAAAA4QBIdAAAAAAAAAAAHXJpEnzdvnlq3bq2goCAFBQUpKipKn332mcP6cXFxMgyjwOuXX35xYtSl4+nhqejG0YpuHC1PD09XhwMAAJDH01OKjs57eTJGAQAAAIALebmy8Tp16ujll19Wo0aNJEmLFi3Srbfeql27dqlly5YOjztw4ICCgoJs+yEhIeUe66Xy8/LTurvXuToMAAAAe35+0jrGKAAAAADgiEuT6P369bPbf/HFFzVv3jx99913RSbRQ0NDVa1atXKODgAAAAAAAABQ2bk0iX6+3NxcrVixQqmpqYqKiiqybrt27ZSRkaEWLVpo8uTJ6t69e5H1MzMzlZmZadtPSkqSJFmtVlmt1ksPvhKzWq0yTZP7WEnQ35UHfV250N+Vy+XU36Zp5i3fJ8mQWWRdQ5JhGG59be4aFwAAAICiuTyJ/tNPPykqKkoZGRmqUqWKVq9erRYtWhRaNzw8XO+8847at2+vzMxMffDBB+rRo4fi4uLUpUsXh21Mnz5dMTExBcpPnDihjIyMMruWoqRlp6nV4laSpL2D9yrAO8Ap7ZY3q9Wqs2fPyjRNeXjwnNqKjv6uPOjryoX+rlwu7G8jLU0hrfLGKCf27pUZ4D5jlOTkZDWqH6FQixTgnVlk3SoWyat+hJKTk5WYmOikCEsmOTnZ1SEAAAAAKAWXJ9GbNm2q3bt36++//9bKlSs1ZMgQbd68udBEetOmTdW0aVPbflRUlH7//XfNmjWryCT6xIkTNW7cONt+UlKS6tatq5CQELu11ctTalaq0nPSJeWt4W7xsTil3fJmtVplGIZCQkJIvFQC9HflQV9XLvR35VKgv1NT5ZF+bowii/uMUVJSUnTo8FHlNJeCLL5F1k1KlY4cPqrAwECFhoY6KcKS8fPzc3UIAAAAAErB5Ul0Hx8f24NFO3TooG3btmnu3Ll6++23i3V8p06dtGTJkiLr+Pr6yte34AcvDw8PpyULzm/Hme06g2EYFe6a4Bj9XXnQ15UL/V252PX3BWMUudHPQP7yLKYkU0aRdU2dW/7FXX+O3TUuAAAAAEVzu5G8aZp265dfzK5duxQeHl6OEQEAAAAAAAAAKiuXzkSfNGmS+vTpo7p16yo5OVkfffSR4uLitGHDBkl5y7AcP35cixcvliTNmTNHkZGRatmypbKysrRkyRKtXLlSK1eudOVlAAAAAAAAAAAqKJcm0f/66y/dd999io+PV9WqVdW6dWtt2LBBPXv2lCTFx8fr2LFjtvpZWVkaP368jh8/Ln9/f7Vs2VLr1q1TdHS0qy4BAAAAAAAAAFCBuTSJvmDBgiLfj42NtdufMGGCJkyYUI4RAQAAAAAAAABwjssfLFpZeBge6hrR1bYNAADgFjw8pK5dz20DAAAAAOyQRHcSf29/xQ2Nc3UYAAAA9vz9pbg4V0cBAAAAAG6L6UYAAAAAAAAAADhAEh0AAAAAAAAAAAdIojtJalaqQmaGKGRmiFKzUl0dDgAAQJ7UVCkkJO+VyhgFAAAAAC7EmuhOdDLtpKtDAAAAKOgkYxQAAAAAcISZ6AAAAAAAAAAAOEASHQAAAAAAAAAAB0iiAwAAAAAAAADgAEl0AAAAAAAAAAAcIIkOAAAAAAAAAIADXq4OoLLwMDzUoXYH2zYAAIBb8PCQOnQ4tw0AAAAAsEMS3Un8vf217cFtrg4DAADAnr+/tI0xCgAAAAA4wnQjAAAAAAAAAAAcIIkOAAAAAAAAAIADJNGdJC07TZFzIhU5J1Jp2WmuDgcAACBPWpoUGZn3SmOMAgAAAAAXYk10JzFNU0fPHrVtAwAAuAXTlI4ePbcNAAAAALDDTHQAAAAAAAAAABwgiQ4AAAAAAAAAgAMk0QEAAAAAAAAAcIAkOgAAAAAAAAAADpBEBwAAAAAAAADAAS9XB1BZGIahFiEtbNsAAABuwTCkFi3ObQMAAAAA7JBEd5IA7wDte3Sfq8MAAACwFxAg7WOMAgAAAACOsJwLAAAAAAAAAAAOkEQHAAAAAAAAAMABkuhOkpadppZvtVTLt1oqLTvN1eEAAADkSUuTWrbMe6UxRgEAAACAC7EmupOYpqmfT/xs2wYAAHALpin9/PO5bcDFbp+5UZ5+FleHcVkwZKpeoKljyYZM8WDg4uK+lRz3rOS4ZyXHPSs57lnJcc9Kp7zv2+fP9i3zc5Y1ZqIDAAAAAAAAAOAASXQAAAAAAAAAABwgiQ4AAAAAAAAAgAMk0QEAAAAAAAAAcIAkOgAAAAAAAAAADni5OoDKwjAMRVSNsG0DAAC4BcOQIiLObQMAAAAA7JBEd5IA7wAdGXPE1WEAAADYCwiQjhxxdRQAAAAA4LZYzgUAAAAAAAAAAAdIogMAAAAAAAAA4IBLk+jz5s1T69atFRQUpKCgIEVFRemzzz4r8pjNmzerffv28vPzU4MGDTR//nwnRXtp0rPT1fHdjur4bkelZ6e7OhwAAIA86elSx455r3TGKAAAAABwIZeuiV6nTh29/PLLatSokSRp0aJFuvXWW7Vr1y61bNmyQP3Dhw8rOjpaDz74oJYsWaJvvvlGjz76qEJCQjRw4EBnh18iVtOq7X9ut20DAAC4BatV2r793DYAAAAAwI5Lk+j9+vWz23/xxRc1b948fffdd4Um0efPn6969eppzpw5kqTmzZtr+/btmjVrltsn0QEAAAAAAAAAlx+XJtHPl5ubqxUrVig1NVVRUVGF1tm6dat69eplV9a7d28tWLBA2dnZ8vb2LvS4zMxMZWZm2vaTkpIkSVarVVYnzbg6vx1ntlverFarTNOsMNeDotHflQd9XbnQ35VLgf62Wm3r+1mtVreajW6apgzDkCHJkFlkXUOSYRhu/bPsrnEBAAAAKJrLk+g//fSToqKilJGRoSpVqmj16tVq0aJFoXUTEhJUq1Ytu7JatWopJydHJ0+eVHh4eKHHTZ8+XTExMQXKT5w4oYyMjEu/iGJIy06zazfVO9Up7ZY3q9Wqs2fPyjRNeXjwnNqKjv6uPOjryoX+rlwu7G8jLU35o6sTJ07ITHWfMUpycrIa1Y9QqEUK8M4ssm4Vi+RVP0LJyclKTEx0UoQlk5yc7OoQAAAAAJSCy5PoTZs21e7du/X3339r5cqVGjJkiDZv3uwwkW4Yht2+aZqFlp9v4sSJGjdunG0/KSlJdevWVUhIiIKCgsrgKi4uNevcB9KQkBBZfCxOabe8Wa1WGYahkJAQEi+VAP1dedDXlQv9XbkU6O9U+zGKLO4zRklJSdGhw0eV01wKsvgWWTcpVTpy+KgCAwMVGhrqpAhLxs/Pz9UhAAAAACgFlyfRfXx8bA8W7dChg7Zt26a5c+fq7bffLlA3LCxMCQkJdmWJiYny8vJSzZo1Hbbh6+srX9+CH7w8PDycliw4vx1ntusMhmFUuGuCY/R35UFfVy70d+Vi198XjFHkRj8D+cuzmJJMOZ4wISmvzv8v/+KuP8fuGhcAAACAork8iX4h0zTt1i8/X1RUlNauXWtXtnHjRnXo0MHheujuJDgg2NUhAAAAFBTMGAUAAAAAHHFpEn3SpEnq06eP6tatq+TkZH300UeKi4vThg0bJOUtw3L8+HEtXrxYkjRixAi98cYbGjdunB588EFt3bpVCxYs0LJly1x5GcVi8bHoxJMnXB0GAACAPYtFOsEYBQAAAAAccWkS/a+//tJ9992n+Ph4Va1aVa1bt9aGDRvUs2dPSVJ8fLyOHTtmq1+/fn2tX79eY8eO1ZtvvqnatWvrtdde08CBA111CQAAAAAAAACACsylSfQFCxYU+X5sbGyBsq5du2rnzp3lFBEAAAAAAAAAAOfwdCMnSc9OV7fYbuoW203p2emuDgcAACBPerrUrVveK50xCgAAAABcyO0eLFpRWU2rNh/dbNsGAABwC1artHnzuW0AAAAAgB1mogMAAAAAAAAA4ABJdAAAAAAAAAAAHCCJDgAAAAAAAACAAyTRAQAAAAAAAABwgCQ6AAAAAAAAAAAOeLk6gMokwDvA1SEAAAAUFMAYBQAAAAAcIYnuJBYfi1Inpbo6DAAAAHsWi5TKGAUAAAAAHGE5FwAAAAAAAAAAHCCJDgAAAAAAAACAAyTRnSQjJ0N9l/ZV36V9lZGT4epwAAAA8mRkSH375r0yGKMAAAAAwIVYE91Jcq25Wv/rets2AACAW8jNldavP7cNAAAAALDDTHQAAAAAAAAAABwgiQ4AAAAAAAAAgAMk0QEAAAAAAAAAcIAkOgAAAAAAAAAADpBEBwAAAAAAAADAAZLoAAAAAAAAAAA44OXqACoLi49F5hTT1WEAAADYs1gkkzEKAAAAADjCTHQAAAAAAAAAABwgiQ4AAAAAAAAAgAMk0Z0kIydDd6y4Q3esuEMZORmuDgcAACBPRoZ0xx15rwzGKAAAAABwIZLoTpJrzdXHP3+sj3/+WLnWXFeHAwAAkCc3V/r447xXLmMUAAAAALgQSXQAAAAAAAAAABwgiQ4AAAAAAAAAgAMk0QEAAAAAAAAAcIAkOgAAAAAAAAAADpBEBwAAAAAAAADAAZLoAAAAAAAAAAA44OXqACqLAO8ApUxMsW0DAAC4hYAAKSXl3DYAAAAAwA5JdCcxDEMWH4urwwAAALBnGJKFMQoAAAAAOMJyLgAAAAAAAAAAOEAS3UkyczI1dM1QDV0zVJk5ma4OBwAAIE9mpjR0aN4rkzEKAAAAAFyIJLqT5FhztOjHRVr04yLlWHNcHQ4AAECenBxp0aK8Vw5jFAAAAAC4EEl0AAAAAAAAAAAcIIkOAAAAAAAAAIADLk2iT58+XR07dlRgYKBCQ0N122236cCBA0UeExcXJ8MwCrx++eUXJ0UNAAAAAAAAAKgsXJpE37x5s0aOHKnvvvtOX3zxhXJyctSrVy+lpqZe9NgDBw4oPj7e9mrcuLETIgYAAAAAAAAAVCZermx8w4YNdvsLFy5UaGioduzYoS5duhR5bGhoqKpVq1aO0QEAAAAAAAAAKju3WhP97NmzkqQaNWpctG67du0UHh6uHj16aNOmTeUdGgAAAAAAAACgEnLpTPTzmaapcePG6frrr1erVq0c1gsPD9c777yj9u3bKzMzUx988IF69OihuLg4h7PXMzMzlZmZadtPSkqSJFmtVlmt1rK9EAf8PP2UMC7Btu2sdsub1WqVaZoV5npQNPq78qCvKxf6u3Ip0N9+flJCwrltN/o5ME0z7/k3kgyZRdY1JBmG4dY/y+4aFwAAAICiuU0S/bHHHtOePXu0ZcuWIus1bdpUTZs2te1HRUXp999/16xZsxwm0adPn66YmJgC5SdOnFBGRsalBV4KJ1JPOL3N8mK1WnX27FmZpikPD7f6YgPKAf1dedDXlQv9XbkU2d8n3GuMkpycrEb1IxRqkQK8M4usW8UiedWPUHJyshITE50UYckkJye7OgQAAAAApeAWSfTHH39cn3zyib766ivVqVOnxMd36tRJS5Yscfj+xIkTNW7cONt+UlKS6tatq5CQEAUFBZUqZuSxWq0yDEMhISEkXioB+rvyoK8rF/q7crmc+jslJUWHDh9VTnMpyOJbZN2kVOnI4aMKDAxUaGiokyIsGT8/P1eHAAAAAKAUXJpEN01Tjz/+uFavXq24uDjVr1+/VOfZtWuXwsPDHb7v6+srX9+CH7w8PDyc9uExMydT4z7PS+S/2vtV+XoV/UHwcmIYhlPvJVyL/q486OvKhf6uXOz6OzNTyp9s8OqrUiFjJlfJX57FlGTKKLKuqXPLv7jrz7G7xgUAAACgaC5Noo8cOVJLly7Vv//9bwUGBirh/9fjrFq1qvz9/SXlzSI/fvy4Fi9eLEmaM2eOIiMj1bJlS2VlZWnJkiVauXKlVq5c6bLrKI4ca47e2v6WJGlGzxnylft8QAUAAJVYTo70Vt4YRTNmuFUSHQAAAADcgUuT6PPmzZMkdevWza584cKFGjp0qCQpPj5ex44ds72XlZWl8ePH6/jx4/L391fLli21bt06RUdHOytsAAAAAAAAAEAl4fLlXC4mNjbWbn/ChAmaMGFCOUUEAAAAuK/s7Gw99NBDevbZZ9WgQQNXhwMAAABUCpe8MOOhQ4f0+eefKz09XVLxEuMAAAAASs7b21urV692dRgAAABApVLqJPqpU6d04403qkmTJoqOjlZ8fLwk6YEHHtATTzxRZgECAAAAOKd///5as2aNq8MAAAAAKo1SL+cyduxYeXl56dixY2revLmtfNCgQRo7dqxmz55dJgECAAAAOKdRo0Z6/vnn9e2336p9+/ayWCx2748aNcpFkQEAAAAVU6mT6Bs3btTnn3+uOnXq2JU3btxYR48eveTAAAAAABT03nvvqVq1atqxY4d27Nhh955hGCTRAQAAgDJW6iR6amqqAgICCpSfPHlSvr6+lxRUReTv7a/Dow/btgEAANyCv790+PC5bbi9w/n9BQAAAMApSr0mepcuXbR48WLbvmEYslqtmjlzprp3714mwVUkHoaHIqtFKrJapDyMS36eKwAAQNnw8JAiI/NeHoxRLidZWVk6cOCAcnJyXB0KAAAAUKGVeib6zJkz1a1bN23fvl1ZWVmaMGGC9u3bp9OnT+ubb74pyxgBAAAA/L+0tDQ9/vjjWrRokSTp4MGDatCggUaNGqXatWvr6aefdnGEAAAAQMVS6ulGLVq00J49e3T11VerZ8+eSk1N1YABA7Rr1y41bNiwLGOsELJys/Tkxif15MYnlZWb5epwAAAA8mRlSU8+mffKYoxyOZg4caJ+/PFHxcXFyc/Pz1Z+4403avny5S6MDAAAAKiYSj0TXZLCwsIUExNTVrFUaNm52Zq1dZYkaWq3qfLx9HFxRAAAAJKys6VZeWMUTZ0q+TBGcXdr1qzR8uXL1alTJxmGYStv0aKFfvvtNxdGBgAAAFRMpZ6JvnDhQq1YsaJA+YoVK2xfLQUAAABQtk6cOKHQ0NAC5ampqXZJdQAAAABlo9RJ9JdfflnBwcEFykNDQ/XSSy9dUlAAAAAACtexY0etW7fOtp+fOH/33XcVFRXlqrAAAACACqvUy7kcPXpU9evXL1AeERGhY8eOXVJQAAAAAAo3ffp03XTTTfr555+Vk5OjuXPnat++fdq6das2b97s6vAAAACACqfUM9FDQ0O1Z8+eAuU//vijataseUlBAQAAACjctddeq2+++UZpaWlq2LChNm7cqFq1amnr1q1q3769q8MDAAAAKpxSz0S/8847NWrUKAUGBqpLly6SpM2bN2v06NG68847yyxAAAAAAPauvPJKnkMEAAAAOEmpZ6K/8MILuuaaa9SjRw/5+/vL399fvXr10g033MCa6AAAAEA5ueGGGxQTE1Og/MyZM7rhhhtcEBEAAABQsZV6JrqPj4+WL1+u559/Xj/++KP8/f115ZVXKiIioizjqzD8vf2195G9tm0AAAC34O8v7d17bhtuLy4uTj/99JN27dqlDz/8UBaLRZKUlZXFmugAAABAOSh1Ej1fkyZN1KRJk7KIpULzMDzUMrSlq8MAAACw5+EhtWSMcrn5z3/+o4cfflidOnXS2rVrFRkZ6eqQAAAAgAqr1En03NxcxcbG6ssvv1RiYqKsVqvd+//9738vOTgAAAAABYWHh2vz5s0aPny4OnbsqBUrVqh58+auDgsAAACokEqdRB89erRiY2PVt29ftWrVSoZhlGVcFU5WbpZe+jpvrfhJnSfJx9PHxREBAABIysqS8p9nM2mS5MMYxd3lj7t9fX314Ycf6oUXXtBNN92kp556ysWRAQAAABVTqZPoH330kf71r38pOjq6LOOpsLJzsxWzOe8BUE9e+yRJdAAA4B6ys6X8h1Q++SRJ9MuAaZp2+5MnT1bz5s01ZMgQF0UEAAAAVGyX9GDRRo0alWUsAAAAAC7i8OHDCg4OtisbOHCgmjZtqh07drgoKgAAAKDi8ijtgU888YTmzp1bYCYMAAAAgPITEREhD4+Cw/hWrVoxGx0AAAAoB6Weib5lyxZt2rRJn332mVq2bClvb2+791etWnXJwQEAAACQBgwYoNjYWAUFBWnAgAFF1mUcDgAAAJStUifRq1Wrpv79+5dlLAAAAAAKUbVqVdsDRatWreriaAAAAIDKpdRJ9IULF5ZlHAAAAAAcyB97m6apqVOnKiQkRAEBAS6OCgAAAKgcSr0muiTl5OToP//5j95++20lJydLkv7880+lpKSUSXAAAAAAzjFNU40bN9bx48ddHQoAAABQaZR6JvrRo0d100036dixY8rMzFTPnj0VGBioGTNmKCMjQ/Pnzy/LOC97fl5++uGBH2zbAAAAbsHPT/rhh3PbcGseHh5q3LixTp06pcaNG7s6HAAAAKBSKPVM9NGjR6tDhw46c+aM/P39beX9+/fXl19+WSbBVSSeHp7qeEVHdbyiozw9PF0dDgAAQB5PT6ljx7yXJ2OUy8GMGTP05JNPau/eveVy/m+++UZXXnmlvL29ddtttykuLk6GYejvv/92eExsbKyqVatmV/bOO++obt268vDw0Jw5c8olVgAAAMAZSj0TfcuWLfrmm2/k4+NjVx4REcHXSwEAAIBycu+99yotLU1t2rSRj4+P3YQWSTp9+nSxz9WtWze1bdvWLsk9btw4tW3bVp999pmqVKmigIAAxcfHl+iBpklJSXrsscf06quvauDAgTwMFQAAAJe1UifRrVarcnNzC5T/8ccfCgwMvKSgKqKs3CzN/W6uJGl0p9Hy8fS5yBEAAABOkJUlzc0bo2j0aMmHMYq7K+9Z3b/99ptGjBihOnXq2MrCwsJKdI5jx44pOztbffv2VXh4eFmHCAAAADhVqZPoPXv21Jw5c/TOO+9IkgzDUEpKiqZMmaLo6OgyC7CiyM7N1oT/TJAkPdrxUZLoAADAPWRnSxPyxih69FGS6JeBIUOGlMl5hg4dqs2bN2vz5s2am/+HlP83fPhwDR8+XAsXLlRkZKS6d++uM2fO2JZsiY2N1XPPPaeTJ0+qd+/euv76623HxsbGatiwYZKkBg0aSJIOHz6syMjIMokbAAAAcLZSJ9H/+c9/qnv37mrRooUyMjJ0991369dff1VwcLCWLVtWljECAAAAOE9ubq7WrFmj/fv3yzAMtWjRQrfccos8S7Cu/dy5c3Xw4EG1atVK06ZNs33LtEWLFpo2bZoGDRqkqlWr6vvvv7c77vvvv9fw4cP10ksvacCAAdqwYYOmTJlie3/QoEGqW7eubrzxRv3www+qW7euQkJCyubCAQAAABcodRK9du3a2r17t5YtW6adO3fKarXq/vvv1z333FNgXUYAAAAAZePQoUOKjo7W8ePH1bRpU5mmqYMHD6pu3bpat26dGjZsWKzzVK1aVT4+PgoICLBbrsUwDFWtWtXhEi5z585V79699fTTT0uSmjRpom+//VYbNmyQJPn7+6tmzZqSpJCQEIfnyczMVGZmpm0/KSmpWHEDAAAAzlbqJLqUN0DO/6onAAAAgPI3atQoNWzYUN99951q1KghSTp16pTuvfdejRo1SuvWrSvX9vfv36/+/fvblUVFRdmS6MU1ffp0xcTElGVoAAAAQLkodRJ98eLFRb4/ePDg0p4aAAAAgAObN2+2S6BLUs2aNfXyyy/ruuuuK/f2TdMsk/NMnDhR48aNs+0nJSWpbt26ZXJuAAAAoCyVOok+evRou/3s7GylpaXZvhJKEh0AAAAoe76+vkpOTi5QnpKSIp8SPhjWx8fHthZ6cbVo0ULfffedXdmF+8Xh6+srX1/fEh8HAAAAOJtHaQ88c+aM3SslJUUHDhzQ9ddfz4NFAQAAgHJy880366GHHtL3338v0zRlmqa+++47jRgxQrfcckuJzhUZGanvv/9eR44c0cmTJ2W1Wi96zKhRo7RhwwbNmDFDBw8e1BtvvFHipVwAAACAy0mpk+iFady4sV5++eUCs9Qh+Xn5adOQTdo0ZJP8vPxcHQ4AAEAePz9p06a8lx9jlMvBa6+9poYNGyoqKkp+fn7y8/PTddddp0aNGmnOnDklOtf48ePl6empFi1aKCQkRMeOHbvoMZ06ddJ7772n119/XW3bttXGjRs1efLkUl4NAAAA4P4u6cGihfH09NSff/5ZrLrTp0/XqlWr9Msvv8jf31/XXnutXnnlFTVt2rTI4zZv3qxx48Zp3759ql27tiZMmKARI0aURfjlxtPDU90iu7k6DAAAAHuenlK3bq6OAiVQrVo1/fvf/9ahQ4e0f/9+maapFi1aqFGjRiU+V5MmTbR161a7sr///ttuv1u3bgXWQR8+fLiGDx9uV/bEE0/Yttu2bVtma6cDAAAArlbqJPonn3xit2+apuLj4/XGG28U+4FGmzdv1siRI9WxY0fl5OTomWeeUa9evfTzzz/LYrEUeszhw4cVHR2tBx98UEuWLNE333yjRx99VCEhIRo4cGBpLwcAAAC4LEybNk3jx49Xo0aN7BLn6enpmjlzpp577jkXRgcAAABUPKVOot922212+4ZhKCQkRDfccINmz55drHNcuHbiwoULFRoaqh07dqhLly6FHjN//nzVq1fP9lXV5s2ba/v27Zo1a5ZbJ9Gzc7P1zo53JEkPtX9I3p7eLo4IAABAUna29E7eGEUPPSR5M0ZxdzExMRoxYoQCAgLsytPS0hQTE0MSHQAAAChjpU6iF+ehQyV19uxZSVKNGjUc1tm6dat69eplV9a7d28tWLBA2dnZ8i7kg19mZqYyMzNt+0lJSZLyrqE8rqMwGdkZeuyzxyRJg1sPlqfh6ZR2y5vVapVpmk67j3At+rvyoK8rF/q7cinQ3xkZ8ngsb4xiHTw4b3kXN2GapgzDkCHJUNFLgxiScrOzdeTIkYsuIxIUFKTg4OCyC7SYyup3LP++XOjHH38schwNAAAAoHTKfE300jJNU+PGjdP111+vVq1aOayXkJCgWrVq2ZXVqlVLOTk5OnnypMLDwwscM336dMXExBQoP3HihDIyMi49+GJIy06zazfVO9Up7ZY3q9Wqs2fPyjRNeXiU6XNq4Ybo78qDvq5c6O/K5cL+NtLSlD+yOnHihMxU9xmjJCcnq1H9CIVapADvzCLr+nqmKbGKv+Yv/KDQSRXnq+LvqydGP66qVauWZbgXlZycfEnHV69ePe+PCoahJk2a2CXSc3NzlZKS4vbPCQIAAAAuR6VOoo8bN67YdV999dWL1nnssce0Z88ebdmy5aJ1L5x5kz/bqLAZOZI0ceJEu3iTkpJUt25dhYSEKCgo6KLtlYXUrHMfSENCQmTxKXzN98uN1Wq1LeVD4qXio78rD/q6cqG/K5cC/Z1qP0aRg+fSuEJKSooOHT6qnOZSkMW3yLp/JiZp976Dat+yt2qGRzisl3r6Lx36bpU8PT0VGhpa1iEXyc/P75KOnzNnjkzT1PDhwxUTE2P3RwAfHx9FRkYqKirqUsMEAAAAcIFSJ9F37dqlnTt3KicnR02bNpUkHTx4UJ6enrrqqqts9Rwlts/3+OOP65NPPtFXX32lOnXqFFk3LCxMCQkJdmWJiYny8vJSzZo1Cz3G19dXvr4FP3h5eHg4LVlwfjvObNcZDMOocNcEx+jvyoO+rlzo78rFrr8vGKPIjX4GDMOQaZoyJZkqekxpKu8PBP7VQhQY6ng8aerccijO/nm/1PaGDBkiSapfv76uu+46eXm5zZdKAQAAgAqt1CPvfv36KTAwUIsWLVL16tUlSWfOnNGwYcPUuXNnPfHEExc9h2maevzxx7V69WrFxcWpfv36Fz0mKipKa9eutSvbuHGjOnTocNGv7gIAAACXu9TUVH355Zfq3bu3Xfnnn38uq9WqPn36uCgyAAAAoGIq9XSY2bNna/r06bYEupS3TuMLL7yg2bNnF+scI0eO1JIlS7R06VIFBgYqISFBCQkJSk9Pt9WZOHGiBg8ebNsfMWKEjh49qnHjxmn//v16//33tWDBAo0fP760lwIAAABcNp5++mnl5uYWKDdNU08//bQLIgIAAAAqtlIn0ZOSkvTXX38VKE9MTCz2Q5PmzZuns2fPqlu3bgoPD7e9li9fbqsTHx+vY8eO2fbr16+v9evXKy4uTm3bttXzzz+v1157TQMHDiztpQAAAACXjV9//VUtWrQoUN6sWTMdOnTIBREBAAAAFVupl3Pp37+/hg0bptmzZ6tTp06SpO+++05PPvmkBgwYUKxz5D8QtCixsbEFyrp27aqdO3eWKF5X8/Xy1ad3fWrbBgAAcAu+vtKnn57bhturWrWq/ve//ykyMtKu/NChQ7K40YNhAQAAgIqi1En0+fPna/z48br33nuVnZ2ddzIvL91///2aOXNmmQVYUXh5eKlvk76uDgMAAMCel5fUlzHK5eSWW27RmDFjtHr1ajVs2FBSXgL9iSee0C233OLi6AAAAICKp9RJ9ICAAL311luaOXOmfvvtN5mmqUaNGjH7BQAAAChHM2fO1E033aRmzZqpTp06kqQ//vhDnTt31qxZs1wcHQAAAFDxlDqJni8+Pl7x8fHq0qWL/P39ZZqmDMMoi9gqlOzcbH3404eSpHuuvEfent4ujggAAEBSdrb0Yd4YRffcI3kzRnF3VatW1bfffqsvvvhCP/74o/z9/dW6dWt16dLF1aEBAAAAFVKxk+hWq1UeHueeQ3rq1Cn94x//0KZNm2QYhn799Vc1aNBADzzwgKpVq6bZs2eXS8CXq6zcLA379zBJ0h0t7iCJDgAA3ENWljQsb4yiO+4giX6ZMAxDvXr1UpcuXeTr68skFgAAAKAceVy8Sp5XX31V69evt+2PHTtW3t7eOnbsmAICAmzlgwYN0oYNG8o2SgAAAACS8ia3PP/887riiitUpUoVHT58WJL07LPPasGCBS6ODgAAAKh4ip1E79mzp0aPHm0bmG/cuFGvvPKKbR3GfI0bN9bRo0fLNkoAAAAAkqQXXnhBsbGxmjFjhnx8fGzlV155pd577z0XRgYAAABUTMVOordp00Y//PCD1q5dK0lKTU21m4Ge7+TJk/L19S27CAEAAADYLF68WO+8847uueceeXp62spbt26tX375xYWRAQAAABVTsZPoklS9enWtWbNGktSlSxctXrzY9p5hGLJarZo5c6a6d+9epkECAAAAyHP8+HE1atSoQLnValV2drYLIgIAAAAqtmI/WPRCM2fOVLdu3bR9+3ZlZWVpwoQJ2rdvn06fPq1vvvmmLGMEAAAA8P9atmypr7/+WhEREXblK1asULt27VwUFQAAAFBxlTqJ3qJFC+3Zs0fz5s2Tp6enUlNTNWDAAI0cOVLh4eFlGSMAAACA/zdlyhTdd999On78uKxWq1atWqUDBw5o8eLF+vTTT10dHgAAAFDhlCqJnp2drV69euntt99WTExMWcdUIfl6+epft//Ltg0AAOAWfH2lf/3r3DbcXr9+/bR8+XK99NJLMgxDzz33nK666iqtXbtWPXv2dHV4AAAAQIVTqiS6t7e39u7dK8MwyjqeCsvLw0t3tLzD1WEAAADY8/KS7mCMcjkZNmyY7r33XsXFxTEeBwAAAJygRA8WPd/gwYO1YMGCsowFAAAAwEWcOnVKffv2VZ06dTR+/Hjt3r3b1SEBAAAAFVqp10TPysrSe++9py+++EIdOnSQxWKxe//VV1+95OAqkhxrjlbvXy1J6t+8v7w8Sn3rAQAAyk5OjrQ6b4yi/v3zZqbDrX3yySf6+++/9a9//UtLly7VP//5TzVt2lT33nuv7r77bkVGRro6RAAAAKBCKfGnpP/973+KjIzU3r17ddVVV0mSDh48aFeHr5UWlJmTqX98/A9JUsrEFHn58AEVAAC4gcxM6R95YxSlpJBEv0xUq1ZNDz30kB566CH98ccfWrZsmd5//30999xzysnJcXV4AAAAQIVS4k9JjRs3Vnx8vDZt2iRJGjRokF577TXVqlWrzIMDAAAA4Fh2dra2b9+u77//XkeOHGFMDgAAAJSDEq+Jbpqm3f5nn32m1NTUMgsIAAAAQNE2bdqkBx98ULVq1dKQIUMUGBiotWvX6vfff3d1aAAAAECFc8nf170wqQ4AAACg/NSpU0enTp1S79699fbbb6tfv37y8/NzdVgAAABAhVXiJLphGAXWPGcNdAAAAMA5nnvuOd1xxx2qXr26q0MBAAAAKoUSJ9FN09TQoUPl6+srScrIyNCIESNksVjs6q1atapsIgQAAABg89BDD7k6BAAAAKBSKXESfciQIXb79957b5kFAwAAAAAAAACAOylxEn3hwoXlEUeF5+Ppo4W3LrRtAwAAuAUfHyl/fOfDGAUAAAAALnTJDxZF8Xh7emto26GuDgMAAMCet7c0dKirowAAAAAAt+Xh6gAAAAAAAAAAAHBXzER3khxrjj4/9LkkqXej3vLy4NYDAAA3kJMjfZ43RlHv3pIXYxQAAAAAOB+fkpwkMydTNy+7WZKUMjFFXj7cegAA4AYyM6Wb88YoSkkhiQ4AAAAAF2A5FwAAAAAAAAAAHCCJDgAAAAAAAACAAyTRAQAAAAAAAABwgCQ6AAAAAAAAAAAOkEQHAAAAAAAAAMABkugAAAAAAAAAADjg5eoAKgsfTx+90ecN2zYAAIBb8PGR3njj3DYAAAAAwA5JdCfx9vTWyKtHujoMAAAAe97e0kjGKAAAAADgCMu5AAAAAAAAAADgADPRnSTXmquvj30tSepcr7M8PTxdHBEAAICk3Fzp67wxijp3ljwZowAAAADA+UiiO0lGToa6L+ouSUqZmCKLj8XFEQEAAEjKyJC6541RlJIiWRijAAAAAMD5XL6cy1dffaV+/fqpdu3aMgxDa9asKbJ+XFycDMMo8Prll1+cEzAAAAAAAAAAoNJw+Uz01NRUtWnTRsOGDdPAgQOLfdyBAwcUFBRk2w8JCSmP8AAAAAAAAAAAlZjLk+h9+vRRnz59SnxcaGioqlWrVvYBAQAAAAAAAADw/1y+nEtptWvXTuHh4erRo4c2bdrk6nAAAAAAAAAAABWQy2eil1R4eLjeeecdtW/fXpmZmfrggw/Uo0cPxcXFqUuXLoUek5mZqczMTNt+UlKSJMlqtcpqtTol7vPbcWa75c1qtco0zQpzPSga/V150NeVC/1duRTob6vVNqvCarVKbvRzYJpm3vNvJBkyi6xrSPLw8LhoXUOSYRgu+ZnndwwAAAC4PF12SfSmTZuqadOmtv2oqCj9/vvvmjVrlsMk+vTp0xUTE1Og/MSJE8rIyCi3WM+Xlp1m126qd6pT2i1vVqtVZ8+elWma8vC4bL/YgGKivysP+rpyob8rlwv720hLU63/f+/EiRMyU91njJKcnKxG9SMUapECvDOLrOtV3VepLZurbpCnqhVRt4pF8qofoeTkZCUmJpZ1yEVKTk52ansAAAAAysZll0QvTKdOnbRkyRKH70+cOFHjxo2z7SclJalu3boKCQmxezhpecrKzdIrPV6RJNUOqy0fTx+ntFverFarDMNQSEgIiZdKgP6uPOjryoX+rlwK9HdWlqyv5I1RQmrXlnzcZ4ySkpKiQ4ePKqe5FGTxLbLun2cy9eO+/Qq6LldZ1R3XTUqVjhw+qsDAQIWGhpZ1yEXy8/NzansAAAAAykaFSKLv2rVL4eHhDt/39fWVr2/BD1MeHh5OSxb4efhpwvUTnNKWsxmG4dR7CdeivysP+rpyob8rF7v+9vOTJrjnGCV/2RVTkimjyLqm/n+pmovUNXVumRhn/7zz+wUAAABcnlyeRE9JSdGhQ4ds+4cPH9bu3btVo0YN1atXTxMnTtTx48e1ePFiSdKcOXMUGRmpli1bKisrS0uWLNHKlSu1cuVKV10CAAAAAAAAAKCCcnkSffv27erevbttP3/ZlSFDhig2Nlbx8fE6duyY7f2srCyNHz9ex48fl7+/v1q2bKl169YpOjra6bGXRK41Vzvjd0qSrgq/Sp4eni6OCAAAQFJurrQzb4yiq66SPBmjAAAAAMD5XJ5E79atm0zTdPh+bGys3f6ECRM0wU2/clyUjJwMXf3e1ZKklIkpsvhYXBwRAACApIwM6eq8MYpSUiQLYxQAAAAAOB8LMwIAAAAAAAAA4ABJdAAAAAAAAAAAHCCJDgAAAAAAAACAAyTRAQAAAAAAAABwgCQ6AAAAAAAAAAAOkEQHAAAAAAAAAMABL1cHUFl4e3prStcptm0AAAC34O0tTZlybhsAAAAAYIckupP4ePpoareprg4DAADAno+PNHWqq6MAAAAAALfFci4AAAAAAAAAADjATHQnsZpW7T+xX5LUPKS5PAz+fgEAANyA1SrtzxujqHlzyYMxCgAAAACcjyS6k6Rnp6vVvFaSpJSJKbL4WFwcEQAAgKT0dKlV3hhFKSmShTEKAAAAAJyPqUYAAAAAAAAAADhAEh0AAAAAAAAAAAdIogMAAAAAAAAA4ABJdAAAAAAAAAAAHCCJDgAAAAAAAACAAyTRAQAAAAAAAABwwMvVAVQW3p7eGh813rYNAADgFry9pfHjz20DAAAAAOyQRHcSH08fzew109VhAAAA2PPxkWYyRgEAAAAAR1jOBQAAAAAAAAAAB5iJ7iRW06pjZ49JkupVrScPg79fAAAAN2C1SsfyxiiqV0/yYIwCAAAAAOcjie4k6dnpqj+3viQpZWKKLD4WF0cEAAAgKT1dqp83RlFKimRhjAIAAAAA52OqEQAAAAAAAAAADpBEBwAAAAAAAADAAZLoAAAAAAAAAAA4QBIdAAAAAAAAAAAHSKIDAAAAAAAAAOAASXQAAAAAAAAAABzwcnUAlYWXh5ce7fCobRsAAMAteHlJjz56bhsAAAAAYIdPSk7i6+WrN/u+6eowAAAA7Pn6Sm8yRgEAAAAAR1jOBQAAAAAAAAAAB5iJ7iSmaepk2klJUnBAsAzDcHFEAAAAkkxTOpk3RlFwsMQYBQAAAADskER3krTsNIXOCpUkpUxMkcXH4uKIAAAAJKWlSaF5YxSlpEgWxigAAAAAcD6WcwEAAAAAAAAAwAGS6AAAAAAAAAAAOEASHQAAAAAAAAAAB0iiAwAAAAAAAADggMsfLPrVV19p5syZ2rFjh+Lj47V69WrddtttRR6zefNmjRs3Tvv27VPt2rU1YcIEjRgxwjkBAwAAACg3Hz/ZS9WqVXN1GJcFq9WqxMREhYaGysOD+VHFxX0rOe5ZyXHPSo57VnLcs5LjnpUO980NZqKnpqaqTZs2euONN4pV//Dhw4qOjlbnzp21a9cuTZo0SaNGjdLKlSvLOVIAAAAAAAAAQGXj8pnoffr0UZ8+fYpdf/78+apXr57mzJkjSWrevLm2b9+uWbNmaeDAgeUU5aXz8vDSkDZDbNsAAABuwctLGjLk3DYAAAAAwM5l90lp69at6tWrl11Z7969tWDBAmVnZ8vb29tFkRXN18tXsbfFujoMAAAAe76+Umysq6MAAAAAALd12SXRExISVKtWLbuyWrVqKScnRydPnlR4eHiBYzIzM5WZmWnbT0pKkpS3no/Vai3fgCs4q9Uq0zS5j5UE/V150Nfu7eTJk7b/lxUlKChIwcHBF61XHv1d3Bil4sdZHu2XR9vuLr+/T5w4oeTk5GIdk5WVJR8fnzKrJxXv3pumKcMwZEgyZBZZ15Dk4eFx0bqGJMMwXPJvHP+mAgAAAJenyy6JLuV98DmfaZqFluebPn26YmJiCpSfOHFCGRkZZR9gIUzTVHpOuiTJ38vfYayXG6vVqrNnz8o0zUr7YIHKhP6uPOhr93X27FnNnvu6UtIzL1q3ir+vnhj9uKpWrVpkvbLu75LEWJI4y6P9sm77cmC1WvXnn3/q3YWLlJyWIZmmfHNyJEmZXl7SBWOUnJwcnT6RqJohteTp5enwvMWtl6849z45OVmN6kco1CIFeBfdn17VfZXasrnqBnmqWhF1q1gkr/oRSk5OVmJi4kXjLEvF/aMFAAAAAPdy2SXRw8LClJCQYFeWmJgoLy8v1axZs9BjJk6cqHHjxtn2k5KSVLduXYWEhCgoKKhc482XmpWq2q/Uzmv/qSRZfCxOabe8Wa1WGYahkJAQEm2VAP1dedDX7islJUW7fz6okE4DZKlRy2G91NN/6dB3q+Tp6anQ0NAiz1nW/V3cGEsaZ1m3Xx5tXw6sVqsSEhK0++eDCr6mv2pUqaqFz94tSbrv+aXK9PGzq5/4v73a/d+v1P4fPVUzPMLheYtbTyr+vU9JSdGhw0eV01wKsvgWec4/z2Tqx337FXRdrrKqO66blCodOXxUgYGBTu93Pz+/i1cCAAAA4HYuuyR6VFSU1q5da1e2ceNGdejQweF66L6+vvL1LfhhysPDw2nJofPbcWa7zmAYRoW7JjhGf1ce9LV7yl+GIqBGLQWG1nFYz9S5pTCK04dl2d/FjbE0cZZl++XR9uXi/HtUpeq5SQhVQq6Qj6+/Xd3kUwmyWq3yrxZS5P0sbj2p+Pc+P05Tkqmiv8Vn6v+XqrlIXVf2e2X7OQMAAAAqCpeP5FNSUrR7927t3r1bknT48GHt3r1bx44dk5Q3i3zw4MG2+iNGjNDRo0c1btw47d+/X++//74WLFig8ePHuyJ8AAAAAAAAAEAF5vKZ6Nu3b1f37t1t+/nLrgwZMkSxsbGKj4+3JdQlqX79+lq/fr3Gjh2rN998U7Vr19Zrr72mgQMHOj12AAAAAAAAAEDF5vIkerdu3WwPBi1MbGxsgbKuXbtq586d5RgVAAAAAAAAAABusJwLAAAAAAAAAADuiiQ6AAAAAAAAAAAOuHw5l8rC08NTt7e43bYNAADgDqweHtre4QbbNgAAAADAHkl0J/Hz8tOKO1a4OgwAAAA7Od6+mjfyZVeHAQAAAABui+lGAAAAAAAAAAA4QBIdAAAAAAAAAAAHSKI7SWpWqowYQ0aModSsVFeHAwAAIEnyyUzXgmFXa8Gwq+WTme7qcAAAAADA7ZBEBwAAAAAAAADAAZLoAAAAAAAAAAA4QBIdAAAAAAAAAAAHSKIDAAAAAAAAAOAASXQAAAAAAAAAABwgiQ4AAAAAAAAAgANerg6gsvD08FR042jbNgAAgDuwenhoT+vrbNsAAAAAAHsk0Z3Ez8tP6+5e5+owAAAA7OR4+2ru2H+6OgwAAAAAcFtMNwIAAAAAAAAAwAGS6AAAAAAAAAAAOEAS3UlSs1Jleckiy0sWpWalujocAAAASZJPZrreeriL3nq4i3wy010dDgAAAAC4HdZEd6K07DRXhwAAAFCAb1aGq0MAAAAAALfFTHQAAAAAAAAAABwgiQ4AAAAAAAAAgAMk0QEAAAAAAAAAcIAkOgAAAAAAAAAADpBEBwAAAAAAAADAAS9XB1BZeBge6hrR1bYNAADgDkzD0C9Nr7JtAwAAAADskUR3En9vf8UNjXN1GAAAAHayffw08+n5rg4DAAAAANwWU6IBAAAAAAAAAHCAJDoAAAAAAAAAAA6QRHeS1KxUhcwMUcjMEKVmpbo6HAAAAEmST2a65jzeS3Me7yWfzHRXhwMAAAAAboc10Z3oZNpJV4cAAABQQGDK364OAQAAAADcFjPRAQAAAAAAAABwgCQ6AAAAAAAAAAAOkEQHAAAAAAAAAMABkugAAAAAAAAAADhAEh0AAAAAAAAAAAe8XB1AZeFheKhD7Q62bQAAAHdgGoYORza3bQMAAAAA7JFEdxJ/b39te3Cbq8MAAACwk+3jpxemLHJ1GAAAAADgtkiiA6gUcnNzlZ2d7eowLhtWq1XZ2dnKyMiQhwffnilP3t7e8vT0dHUYAAAAAADAAbdIor/11luaOXOm4uPj1bJlS82ZM0edO3cutG5cXJy6d+9eoHz//v1q1qxZeYcK4DJjmqYSEhL0999/uzqUy4ppmrJarUpOTpbB8g7lrlq1agoLC+NeAwAAAADghlyeRF++fLnGjBmjt956S9ddd53efvtt9enTRz///LPq1avn8LgDBw4oKCjIth8SEuKMcEstLTtNLd5sIUn6eeTPCvAOcHFEQOWQn0APDQ1VQEAAScpiMk1TOTk58vLy4p6VI9M0lZaWpsTERElSeHi4iyNCZeSTmaHnnxkkSXr2xeXK8vVzcUQAAAAA4F5cnkR/9dVXdf/99+uBBx6QJM2ZM0eff/655s2bp+nTpzs8LjQ0VNWqVXNSlJfONE0dPXvUtg2g/OXm5toS6DVr1nR1OJcVkujO4+/vL0lKTExUaGgoS7vABUwFn4q3bQMAAAAA7Lk0iZ6VlaUdO3bo6aeftivv1auXvv322yKPbdeunTIyMtSiRQtNnjy50CVe8mVmZiozM9O2n5SUJClvzV+r1XoJV1B857fjzHbLm9VqtS37gIrvcuvvzMxMmaYpf39//nhVCvn3jHtX/vJ/RjMzM+XnV/QsYNM0ZRiGDElGEQlPQ5JhGMX6nS3r3+3ixljSOMu6/fJo+3KQ39+F3SNDZoF7Zkjy8PAo1v0sTr38usW59yX9WSpunK7q98r0cwYAAABUJC5Nop88eVK5ubmqVauWXXmtWrWUkJBQ6DHh4eF655131L59e2VmZuqDDz5Qjx49FBcXpy5duhR6zPTp0xUTE1Og/MSJE8rIyLj0CymGtOw0u3ZTvVOd0m55s1qtOnv2rEzT5OGDlcDl1t/Z2dmyWq3Kzc1VTk6Oq8O5rJimqdzcXEliJroT5Obmymq16tSpU/L29i6ybnJyshrVj1CoRQrwznRYr4pF8qofoeTkZNtyMY6U9e92cWMsaZxl3X55tH05sFqtSk9PV6P6EQqxSFW9zt2jEK9MZXvb/wx4VfdVasvmqhvkqWpF3M/i1pOKf+9L8rNU3PZd2e/JyclObQ8AAABA2XD5ci5SwQRN/qyjwjRt2lRNmza17UdFRen333/XrFmzHCbRJ06cqHHjxtn2k5KSVLduXYWEhNitq16eUrPOJc1DQkJk8bE4pd3yZrVaZRiGQkJCLoukKi7N5dbfGRkZSk5OlpeXl7y83OKfu8vOxRK6KBteXl7y8PBQzZo1LzoTPSUlRYcOH1VOcynI4uuwXlKqdOTwUQUGBio0NLTIc5b173ZxYyxpnGXdfnm0fTmwWq1KSEjQocNHld1MyvI6d49O5PgqK9v+nv15JlM/7tuvoOtylVXd8f0sbj2p+Pe+JD9LxW3flf1+sd9vAAAAAO7JpVmw4OBgeXp6Fph1npiYWGB2elE6deqkX3/91eH7vr6+CgoKsntJeV/5deYrn7PbLe+XYRguj4EX/V1UvJfba9iwYfLw8NAjjzxS4L2RI0fKw8NDw4YNK9cYJNn9191eX331lTp06CB/f381bNhQb7/99kWP2b59u2688UZVr15dNWrUUO/evfXjjz/a3j948KBuuOEGhYWF2c777LPPKicnp9RtL1++XB4eHurfv3+xrqu4P9OmacqU/n/hDUevc3+UdvbvdvFjLHmcrrxHFel14T3K5+g+Wa3WYt3P4tUr/r0v6c9SceN0Zb8DAAAAuPy4dCTv4+Oj9u3b64svvrAr/+KLL3TttdcW+zy7du1SeHh4WYcHAC5Tt25dffTRR0pPT7eVZWRkaNmyZapXr54LI3O9w4cPKzo6Wp07d9auXbs0adIkjRo1SitXrnR4THJysnr37q169erp+++/15YtWxQUFKTevXsrOztbUt6s+8GDB2vjxo06cOCA5syZo3fffVdTpkwpVdtHjx7V+PHj1blz57K/CQAAAAAAwGlcPh1m3Lhxeu+99/T+++9r//79Gjt2rI4dO6YRI0ZIyluKZfDgwbb6c+bM0Zo1a/Trr79q3759mjhxolauXKnHHnvMVZdQLIZhqEVIC7UIaWGb3QkAjlx11VWqV6+eVq1aZStbtWqV6tatq3bt2tnVzczM1KhRoxQaGio/Pz9df/312rZtm6S82ZaNGjXSrFmz7I7Zu3evPDw89Ntvv0mSzp49q4ceekihoaEKCgpSjx499OOPP9rqT506VW3bttUHH3ygyMhIVa1aVXfeeafd+r7dunXTqFGjNGHCBNWoUUNhYWGaOnWqXbsXtnPDDTfYtVMc8+fPV7169TRnzhw1b95cDzzwgIYPH17gGs934MABnTlzRtOmTVPTpk3VsmVLTZkyRYmJiTp27JgkqUGDBho2bJjatGmjiIgI3XLLLbrnnnv09ddfl7jt3Nxc3XPPPYqJiVGDBg1KdH2A8xk6Xru+jteuL4kxCgAAAABcyOVJ9EGDBmnOnDmaNm2a2rZtq6+++krr169XRESEJCk+Pt6W4JCkrKwsjR8/Xq1bt1bnzp21ZcsWrVu3TgMGDHDVJRRLgHeA9j26T/se3acA7wBXhwNUbqmpjl8XPmy4qLrnzRIvsm4pDRs2TAsXLrTtv//++xo+fHiBehMmTNDKlSu1aNEi7dy5U40aNVLv3r11+vRpGYah4cOH250n/1ydO3dWw4YNZZqm+vbtq4SEBK1fv147duxQu3btdNNNN+n06dO2Y3777TetWbNGn376qT799FNt3rxZL7/8st15Fy1aJIvFou+//14zZszQtGnTbN82Kqydq666Sj169LC1c+TIERmGobi4OIf3ZevWrerVq5ddWe/evbV9+3bbrPILNW3aVMHBwVqwYIGysrKUnp6uBQsWqGXLlrb/31zo0KFD2rBhg7p27VritqdNm6aQkBDdf//9Dq8DcBdZvn567sXleu7F5cryZc1uAAAAALiQy5PokvToo4/qyJEjyszM1I4dO+weEBobG2uXTJkwYYIOHTqk9PR0nT59Wl9//bWio6NdEDWAy1aVKo5fAwfa1w0NdVy3Tx/7upGRhdcrpfvuu09btmzRkSNHdPToUX3zzTe699577eqkpqZq3rx5mjlzpvr06aMWLVro3Xfflb+/vxYsWCApLxl/4MAB/fDDD5Kk7OxsLVmyxJaQ37Rpk3766SetWLFCHTp0UOPGjTVr1ixVq1ZNH3/8sa0tq9Wq2NhYtWrVSp07d9Z9992nL7/80i6e1q1ba8qUKWrcuLEGDx6sDh062OoUpx1vb281bdpUAQGO/9iYkJBQ4LkZtWrVUk5Ojk6ePFnoMYGBgYqLi9OSJUvk7++vKlWq6PPPP9f69esLPHT22muvlZ+fnxo3bqzOnTtr2rRpJWr7m2++0YIFC/Tuu+86vAYAAAAAAHD58Lp4FQCAKwQHB6tv375atGiRbRZ3cHCwXZ3ffvtN2dnZuu6662xl3t7euvrqq7V//35JUnh4uPr27av3339fV199tT799FNlZGTojjvukCTt2LFDKSkpqlmzpt2509PTbcu9SFJkZKQCAwNt++Hh4UpMTLQ7pnXr1nb759cpTjtXXHGFfvnll4vemwuXxTJNs9Dy89sYPny4rrvuOi1btky5ubmaNWuWoqOjtW3bNvn7+9vqLl++XMnJyfrxxx/15JNPatasWZowYUKx2k5OTta9996rd999t0BfAQAAAACAyxNJdCdJy05Tx3c7SpK2PbiNJV0AV0pJcfyep6f9/gVJYjseF3yZ58iRUofkyPDhw23PfHjzzTcLvO8oeWyapl3ZAw88oPvuu0///Oc/tXDhQg0aNMg229tqtSo8PNzuWz+maSonJ8cuEezt7W3XhmEYslqtdmVF1SmsnXzVqlUr7PILFRYWpoSEBLuyxMREeXl5FUjQ51u6dKmOHDmirVu3yuP/+23p0qWqXr26/v3vf+vOO++01a1bt64kqUWLFsrNzdVDDz2kJ554Qp6enhdte9++fTpy5Ij69etnez//+r28vHTgwAE1bNiw2NcKOINPZoYmTxsiSXrhuUUs6QIAAAAAFyCJ7iSmaernEz/btgG4kMXi+rrFdNNNNykrK0tS3trbF2rUqJF8fHy0ZcsW3X333ZLylmvZvn27xowZY6sXHR0ti8WiefPm6bPPPtNXX31le++qq65SQkKCvLy8FBkZKelcEv3CpU4uRWHtlEZUVJTWrl1rV7Zx40Z16NChQBI/X1pamjw8POz+sJC/f+EfAs5nmqays7Nt/25frO1mzZrpp59+snt/8uTJSk5O1ty5c20JesC9mLriz8O2bQAAAACAPbdYEx0AUDhPT0/t379f+/fvl+eFs+QlWSwWPfLII3ryySe1YcMG/fzzz3rwwQeVlpZm91BLT09PDR06VBMnTlSjRo0UFRVle+/GG29UVFSUbrvtNn3++ec6cuSIvv32Wz333HPavn17mV2Lo3YmT55sa+f48eNq1qyZbf32wowYMUJHjx7VuHHjtH//fr3//vtasGCBxo8fb6uzevVqNWvWzLbfs2dPnTlzRiNHjtT+/fu1b98+DRs2TF5eXurevbsk6cMPP9S//vUv7d+/X//73/+0YsUKTZw4UYMGDbL9MeFibfv5+alVq1Z2r2rVqikwMFCtWrWSj49Pmd1PAAAAAADgHMxEBwA3FxQUVOT7L7/8sqxWq+677z4lJyerQ4cO+vzzz1W9enW7evfff79eeukl2wNF8xmGofXr1+uZZ57R8OHDdeLECYWFhen6668v8BDNS+GonS5dutjayc7O1oEDB5SWlubwPPXr19f69es1duxYvfnmm6pdu7Zee+01DTzvobBnz57VgQMHbPvNmjXT2rVrFRMTo6ioKHl4eKhdu3basGGDwsPDJeUtt/LKK6/o4MGDMk1TERERGjlypMaOHVuitgEAAAAAQMVCEh0A3ExsbGyR769Zs8Zu38/PT6+99ppee+21Io+Lj4+Xl5eXBg8eXOC9wMBAu3NcuJzL1KlTNXXqVLtjxowZY7dkTGFrnV8Y64XtXCgyMrJYS1517dpVO3fudPj+0KFDNXToULuynj17qmfPng6PGTRokAYNGnTJbV/oYv0JAAAAAADcG0l0AKjgMjMz9fvvv+vZZ5/VP/7xjzKdXQ4AAAAAAFDRsSY6AFRwy5YtU9OmTXX27FnNmDHD1eEAAAAAAABcVpiJ7iSGYSiiaoRtGwCcpbClTQDgHEMna4bbtgEAAAAA9kiiO0mAd4COjDni6jAAAADsZPn66alZ/3Z1GAAAAADgtljOBQAAAAAAAAAAB0iiAwAAAAAAAADgAEl0J0nPTlfHdzuq47sdlZ6d7upwAAAAJEneWRmaHDNEk2OGyDsrw9XhAAAAAIDbYU10J7GaVm3/c7ttGwAAwB0Ypqn6R/bbtgEAAAAA9piJDgAAAAAAAACAAyTRAQCFaty4sebMmWPbNwxDa9ascVk8AAAAAAAArkASHQDczNChQ2UYhu1Vs2ZN3XTTTdqzZ49L44qPj1efPn2c3q5pmpo6dapq164tf39/devWTfv27SvymG7dutndw/xX3759bXVycnI0efJk1a9fX/7+/mrQoIGmTZsmq/XcklsX9oVhGOrUqZNdWwkJCbrvvvsUFhYmi8Wiq666Sh9//HHZ3gQAAAAAAOAyJNEBwA3ddNNNio+PV3x8vL788kt5eXnp5ptvdmlMYWFh8vX1dXq7M2bM0Kuvvqo33nhD27ZtU1hYmHr27Knk5GSHx6xatcp2/+Lj47V37155enrqjjvusNV55ZVXNH/+fL3xxhvav3+/ZsyYoZkzZ+r111+3O9f5fREfH6/169fbvX/ffffpwIED+uSTT/TTTz9pwIABGjRokHbt2lW2NwIAAAAAALgESXQAcEO+vr4KCwtTWFiY2rZtq6eeekq///67Tpw4Yavz1FNPqUmTJgoICFCDBg307LPPKjs72/b+jz/+qO7duyswMFBBQUFq3769tm/fbnv/22+/VZcuXeTv76+6detq1KhRSk1NdRjT+cu5HDlyRIZhaNWqVerevbsCAgLUpk0bbd261e6YkrZxIdM0NWfOHD3zzDMaMGCAWrVqpUWLFiktLU1Lly51eFyNGjVs9y8sLExffPGFAgIC7JLoW7du1a233qq+ffsqMjJSt99+u3r16mV3jyT7vggLC1ONGjXs3t+6dasef/xxXX311WrQoIEmT56satWqaefOncW+TgAAAAAA4L5IojtRcECwggOCXR0GUOmlZqU6fGXkZBS7bnp2erHqXqqUlBR9+OGHatSokWrWrGkrDwwMVGxsrH7++WfNnTtX7777rv75z3/a3r/nnntUp04dbdu2TTt27NDTTz8tb29vSdJPP/2k3r17a8CAAdqzZ4+WL1+uLVu26LHHHitRbM8884zGjx+v3bt3q0mTJrrrrruUk5NT7DamTp2qyMhIh+c/fPiwEhIS1KtXL1uZr6+vunbtqm+//bbYcS5YsEB33nmnLBaLrez666/Xl19+qYMHD0rK+6PDli1bFB0dbXdsXFycQkND1aRJEz344INKTEy0e//666/X8uXLdfr0aVmtVn300UfKzMxUt27dih0f4GrJVaopuUo1V4cBAAAAAG7Jy9UBVBYWH4tOPHni4hUBlLsq06s4fC+6cbTW3b3Oth86K1Rp2WmF1u0a0VVxQ+Ns+5FzI3Uy7WSBeuYUs8Qxfvrpp6pSJS/O1NRUhYeH69NPP5WHx7m/fU6ePPlc25GReuKJJ7R8+XJNmDBBknTs2DE9+eSTatasmaS8B4Xmmzlzpu6++26NGTPG9t5rr72mrl27at68ecVetmX8+PG2dcZjYmLUsmVLHTp0SM2aNbtoG35+fgoODlbDhg0dnj8hIUGSVKtWLbvyWrVq6ejRo8WK8YcfftDevXu1YMECu/KnnnpKZ8+eVbNmzeTp6anc3Fy9+OKLuuuuu2x1+vTpozvuuEMRERE6fPiwnn32Wd1www3asWOH7R4tX75cgwYNUs2aNeXl5aWAgACtXr26yOsC3EmWr7/GvL7R1WEAAAAAgNsiiQ4Abqh79+6aN2+eJOn06dN666231KdPH/3www+KiIiQJH388ceaM2eODh06pJSUFOXk5CgoKMh2jnHjxumBBx7QBx98oBtvvFF33HGHLbG7Y8cOHTp0SB9++KGtvmmaslqtOnz4sC3xfjGtW7e2bYeHh0uSEhMT1axZs4u20bx5cz322GPFmv1uGIbdvmmaBcocWbBggVq1aqWrr77arnz58uVasmSJli5dqpYtW2r37t0aM2aMateurSFDhkiSBg0aZKvfqlUrdejQQREREVq3bp0GDBggKe+PGWfOnNF//vMfBQcHa82aNbrjjjv09ddf68orryxWjAAAAAAAwH2RRAdQ6aRMTHH4nqeHp91+4vhEBzUlD8N+Rawjo49cUlzns1gsatSokW2/ffv2qlq1qt5991298MIL+u6773TnnXcqJiZGvXv3VtWqVfXRRx9p9uzZtmOmTp2qu+++W+vWrdNnn32mKVOm6KOPPlL//v1ltVr18MMPa9SoUQXarlevXrHjzF8eRjqX6LZarbb/XmobYWFhkvJmpOcn6aW8RP2Fs9MLk5aWpo8++kjTpk0r8N6TTz6pp59+Wnfeeack6corr9TRo0c1ffp0WxL9QuHh4YqIiNCvv/4qSfrtt9/0xhtvaO/evWrZsqUkqU2bNvr666/15ptvav78+cW6TgAAAAAA4L5IojtJena6+nzYR5L02T2fyd/b38URAZWXxcdy8UrlXLekDMOQh4eH0tPz1mH/5ptvFBERoWeeecZWp7DlTZo0aaImTZpo7Nixuuuuu7Rw4UL1799fV111lfbt22eXqD+faZZ8CZoLXayN4qhfv77twaDt2rWTJGVlZWnz5s165ZVXLnr8v/71L2VmZuree+8t8F5aWprd8jiS5OnpafsjQGFOnTql33//3ZbQT0vLW+qnpOcB3Il3VobGvDpGkjRn3Bxl+/i5NiAAAAAAcDM8WNRJrKZVm49u1uajm2U1SawAKFpmZqYSEhKUkJCg/fv36/HHH1dKSor69esnSWrUqJGOHTumjz76SL/99ptee+01rV692nZ8enq6HnvsMcXFxeno0aP65ptvtG3bNjVv3lxS3nrgW7du1ciRI7V79279+uuv+uSTT/T444+X2TUUp4033nhDPXr0cHgOwzA0ZswYvfTSS1q9erX27t2roUOHKiAgQHfffbet3uDBgzVx4sQCxy9YsEC33Xab3QNZ8/Xr108vvvii1q1bpyNHjmj16tV69dVX1b9/f0l5D3QdP368tm7dqiNHjiguLk79+vVTcHCwrU6zZs3UqFEjPfzww/rhhx/022+/afbs2friiy902223lfbWAU5lmKaaHdipZgd2yiiDP6ABAAAAQEXDTHQAcEMbNmywzXYODAxUs2bNtGLFCnXr1k2SdOutt2rs2LF67LHHlJmZqb59++rZZ5/V1KlTJeXNhD516pQGDx6sv/76S8HBwRowYIBiYmIk5a1lvnnzZj3zzDPq3LmzTNNUw4YN7dYAv1TFaePkyZP67bffijzPhAkTlJ6erkcffVRnzpzRNddco40bNyowMNBW59ixYwVmgx88eFBbtmzRxo2FPzDx9ddf17PPPqtHH31UiYmJql27th5++GE999xzkvLu4U8//aTFixfr77//Vnh4uLp3767ly5fb2vb29tb69ev19NNPq1+/fkpJSVGjRo20aNEiRUdHl+q+AQAAAAAA90ISHQDcTGxsrGJjYy9ab8aMGZoxY4Zd2ZgxYyRJPj4+WrZsWZHHd+zY0WGCWZJ+/fVXeXmd+9/E+Uu8REZGFljypVq1agXKLtbG1KlTbYl/RwzDuGi9uLi4AmVNmjQpclmawMBAzZkzR3PmzCn0fX9/f33++edFxiZJjRs31sqVKy9aDwAAAAAAXJ5YzgUAAAAAAAAAAAdIogMAAAAAAAAA4ABJdAAAAAAAAAAAHGBNdCcK8A5wdQgAAAAFZPr4uToEAAAAAHBbJNGdxOJjUeqkVFeHAQAAYCfL11+Pvv2Vq8MA8H/t3XtYlGX6B/DvMDMwQiKCBQMYISmIm4CShkZYmaS2HdbUVjMz6VoyQyN1Md1QU8pDSaZk4THz0Cbaaqnpbmkqua4Kq8nkkRJtjPUUqBsCc//+aJnfzsIo78AcnPl+rmuui3l43nfu534e4JmblxciIiIiclm8nQsRuT0RcXYIRNfFNUpERERERETkulhEJyK3pdVqAQBXr151ciRE11e3RuvWLBERERERERG5Dt7OxUF+qfkFA/48AABQMKgAOg3vPUpkb2q1GgEBASgvLwcA+Pr6QqVSOTmqm4OIoKamBhqNhjmzIxHB1atXUV5ejoCAAKjVameHRB5IU12FF+dnAQAWjH4TNVofJ0dERERERETkWlhEd5BaUy02Hdtk/piIHCMkJAQAzIV0ahwRgclkgpeXF4voDhAQEGBeq0SO5mUyofPB3eaPiYiIiIiIyBKL6ETk1lQqFfR6PW677TZUV1c7O5ybhslkwvnz5xEUFAQvL975y560Wi2vQCciIiIiIiJyYS5RRM/Ly8Ps2bNhNBrRqVMn5ObmIjk52Wr/HTt2IDMzE4cPH0ZoaCgmTJiA9PR0B0ZMRDcbtVrNQqUCJpMJWq0WOp2ORXQiIiIiIiIi8mhOr4x8/PHHGDt2LCZNmoSioiIkJyejb9++OHXqVIP9S0tL0a9fPyQnJ6OoqAivvvoqMjIyUFBQ4ODIiYiIiIiIiIiIiMjdOb2I/vbbb2PkyJFIS0tDx44dkZubi7Zt2+K9995rsP/ChQtx++23Izc3Fx07dkRaWhqee+45zJkzx8GRExEREREREREREZG7c2oR/dq1a9i/fz/69Olj0d6nTx8UFhY2eMw333xTr39qair27dvH+x0TERERERERERERUbNy6j3Rz507h9raWgQHB1u0BwcH4+zZsw0ec/bs2Qb719TU4Ny5c9Dr9fWOqaqqQlVVlfn5zz//DAC4dOkSTCZTU4fRKFeuXQF+gfl1q73do+BvMplQUVEBb29v3jfZA3C+PQfn2nVVVFTAVFuLn43fo+aXq1b7XblYjupffsHhw4dRUVFxw/NWVlbCaDQ2S4xlZWWorqq6YYy2xNmcr2+P175ZlJaWmnPkVXEBdaO/UHYM17x1Fn0r/3UaKgCVP5VBq7J+zsb2AxqfeyVrqbGvf+ViOUy1taioqMClS5euH2gzqxuriDj0dW8WdXmpqKjgz55GMplMqKys5P8wUYh5U445U445U445U445U445s407562xe3SVOHEX/+OPPyIsLAyFhYVISkoyt8+YMQMrVqzAd999V++YDh06YMSIEZg4caK5bffu3bj33nthNBoREhJS75gpU6Zg6tSp9hkEEREREZECZWVlCA8Pd3YYLufkyZOIiopydhhERERE5IFutEd36pXobdq0gVqtrnfVeXl5eb2rzeuEhIQ02F+j0SAoKKjBYyZOnIjMzEzzc5PJhAsXLiAoKAgq1Q0ulaLrqqioQNu2bVFWVgZ/f39nh0N2xvn2HJxrz8L59iycb+cREVRWViI0NNTZobikwMBAAMCpU6fQqlUrJ0dzc+DXs22YN+WYM+WYM+WYM+WYM+WYM9u4c94au0d3ahHd29sbXbt2xbZt2/DEE0+Y27dt24bHHnuswWOSkpKwceNGi7atW7ciMTERWq22wWN8fHzg4+Nj0RYQENC04MmCv7+/230RkXWcb8/BufYsnG/Pwvl2DhaHrav70+BWrVpxbSrEr2fbMG/KMWfKMWfKMWfKMWfKMWe2cde8NWaP7vSb2GRmZmLRokVYsmQJDAYDXn75ZZw6dQrp6ekAfr2K/JlnnjH3T09Pxw8//IDMzEwYDAYsWbIEixcvxrhx45w1BCIiIiIiIiIiIiJyU069Eh0ABg8ejPPnz2PatGkwGo34zW9+g02bNiEiIgIAYDQacerUKXP/yMhIbNq0CS+//DIWLFiA0NBQzJs3DwMGDHDWEIiIiIiIiIiIiIjITTm9iA4Ao0aNwqhRoxr83LJly+q1paSk4MCBA3aOihrDx8cH2dnZ9W6XQ+6J8+05ONeehfPtWTjf5Kq4NpVjzmzDvCnHnCnHnCnHnCnHnCnHnNmGeQNUIiLODoKIiIiIiIiIiIiIyBU5/Z7oRERERERERERERESuikV0IiIiIiIiIiIiIiIrWEQnIiIiIiIiIiIiIrKCRXSykJeXh8jISOh0OnTt2hU7d+602vfZZ5+FSqWq9+jUqZO5T3V1NaZNm4aoqCjodDrExcVhy5YtjhgKNYKS+QaAlStXIi4uDr6+vtDr9RgxYgTOnz9v0aegoACxsbHw8fFBbGws1q9fb88hkALNPd+HDx/GgAEDcMcdd0ClUiE3N9fOIyAlmnu+8/PzkZycjNatW6N169bo3bs39u7da+9hUCM193yvW7cOiYmJCAgIgJ+fH+Lj47FixQp7D4M8gNK1umPHDnTt2hU6nQ7t2rXDwoULHRSp61CSM6PRiCFDhiA6OhpeXl4YO3as4wJ1IUpytm7dOjz00EO49dZb4e/vj6SkJHzxxRcOjNZ1KMnbrl270LNnTwQFBaFFixaIiYnB3LlzHRita1D6Pa3O7t27odFoEB8fb98AXZCSnG3fvr3BmsN3333nwIidT+k6q6qqwqRJkxAREQEfHx9ERUVhyZIlDorWNTR3bcsT2KM+5HaE6D/WrFkjWq1W8vPzpaSkRMaMGSN+fn7yww8/NNj/0qVLYjQazY+ysjIJDAyU7Oxsc58JEyZIaGiofP7553LixAnJy8sTnU4nBw4ccNCoyBql871z507x8vKSd955R06ePCk7d+6UTp06yeOPP27uU1hYKGq1WnJycsRgMEhOTo5oNBrZs2ePo4ZFVthjvvfu3Svjxo2T1atXS0hIiMydO9dBo6Ebscd8DxkyRBYsWCBFRUViMBhkxIgR0qpVKzl9+rSjhkVW2GO+v/rqK1m3bp2UlJTI8ePHJTc3V9RqtWzZssVRwyI3pHStnjx5Unx9fWXMmDFSUlIi+fn5otVqZe3atQ6O3HmU5qy0tFQyMjJk+fLlEh8fL2PGjHFswC5Aac7GjBkjM2fOlL1798rRo0dl4sSJotVqPe79itK8HThwQFatWiXffvutlJaWyooVK8TX11fef/99B0fuPEpzVufSpUvSrl076dOnj8TFxTkmWBehNGdfffWVAJAjR45Y1B5qamocHLnz2LLOHn30Uenevbts27ZNSktL5e9//7vs3r3bgVE7lz1qW+7OHu8n3BGL6GTWrVs3SU9Pt2iLiYmRrKysRh2/fv16UalU8v3335vb9Hq9zJ8/36LfY489JkOHDm16wNQkSud79uzZ0q5dO4u2efPmSXh4uPn5oEGD5OGHH7bok5qaKk899VQzRU22ssd8/7eIiAgW0V2IvedbRKSmpkZatmwpy5cvb3rA1CSOmG8RkYSEBJk8eXLTgiWPpnStTpgwQWJiYiza/vCHP8g999xjtxhdTVP25ykpKR5ZRG/qexoRkdjYWJk6dWpzh+bSmiNvTzzxhDz99NPNHZrLsjVngwcPlsmTJ0t2drbHFdGV5qyuiH7x4kUHROealOZs8+bN0qpVKzl//rwjwnNJ9qhtuTtHvZ+42fF2LgQAuHbtGvbv348+ffpYtPfp0weFhYWNOsfixYvRu3dvREREmNuqqqqg0+ks+rVo0QK7du1qetBkM1vmu0ePHjh9+jQ2bdoEEcFPP/2EtWvXon///uY+33zzTb1zpqamNnoNkX3Ya77JNTlqvq9evYrq6moEBgY2a/ykjCPmW0Twt7/9DUeOHMF9993X7GMgz2DLWrW2r9i3bx+qq6vtFquraI79uadpjpyZTCZUVlZ61M+35shbUVERCgsLkZKSYo8QXY6tOVu6dClOnDiB7Oxse4focpqyzhISEqDX6/Hggw/iq6++smeYLsWWnG3YsAGJiYmYNWsWwsLC0KFDB4wbNw7//ve/HRGy09mrtuXOWC9oPBbRCQBw7tw51NbWIjg42KI9ODgYZ8+eveHxRqMRmzdvRlpamkV7amoq3n77bRw7dgwmkwnbtm3DX/7yFxiNxmaNn5SxZb579OiBlStXYvDgwfD29kZISAgCAgLw7rvvmvucPXvW5jVE9mOv+SbX5Kj5zsrKQlhYGHr37t2s8ZMy9pzvn3/+Gbfccgu8vb3Rv39/vPvuu3jooYfsNhZyb7asVWv7ipqaGpw7d85usbqKpu7PPVFz5Oytt97ClStXMGjQIHuE6JKakrfw8HD4+PggMTERL774Yr33g+7KlpwdO3YMWVlZWLlyJTQajSPCdCm25Eyv1+ODDz5AQUEB1q1bh+joaDz44IP4+uuvHRGy09mSs5MnT2LXrl349ttvsX79euTm5mLt2rV48cUXHRGy09mrtuXOWC9oPBbRyYJKpbJ4LiL12hqybNkyBAQE4PHHH7dof+edd9C+fXvExMTA29sbo0ePxogRI6BWq5szbLKRkvkuKSlBRkYGXnvtNezfvx9btmxBaWkp0tPTbT4nOZY95ptclz3ne9asWVi9ejXWrVtX76+NyDnsMd8tW7ZEcXEx/vGPf2DGjBnIzMzE9u3b7TUE8hBK9wkN9W+o3Z1xb6WcrTlbvXo1pkyZgo8//hi33XabvcJzWbbkbefOndi3bx8WLlyI3NxcrF692p4hupzG5qy2thZDhgzB1KlT0aFDB0eF55KUrLPo6Gg8//zz6NKlC5KSkpCXl4f+/ftjzpw5jgjVZSjJmclkgkqlwsqVK9GtWzf069cPb7/9NpYtW+YxV6MDzV/b8gSsF9yY5/36kxrUpk0bqNXqer9lKi8vr/fbqP8lIliyZAmGDRsGb29vi8/deuut+PTTT/HLL7/g/PnzCA0NRVZWFiIjI5t9DNR4tsz3G2+8gZ49e2L8+PEAgM6dO8PPzw/JycmYPn069Ho9QkJCbFpDZF/2mm9yTfae7zlz5iAnJwd//etf0blzZ/sNhBrFnvPt5eWFO++8EwAQHx8Pg8GAN954A7169bLfgMht2bJWre0rNBoNgoKC7Barq2jK/txTNSVnH3/8MUaOHIlPPvnE4/7Kqil5q3tfd9ddd+Gnn37ClClT8Pvf/95usboKpTmrrKzEvn37UFRUhNGjRwP4tdgpItBoNNi6dSseeOABh8TuLM31Pe2ee+7BRx991NzhuSRbcqbX6xEWFoZWrVqZ2zp27AgRwenTp9G+fXu7xuxs9qptuTPWCxqPV6ITAMDb2xtdu3bFtm3bLNq3bduGHj16XPfYHTt24Pjx4xg5cqTVPjqdDmFhYaipqUFBQQEee+yxZombbGPLfF+9ehVeXpbfMur+oqDuqrCkpKR659y6desN1xDZl73mm1yTPed79uzZeP3117FlyxYkJiY2c+RkC0d+fYsIqqqqmhgxeSpb1qq1fUViYiK0Wq3dYnUVTdmfeypbc7Z69Wo8++yzWLVqldvfz7UhzbXWPOnnhNKc+fv749ChQyguLjY/0tPTER0djeLiYnTv3t1RoTtNc62zoqIity3Q/S9bctazZ0/8+OOPuHz5srnt6NGj8PLyQnh4uF3jdQX2rm25I9YLFLD7vy6lm8aaNWtEq9XK4sWLpaSkRMaOHSt+fn7m/0iclZUlw4YNq3fc008/Ld27d2/wnHv27JGCggI5ceKEfP311/LAAw9IZGSkR/93bVehdL6XLl0qGo1G8vLy5MSJE7Jr1y5JTEyUbt26mfvs3r1b1Gq1vPnmm2IwGOTNN98UjUYje/bscfj4yJI95ruqqkqKioqkqKhI9Hq9jBs3ToqKiuTYsWMOHx9Zssd8z5w5U7y9vWXt2rViNBrNj8rKSoePjyzZY75zcnJk69atcuLECTEYDPLWW2+JRqOR/Px8h4+P3IfStXry5Enx9fWVl19+WUpKSmTx4sWi1Wpl7dq1zhqCw9myP6/72dy1a1cZMmSIFBUVyeHDh50RvlMozdmqVatEo9HIggULLH6+Xbp0yVlDcAqleZs/f75s2LBBjh49KkePHpUlS5aIv7+/TJo0yVlDcDhb3z/Xyc7Olri4OAdF6xqU5mzu3Lmyfv16OXr0qHz77beSlZUlAKSgoMBZQ3A4pTmrrKyU8PBwefLJJ+Xw4cOyY8cOad++vaSlpTlrCA5nj9qWu7PH+wl3xCI6WViwYIFERESIt7e3dOnSRXbs2GH+3PDhwyUlJcWi/6VLl6RFixbywQcfNHi+7du3S8eOHcXHx0eCgoJk2LBhcubMGXsOgRRQOt/z5s2T2NhYadGihej1ehk6dKicPn3aos8nn3wi0dHRotVqJSYmxqM2OK6uuee7tLRUANR7/O95yDmae74jIiIanO/s7GwHjYiup7nne9KkSXLnnXeKTqeT1q1bS1JSkqxZs8ZRwyE3pnStbt++XRISEsTb21vuuOMOee+99xwcsfMpzVlD36sjIiIcG7STKclZSkpKgzkbPny44wN3MiV5mzdvnnTq1El8fX3F399fEhISJC8vT2pra50QufMo/fr8b55YRBdRlrOZM2dKVFSUeT9y7733yueff+6EqJ1L6TozGAzSu3dvadGihYSHh0tmZqZcvXrVwVE7V3PXtjyBPepD7kYl4s7X2RMRERERERERERER2Y73RCciIiIiIiIiIiIisoJFdCIiIiIiIiIiIiIiK1hEJyIiIiIiIiIiIiKygkV0IiIiIiIiIiIiIiIrWEQnIiIiIiIiIiIiIrKCRXQiIiIiIiIiIiIiIitYRCciIiIiIiIiIiIisoJFdCIiIiIiIiIiIiIiK1hEJyIiIiIiIiIiIiKygkV0IqKb2LPPPguVSlXvcfz4cXz99df47W9/i9DQUKhUKnz66aeNOmdRUREeeeQR3HbbbdDpdLjjjjswePBgnDt3zr6DISIiIiKyo+vtnQFw/0xERFaxiE5EdJN7+OGHYTQaLR6RkZG4cuUK4uLiMH/+/Eafq7y8HL1790abNm3wxRdfwGAwYMmSJdDr9bh69ardxlBdXW23cxMRERER1bG2dwbA/TMREVnFIjoR0U3Ox8cHISEhFg+1Wo2+ffti+vTp+N3vftfocxUWFqKiogKLFi1CQkICIiMj8cADDyA3Nxe33367ud/hw4fRv39/+Pv7o2XLlkhOTsaJEycAACaTCdOmTUN4eDh8fHwQHx+PLVu2mI/9/vvvoVKp8Oc//xm9evWCTqfDRx99BABYunQpOnbsCJ1Oh5iYGOTl5TVTloiIiIiIrO+dAXjE/rlXr1546aWXMHbsWLRu3RrBwcH44IMPcOXKFYwYMQItW7ZEVFQUNm/ebHHcjh070K1bN/j4+ECv1yMrKws1NTUAgA8//BBBQUGoqqqyOGbAgAF45plnzM83btyIrl27QqfToV27dpg6dar5HACgUqmwaNEiPPHEE/D19UX79u2xYcOGRs8FEZE9sYhORERmISEhqKmpwfr16yEiDfY5c+YM7rvvPuh0Onz55ZfYv38/nnvuOfMG+J133sFbb72FOXPm4ODBg0hNTcWjjz6KY8eOWZznj3/8IzIyMmAwGJCamor8/HxMmjQJM2bMgMFgQE5ODv70pz9h+fLldh83EREREZEtbsb98/Lly9GmTRvs3bsXL730El544QUMHDgQPXr0wIEDB5Camophw4aZr6Q/c+YM+vXrh7vvvhv//Oc/8d5772Hx4sWYPn06AGDgwIGora21KHifO3cOn332GUaMGAEA+OKLL/D0008jIyMDJSUleP/997Fs2TLMmDHDIrapU6di0KBBOHjwIPr164ehQ4fiwoULCmaEiMhOhIiIblrDhw8XtVotfn5+5seTTz5Zrx8AWb9+faPO+eqrr4pGo5HAwEB5+OGHZdasWXL27Fnz5ydOnCiRkZFy7dq1Bo8PDQ2VGTNmWLTdfffdMmrUKBERKS0tFQCSm5tr0adt27ayatUqi7bXX39dkpKSGhU3EREREdH1NHbvLOK+++eUlBS59957zc9ramrEz89Phg0bZm4zGo0CQL755hvz+KKjo8VkMpn7LFiwQG655Rapra0VEZEXXnhB+vbta/58bm6utGvXznxMcnKy5OTkWMSyYsUK0ev15ucAZPLkyebnly9fFpVKJZs3b7Y6HiIiR+GV6EREN7n7778fxcXF5se8efMadVxOTg5uueUW8+PUqVMAgBkzZuDs2bNYuHAhYmNjsXDhQsTExODQoUMAgOLiYiQnJ0Or1dY7Z0VFBX788Uf07NnTor1nz54wGAwWbYmJieaP//Wvf6GsrAwjR460iGn69OnmP3MlIiIiImoqW/fOgPvsnzt37mz+WK1WIygoCHfddZe5LTg4GMCv93sHAIPBgKSkJKhUKov4Ll++jNOnTwMAnn/+eWzduhVnzpwB8OttZur+kSsA7N+/H9OmTbOI9fnnn4fRaLS4d/x/x+bn54eWLVua4yAiciaNswMgIqKm8fPzw5133qn4uPT0dAwaNMj8PDQ01PxxUFAQBg4ciIEDB+KNN95AQkIC5syZg+XLl6NFixY3PPd/b7ABQETqtfn5+Zk/NplMAID8/Hx0797dol/dPSqJiIiIiJrK1r0z4D775/8t5qtUKou2utete42GYpH/3Lqmrj0hIQFxcXH48MMPkZqaikOHDmHjxo0W8U6dOrXB+83rdLrrxlYXBxGRM7GITkTkoQIDAxEYGHjDft7e3oiKisKVK1cA/Hp1yPLly1FdXV1vk+vv74/Q0FDs2rUL9913n7m9sLAQ3bp1s/oawcHBCAsLw8mTJzF06FAbR0REREREZD+eun+OjY1FQUGBRTG9sLAQLVu2RFhYmLlfWloa5s6dizNnzqB3795o27at+XNdunTBkSNHbP4FBhGRs7GITkTkpi5fvozjx4+bn5eWlqK4uBiBgYG4/fbbGzzms88+w5o1a/DUU0+hQ4cOEBFs3LgRmzZtwtKlSwEAo0ePxrvvvounnnoKEydORKtWrbBnzx5069YN0dHRGD9+PLKzsxEVFYX4+HgsXboUxcXFWLly5XXjnTJlCjIyMuDv74++ffuiqqoK+/btw8WLF5GZmdl8iSEiIiIiagD3zw0bNWoUcnNz8dJLL2H06NE4cuQIsrOzkZmZCS+v/79L8NChQzFu3Djk5+fjww8/tDjHa6+9hkceeQRt27bFwIED4eXlhYMHD+LQoUPmf1BKROTKWEQnInJT+/btw/33329+XreRHj58OJYtW9bgMbGxsfD19cUrr7yCsrIy+Pj4oH379li0aBGGDRsG4Nc/Vf3yyy8xfvx4pKSkQK1WIz4+3nwfx4yMDFRUVOCVV15BeXk5YmNjsWHDBrRv3/668aalpcHX1xezZ8/GhAkT4Ofnh7vuugtjx45tejKIiIiIiG6A++eGhYWFYdOmTRg/fjzi4uIQGBiIkSNHYvLkyRb9/P39MWDAAHz++ed4/PHHLT6XmpqKzz77DNOmTcOsWbOg1WoRExODtLS0Zo2ViMheVFJ3IysiIiIiIiIiIiIbPfTQQ+jYsaOif9hKRHQzYBGdiIiIiIiIiIhsduHCBWzduhVDhw5FSUkJoqOjnR0SEVGz4u1ciIiIiIiIiIjIZl26dMHFixcxc+ZMFtCJyC3xSnQiIiIiIiIiIiIiIiu8btyFiIiIiIiIiIiIiMgzsYhORERERERERERERGQFi+hERERERERERERERFawiE5EREREREREREREZAWL6EREREREREREREREVrCITkRERERERERERERkBYvoRERERERERERERERWsIhORERERERERERERGQFi+hERERERERERERERFb8H9rPQxuahZuwAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyser grid results\n",
    "analyze_experiment_results(\n",
    "    grid_results,\n",
    "    baseline_f1=baseline_f1,\n",
    "    show_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 10. Configuration Optimale\n",
    "\n",
    "Identification et sauvegarde de la meilleure configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🏆 CONFIGURATION OPTIMALE\n",
      "================================================================================\n",
      "Vectorizer: TFIDF\n",
      "Max features: 20000\n",
      "N-gram range: (1, 1)\n",
      "Features manuelles: Oui\n",
      "\n",
      "Performance:\n",
      "  F1-Score: 0.8351\n",
      "  Accuracy: 0.8345\n",
      "  Amélioration vs baseline: +6.28%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Meilleure configuration\n",
    "best = grid_results.iloc[0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🏆 CONFIGURATION OPTIMALE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Vectorizer: {best['vectorizer'].upper()}\")\n",
    "print(f\"Max features: {best['max_features']}\")\n",
    "print(f\"N-gram range: {best['ngram_range']}\")\n",
    "print(f\"Features manuelles: {'Oui' if best['use_features'] else 'Non'}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  F1-Score: {best['f1_score']:.4f}\")\n",
    "print(f\"  Accuracy: {best['accuracy']:.4f}\")\n",
    "improvement_vs_baseline = (best['f1_score'] - baseline_f1) / baseline_f1 * 100\n",
    "print(f\"  Amélioration vs baseline: {improvement_vs_baseline:+.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 11. Export Configuration\n",
    "\n",
    "Sauvegarder pour Notebook 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration sauvegardée: results/configs/best_vectorization_config.json\n",
      "\n",
      "✓ Configuration exportée vers: results/configs/best_vectorization_config.json\n",
      "✓ Prête pour Notebook 03 (Model Selection)\n"
     ]
    }
   ],
   "source": [
    "# Construire le dictionnaire de configuration\n",
    "import ast\n",
    "\n",
    "ngram_tuple = ast.literal_eval(best['ngram_range']) if isinstance(best['ngram_range'], str) else best['ngram_range']\n",
    "\n",
    "BEST_CONFIG = {\n",
    "    'vectorizer_type': best['vectorizer'],\n",
    "    'strategy': 'split',\n",
    "    'text_columns': ['title_clean', 'desc_clean'],\n",
    "    'feature_columns': feature_columns if best['use_features'] else None,\n",
    "    'max_features_title': int(best['max_features']),\n",
    "    'max_features_desc': int(best['max_features']),\n",
    "    'ngram_range': ngram_tuple,\n",
    "    'title_weight': float(best.get('title_weight', 1.0)),\n",
    "    'min_df': 2,\n",
    "    'max_df': 0.95\n",
    "}\n",
    "\n",
    "# Sauvegarder\n",
    "save_vectorization_config(\n",
    "    BEST_CONFIG,\n",
    "    'results/configs/best_vectorization_config.json',\n",
    "    metadata={\n",
    "        'f1_score': float(best['f1_score']),\n",
    "        'accuracy': float(best['accuracy']),\n",
    "        'train_time': float(best['train_time']),\n",
    "        'notebook': '02_Vectorization_Strategies.ipynb',\n",
    "        'baseline_f1': baseline_f1,\n",
    "        'improvement_pct': float(improvement_vs_baseline),\n",
    "        'title_weight': float(best.get('title_weight', 1.0))\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Configuration exportée vers: results/configs/best_vectorization_config.json\")\n",
    "print(\"✓ Prête pour Notebook 03 (Model Selection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 12. Conclusions & Rapport Final\n",
    "\n",
    "**Génération automatique** du rapport complet avec:\n",
    "- ✅ Tracking global de tous les scores\n",
    "- ✅ Vérification de l'optimalité de la configuration\n",
    "- ✅ Résumé détaillé des 5 découvertes principales\n",
    "- ✅ Performance finale et prochaines étapes\n",
    "\n",
    "Le rapport est affiché ci-dessous et sauvegardé dans `results/vectorization_report.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Rapport sauvegardé: results/vectorization_report.txt\n",
      "\n",
      "================================================================================\n",
      "📊 RAPPORT DE VECTORISATION - PHASE 2\n",
      "================================================================================\n",
      "Généré le: 2025-12-18 13:53:35\n",
      "Total expériences: 24\n",
      "\n",
      "================================================================================\n",
      "RÉSUMÉ DES DÉCOUVERTES\n",
      "================================================================================\n",
      "\n",
      "1️⃣  Count vs TF-IDF\n",
      "   → TF-IDF (F1=0.8231) surpasse Count (F1=0.8159)\n",
      "   → Amélioration: +0.88%\n",
      "\n",
      "2️⃣  Split vs Merged\n",
      "   → Split (F1=0.8231) surpasse Merged (F1=0.7854)\n",
      "   → Amélioration: +4.80%\n",
      "\n",
      "3️⃣  Pondération du Titre\n",
      "   → Poids optimal: 1.5x\n",
      "   → F1 = 0.8248 (baseline 1.0x: 0.8231)\n",
      "   → Amélioration: +0.20%\n",
      "\n",
      "4️⃣  Features Manuelles\n",
      "   → Sans features: F1 = 0.8231\n",
      "   → Avec features: F1 = 0.8276\n",
      "   → Amélioration: +0.55%\n",
      "\n",
      "5️⃣  Hyperparamètres Optimaux\n",
      "   → Vectorizer: TFIDF\n",
      "   → Max features: 20,000\n",
      "   → N-gram range: (1, 1)\n",
      "   → Title weight: 1.5x\n",
      "   → Features manuelles: Oui\n",
      "   → F1 Final: 0.8351\n",
      "\n",
      "================================================================================\n",
      "🏆 PERFORMANCE FINALE\n",
      "================================================================================\n",
      "Baseline (raw data, NB01):              F1 = 0.7858\n",
      "Meilleur score (configuration finale):  F1 = 0.8351\n",
      "Amélioration totale:                    +6.28%\n",
      "\n",
      "Statut: ✅ Configuration optimale validée\n",
      "\n",
      "================================================================================\n",
      "🔍 VÉRIFICATION\n",
      "================================================================================\n",
      "Config exportée: results/configs/best_vectorization_config.json\n",
      "Tracking sauvegardé: results/all_scores_tracking.csv\n",
      "Score config finale: F1 = 0.8351\n",
      "Meilleur score global: F1 = 0.8351\n",
      "\n",
      "================================================================================\n",
      "📋 PROCHAINES ÉTAPES\n",
      "================================================================================\n",
      "→ Notebook 03: Sélection du meilleur modèle\n",
      "→ Phase 3: Intégration des features images\n",
      "→ Phase 4: Fusion multimodale\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RÉSUMÉ RAPIDE\n",
      "================================================================================\n",
      "✓ Configuration optimale: ✅ OUI\n",
      "✓ Meilleur F1 score: 0.8351\n",
      "✓ Baseline F1: 0.7858\n",
      "✓ Amélioration totale: +6.28%\n",
      "\n",
      "✓ Rapport sauvegardé: results/vectorization_report.txt\n",
      "✓ Tracking sauvegardé: results/all_scores_tracking.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GÉNÉRATION DU RAPPORT COMPLET\n",
    "# ============================================================================\n",
    "\n",
    "report = generate_vectorization_report(\n",
    "    df_vec=df_vec,\n",
    "    results_weighting=results_weighting,\n",
    "    results_strategies=results_strategies,\n",
    "    grid_results=grid_results,\n",
    "    result_tfidf=result_tfidf,\n",
    "    result_merged=result_merged,\n",
    "    baseline_f1=baseline_f1,\n",
    "    save_report=True,\n",
    "    report_path='results/vectorization_report.txt',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Résumé rapide\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSUMÉ RAPIDE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Configuration optimale: {'✅ OUI' if report['is_optimal'] else '⚠️ NON'}\")\n",
    "print(f\"✓ Meilleur F1 score: {report['best_score']:.4f}\")\n",
    "print(f\"✓ Baseline F1: {report['baseline_score']:.4f}\")\n",
    "print(f\"✓ Amélioration totale: +{report['total_improvement_pct']:.2f}%\")\n",
    "print(f\"\\n✓ Rapport sauvegardé: results/vectorization_report.txt\")\n",
    "print(f\"✓ Tracking sauvegardé: results/all_scores_tracking.csv\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Évaluation Finale sur Hold-out Test Set\n",
    "\n",
    "**Évaluation finale** de la configuration optimale sur le hold-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ÉVALUATION FINALE SUR HOLD-OUT TEST SET\n",
      "================================================================================\n",
      "\n",
      "Entraînement sur development set complet...\n",
      "\n",
      "================================================================================\n",
      "Expérience: TFIDF + LOGREG\n",
      "  Strategy: split\n",
      "  Max features: 20000\n",
      "  N-gram range: (1, 1)\n",
      "  Features manuelles: Oui\n",
      "================================================================================\n",
      "\n",
      "Entraînement du pipeline...\n",
      "✓ Entraînement terminé en 37.10s\n",
      "\n",
      "================================================================================\n",
      "RÉSULTATS DE L'ÉVALUATION\n",
      "================================================================================\n",
      "F1-Score (weighted): 0.8443\n",
      "Accuracy:            0.8444\n",
      "Temps prédiction:    0.51s\n",
      "\n",
      "Rapport de classification:\n",
      "--------------------------------------------------------------------------------\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "     Livres techniques       0.51      0.61      0.56       467\n",
      "          Rétro Gaming       0.74      0.72      0.73       376\n",
      "        Accessoires JV       0.84      0.84      0.84       252\n",
      "              Consoles       0.91      0.81      0.86       125\n",
      "              Figurine       0.80      0.82      0.81       401\n",
      "Cartes à collectionner       0.93      0.96      0.95       593\n",
      "          Jeux de rôle       0.77      0.53      0.63       115\n",
      "    Jouets & Figurines       0.76      0.77      0.77       731\n",
      "        Jeux éducatifs       0.69      0.54      0.61       311\n",
      "    Modélisme & Drones       0.98      0.96      0.97       757\n",
      "           Bébé & Jeux       0.99      0.95      0.97       121\n",
      "       Sport & Loisirs       0.87      0.76      0.81       374\n",
      "   Bébé & Puériculture       0.84      0.83      0.83       486\n",
      "     Équipement maison       0.84      0.83      0.84       761\n",
      "              Textiles       0.91      0.92      0.91       646\n",
      "          Alimentation       0.96      0.90      0.93       120\n",
      "      Déco & Éclairage       0.82      0.85      0.83       749\n",
      "               Animaux       0.97      0.83      0.90       124\n",
      "  Journaux & magazines       0.79      0.86      0.83       714\n",
      "Séries & encyclopédies       0.76      0.77      0.77       716\n",
      "            Jeux Vidéo       0.82      0.77      0.80       213\n",
      "    Fournitures bureau       0.93      0.95      0.94       748\n",
      "      Jardinage & déco       0.82      0.76      0.79       388\n",
      "               Piscine       0.96      0.98      0.97      1531\n",
      "    Jardin & Bricolage       0.84      0.80      0.82       374\n",
      "  Romans & littérature       0.76      0.78      0.77       414\n",
      "               Jeux PC       1.00      0.99      1.00       131\n",
      "\n",
      "              accuracy                           0.84     12738\n",
      "             macro avg       0.85      0.82      0.83     12738\n",
      "          weighted avg       0.85      0.84      0.84     12738\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RÉSULTATS FINAUX\n",
      "================================================================================\n",
      "F1-Score (validation):  0.8351\n",
      "F1-Score (hold-out):    0.8443\n",
      "Différence:             +0.0091 (+1.10%)\n",
      "================================================================================\n",
      "\n",
      "✓ Résultat final sauvegardé: results/final_vectorization_holdout.json\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ÉVALUATION FINALE SUR HOLD-OUT TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Entraîner sur TOUT le development set avec la config optimale\n",
    "print(\"\\nEntraînement sur development set complet...\")\n",
    "\n",
    "# Préparer merged column pour holdout\n",
    "df_holdout['text_merged'] = df_holdout['title_clean'] + ' ' + df_holdout['desc_clean']\n",
    "\n",
    "# Utiliser la meilleure config\n",
    "final_result = run_single_experiment(\n",
    "    df_dev,  # Train sur TOUT le development set\n",
    "    df_holdout,  # Test sur hold-out\n",
    "    y_dev,\n",
    "    y_holdout,\n",
    "    vectorizer_type=best['vectorizer'],\n",
    "    strategy='split',\n",
    "    text_columns=['title_clean', 'desc_clean'],\n",
    "    feature_columns=feature_columns if best['use_features'] else None,\n",
    "    max_features=int(best['max_features']),\n",
    "    ngram_range=ngram_tuple,\n",
    "    title_weight=float(best.get('title_weight', 1.0)),\n",
    "    model_name='logreg',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RÉSULTATS FINAUX\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"F1-Score (validation):  {best['f1_score']:.4f}\")\n",
    "print(f\"F1-Score (hold-out):    {final_result['f1_score']:.4f}\")\n",
    "diff = final_result['f1_score'] - best['f1_score']\n",
    "print(f\"Différence:             {diff:+.4f} ({diff/best['f1_score']*100:+.2f}%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sauvegarder\n",
    "final_eval = {\n",
    "    'val_f1': float(best['f1_score']),\n",
    "    'holdout_f1': float(final_result['f1_score']),\n",
    "    'difference': float(diff),\n",
    "    'config': BEST_CONFIG\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('results/final_vectorization_holdout.json', 'w') as f:\n",
    "    json.dump(final_eval, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Résultat final sauvegardé: results/final_vectorization_holdout.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rakuten_groupe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
