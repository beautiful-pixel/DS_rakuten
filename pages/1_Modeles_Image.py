"""
üñºÔ∏è Mod√®les de Classification d'Images

Cette page pr√©sente les 5 mod√®les de deep learning utilis√©s pour la classification
des images de produits Rakuten, leur histoire, caract√©ristiques et configurations.
"""

import streamlit as st
from pathlib import Path

# Page configuration
st.set_page_config(
    page_title="Mod√®les Image",
    page_icon="üñºÔ∏è",
    layout="wide"
)

# Custom CSS (same style as Index Alignment page)
st.markdown("""
<style>
    .section-header {
        font-size: 1.8rem;
        font-weight: bold;
        color: #1f77b4;
        margin-top: 2rem;
        margin-bottom: 1rem;
        border-bottom: 2px solid #1f77b4;
        padding-bottom: 0.5rem;
    }
    .subsection-header {
        font-size: 1.4rem;
        font-weight: bold;
        color: #ff7f0e;
        margin-top: 1.5rem;
        margin-bottom: 0.8rem;
    }
    .model-card {
        background-color: #f8f9fa;
        border-left: 4px solid #1f77b4;
        padding: 1.5rem;
        margin: 1rem 0;
        border-radius: 0.5rem;
    }
    .key-point {
        background-color: #e8f4f8;
        border-left: 4px solid #2ca02c;
        padding: 0.8rem;
        margin: 0.5rem 0;
    }
    .history-box {
        background-color: #fff9e6;
        border-left: 4px solid #ffa500;
        padding: 1rem;
        margin: 0.8rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Title
st.markdown("# üñºÔ∏è Mod√®les de Classification d'Images")
st.markdown("### Vue d'ensemble des architectures de Deep Learning pour la classification de produits")

st.info("""
**5 architectures de r√©seaux de neurones convolutifs** ont √©t√© explor√©es dans ce projet,
repr√©sentant l'√©volution des approches de computer vision de 1998 √† 2022.
""")

# ========== LeNet-5 ==========
st.markdown('<div class="section-header">1. LeNet-5 (1998)</div>', unsafe_allow_html=True)

st.markdown('<div class="history-box">', unsafe_allow_html=True)
st.markdown("""
**üìö Histoire et Contexte**

LeNet-5, d√©velopp√© par Yann LeCun et ses coll√®gues aux Bell Labs en 1998, est l'un des premiers
r√©seaux de neurones convolutifs (CNN) r√©ussis. Con√ßu initialement pour la reconnaissance de
chiffres manuscrits (MNIST), il a pos√© les bases de l'architecture CNN moderne.

**üéØ Caract√©ristiques Principales**
- **Architecture simple** : Seulement 2 couches convolutives et 3 couches fully-connected
- **Faible nombre de param√®tres** : ~60K param√®tres (tr√®s l√©ger)
- **Taille d'entr√©e r√©duite** : Con√ßu pour des images 32√ó32 pixels
- **Activation** : Utilise tanh (dans la version originale)
- **Pooling** : Average pooling apr√®s chaque convolution
- **Contexte historique** : R√©volutionnaire pour son √©poque, mais consid√©r√© comme basique aujourd'hui
""")
st.markdown('</div>', unsafe_allow_html=True)

st.markdown('<div class="key-point">‚úÖ <b>Utilisation dans le projet</b> : Mod√®le de r√©f√©rence (baseline) pour √©valuer les gains des architectures modernes</div>', unsafe_allow_html=True)

with st.expander("‚öôÔ∏è Configuration LeNet-5 (src/train/image_lenet.py)"):
    st.code('''@dataclass
class LeNetConfig:
    # Data / IO
    raw_dir: str                      # Chemin vers les fichiers CSV bruts
    img_dir: str                      # Chemin vers le r√©pertoire d'images
    out_dir: str                      # R√©pertoire d'export des pr√©dictions
    ckpt_dir: str                     # R√©pertoire des checkpoints

    # Training
    img_size: int = 32                # LeNet utilise typiquement 32x32
    batch_size: int = 128
    num_workers: int = 4
    num_epochs: int = 30
    lr: float = 1e-3                  # Learning rate
    weight_decay: float = 1e-4
    use_amp: bool = True              # Mixed precision training

    # Regularization
    dropout_rate: float = 0.5

    # Scheduler
    plateau_factor: float = 0.5       # R√©duction du LR sur plateau
    plateau_patience: int = 3

    # Runtime
    device: Optional[str] = None      # "cuda" ou "cpu"
    model_name: str = "lenet_v1"
    export_split: str = "val"''', language='python')

    st.markdown("**Architecture du Mod√®le**")
    st.code('''class LeNet5(nn.Module):
    """
    LeNet-5 adapt√© pour la classification de produits Rakuten.

    Architecture :
    - Conv1 : 3 ‚Üí 6 canaux, kernel 5√ó5
    - MaxPool1 : 2√ó2
    - Conv2 : 6 ‚Üí 16 canaux, kernel 5√ó5
    - MaxPool2 : 2√ó2
    - FC1 : ‚Üí 120
    - FC2 : 120 ‚Üí 84
    - FC3 : 84 ‚Üí num_classes (27)
    """''', language='python')

    st.caption("üìÑ Source : `src/train/image_lenet.py`")

# ========== ResNet50 ==========
st.markdown('<div class="section-header">2. ResNet50 (2015)</div>', unsafe_allow_html=True)

st.markdown('<div class="history-box">', unsafe_allow_html=True)
st.markdown("""
**üìö Histoire et Contexte**

ResNet (Residual Network), d√©velopp√© par Microsoft Research en 2015, a remport√© le concours
ImageNet ILSVRC 2015. L'innovation cl√© est l'introduction des **connexions r√©siduelles** (skip connections)
qui permettent d'entra√Æner des r√©seaux tr√®s profonds (50, 101, 152 couches ou plus).

**üéØ Caract√©ristiques Principales**
- **Connexions r√©siduelles** : F(x) + x, r√©sout le probl√®me de d√©gradation des r√©seaux profonds
- **Architecture modulaire** : Blocs r√©siduels empilables
- **50 couches** : √âquilibre entre profondeur et efficacit√© computationnelle
- **Batch Normalization** : Apr√®s chaque convolution
- **Pr√©-entra√Ænement ImageNet** : Transfer learning sur 1000 classes
- **~25M param√®tres** : Plus lourd que LeNet mais g√©rable
""")
st.markdown('</div>', unsafe_allow_html=True)

st.markdown('<div class="key-point">‚úÖ <b>Utilisation dans le projet</b> : Mod√®le CNN classique robuste avec transfer learning ImageNet</div>', unsafe_allow_html=True)

with st.expander("‚öôÔ∏è Configuration ResNet50 (src/train/image_resnet50.py)"):
    st.code('''@dataclass
class ResNet50Config:
    # Data / IO
    raw_dir: str                      # Chemin vers les CSV (donn√©es brutes)
    img_dir: str                      # Chemin vers les images
    out_dir: str                      # Export des pr√©dictions
    ckpt_dir: str                     # Checkpoints du mod√®le

    # Training
    img_size: int = 224               # R√©solution ImageNet standard
    batch_size: int = 64
    num_workers: int = 4
    num_epochs: int = 1
    lr: float = 1e-4
    use_amp: bool = True              # Mixed precision

    # Regularization
    label_smoothing: float = 0.1      # Label smoothing
    dropout_rate: float = 0.3         # Dropout dans la t√™te

    # Scheduler
    plateau_factor: float = 0.1       # R√©duction du LR
    plateau_patience: int = 3

    # Runtime
    device: Optional[str] = None
    model_name: str = "resnet50_rerun_canonical"
    export_split: str = "val"''', language='python')

    st.markdown("**Construction du Mod√®le**")
    st.code('''def _build_resnet50(num_classes: int, dropout_rate: float) -> nn.Module:
    """
    ResNet50 avec torchvision et t√™te personnalis√©e dropout + linear.
    """
    model = torchvision.models.resnet50(
        weights=ResNet50_Weights.IMAGENET1K_V2  # Poids pr√©-entra√Æn√©s
    )
    in_features = model.fc.in_features  # 2048
    model.fc = nn.Sequential(
        nn.Dropout(p=dropout_rate),
        nn.Linear(in_features, num_classes),
    )
    return model''', language='python')

    st.caption("üìÑ Source : `src/train/image_resnet50.py`")

# ========== Vision Transformer (ViT) ==========
st.markdown('<div class="section-header">3. Vision Transformer - ViT (2020)</div>', unsafe_allow_html=True)

st.markdown('<div class="history-box">', unsafe_allow_html=True)
st.markdown("""
**üìö Histoire et Contexte**

Vision Transformer (ViT), publi√© par Google Research en 2020, a marqu√© un tournant en appliquant
l'architecture **Transformer** (initialement con√ßue pour le NLP) √† la vision par ordinateur.
ViT d√©montre qu'on peut obtenir des performances excellentes **sans convolution**, uniquement avec
des m√©canismes d'attention.

**üéØ Caract√©ristiques Principales**
- **Patch-based** : D√©coupe l'image en patches 16√ó16, trait√©s comme des tokens
- **Self-attention multi-t√™tes** : Capture les relations globales entre patches
- **Position embeddings** : Encodage de la position spatiale des patches
- **Architecture pure Transformer** : Pas de convolution
- **Scalabilit√©** : Performance augmente avec la taille du dataset et du mod√®le
- **Variants** : ViT-Tiny, Base, Large, Huge (86M ‚Üí 632M param√®tres)
""")
st.markdown('</div>', unsafe_allow_html=True)

st.markdown('<div class="key-point">‚úÖ <b>Utilisation dans le projet</b> : Repr√©sente l\'approche Transformer pure pour la vision, alternative aux CNNs</div>', unsafe_allow_html=True)

with st.expander("‚öôÔ∏è Configuration ViT (src/train/image_vit.py)"):
    st.code('''@dataclass
class ViTConfig:
    # Data / IO
    raw_dir: str                      # Donn√©es CSV brutes
    img_dir: str                      # Images
    out_dir: str                      # Export des pr√©dictions
    ckpt_dir: str                     # Checkpoints

    # Training
    img_size: int = 224               # R√©solution standard ViT
    batch_size: int = 64
    num_workers: int = 4
    num_epochs: int = 1
    lr: float = 1e-4
    use_amp: bool = True

    # Regularization
    label_smoothing: float = 0.1
    dropout_rate: float = 0.1         # Dropout l√©ger (ViT r√©gularis√© par nature)

    # Scheduler
    plateau_factor: float = 0.1
    plateau_patience: int = 3

    # Model
    vit_model_name: str = "vit_tiny_patch16_224"  # Mod√®le timm
    vit_pretrained: bool = True                   # Poids pr√©-entra√Æn√©s

    # Runtime
    device: Optional[str] = None
    model_name: str = "vit_rerun_canonical_smoke"
    export_split: str = "val"
    force_colab_loader: bool = False''', language='python')

    st.markdown("**Principe de Fonctionnement**")
    st.markdown("""
    1. **Patchification** : Image 224√ó224 ‚Üí 196 patches de 16√ó16
    2. **Linear projection** : Chaque patch ‚Üí embedding de dimension D
    3. **Position embedding** : Ajoute l'information de position spatiale
    4. **Transformer encoder** : N couches d'attention multi-t√™tes
    5. **Classification head** : MLP sur le token [CLS]
    """)

    st.caption("üìÑ Source : `src/train/image_vit.py`")

# ========== Swin Transformer ==========
st.markdown('<div class="section-header">4. Swin Transformer (2021)</div>', unsafe_allow_html=True)

st.markdown('<div class="history-box">', unsafe_allow_html=True)
st.markdown("""
**üìö Histoire et Contexte**

Swin Transformer (Shifted Window Transformer), d√©velopp√© par Microsoft Research en 2021,
am√©liore ViT en introduisant une **hi√©rarchie multi-√©chelle** et une **attention locale par fen√™tres**.
Il combine les avantages des CNNs (inductive bias spatial) et des Transformers (self-attention).

**üéØ Caract√©ristiques Principales**
- **Attention par fen√™tres d√©cal√©es** : R√©duit la complexit√© de O(n¬≤) √† O(n)
- **Hi√©rarchie pyramidale** : Feature maps de r√©solutions d√©croissantes (comme CNN)
- **Patch merging** : R√©duit progressivement la r√©solution spatiale
- **Efficacit√© computationnelle** : Plus rapide que ViT gr√¢ce √† l'attention locale
- **Performances SOTA** : Meilleur que ViT sur plusieurs benchmarks
- **Versatilit√©** : Excellente pour classification, d√©tection, segmentation
""")
st.markdown('</div>', unsafe_allow_html=True)

st.markdown('<div class="key-point">‚úÖ <b>Utilisation dans le projet</b> : Transformer hi√©rarchique optimis√© pour la vision, meilleur compromis pr√©cision/vitesse</div>', unsafe_allow_html=True)

with st.expander("‚öôÔ∏è Configuration Swin Transformer (src/train/image_swin.py)"):
    st.code('''@dataclass
class SwinConfig:
    # Data / IO
    raw_dir: str                      # CSV bruts
    img_dir: str                      # Images
    out_dir: str                      # Pr√©dictions export√©es
    ckpt_dir: str                     # Checkpoints

    # Training
    img_size: int = 224
    batch_size: int = 128
    num_workers: int = 4
    num_epochs: int = 30
    lr: float = 5e-5                  # LR plus faible (fine-tuning)
    weight_decay: float = 0.05
    use_amp: bool = True

    # Regularization
    label_smoothing: float = 0.1
    dropout_rate: float = 0.5         # Head dropout
    head_dropout2: float = 0.3        # Second head dropout
    drop_path_rate: float = 0.3       # Stochastic depth

    # Mixup/CutMix augmentation
    mixup_alpha: float = 0.8
    cutmix_alpha: float = 1.0
    mixup_prob: float = 1.0
    mixup_switch_prob: float = 0.5

    # Scheduler
    cosine_eta_min: float = 1e-6      # Cosine annealing

    # Model
    swin_model_name: str = "swin_base_patch4_window7_224"
    swin_pretrained: bool = True

    # Runtime
    device: Optional[str] = None
    model_name: str = "swin_v2"
    export_split: str = "val"
    force_colab_loader: bool = False''', language='python')

    st.markdown("**Architecture en √âtages**")
    st.markdown("""
    - **Stage 1** : 56√ó56, fen√™tre 7√ó7
    - **Stage 2** : 28√ó28, fen√™tre 7√ó7
    - **Stage 3** : 14√ó14, fen√™tre 7√ó7
    - **Stage 4** : 7√ó7, fen√™tre 7√ó7
    - **Shifted Window** : Alterne attention locale et d√©cal√©e entre couches
    """)

    st.caption("üìÑ Source : `src/train/image_swin.py`")

# ========== ConvNeXt ==========
st.markdown('<div class="section-header">5. ConvNeXt (2022)</div>', unsafe_allow_html=True)

st.markdown('<div class="history-box">', unsafe_allow_html=True)
st.markdown("""
**üìö Histoire et Contexte**

ConvNeXt, publi√© par Meta AI Research en 2022, est une **modernisation de ResNet** qui d√©montre
que les CNNs classiques peuvent rivaliser avec les Transformers si on adopte les bonnes pratiques
de design. C'est une r√©ponse aux Transformers : "Les CNNs ne sont pas morts !"

**üéØ Caract√©ristiques Principales**
- **CNN modernis√©** : Architecture inspir√©e de ResNet avec am√©liorations de Swin
- **Design choices** : Depthwise convolutions 7√ó7, GELU, Layer Normalization
- **Inductive bias** : Pr√©serve les avantages des CNNs (translation equivariance)
- **Simplicit√©** : Pur CNN, pas besoin d'attention complexe
- **Performances comp√©titives** : √âgale Swin Transformer avec moins de complexit√©
- **Scalabilit√©** : Variants Tiny, Small, Base, Large, XLarge
""")
st.markdown('</div>', unsafe_allow_html=True)

st.markdown('<div class="key-point">‚úÖ <b>Utilisation dans le projet</b> : CNN de derni√®re g√©n√©ration, meilleur compromis entre simplicit√© (CNN) et performance (niveau Transformer)</div>', unsafe_allow_html=True)

with st.expander("‚öôÔ∏è Configuration ConvNeXt (src/train/image_convnext.py)"):
    st.code('''@dataclass
class ConvNeXtConfig:
    # Data / IO
    raw_dir: str                      # CSV bruts
    img_dir: str                      # Images
    out_dir: str                      # Export pr√©dictions
    ckpt_dir: str                     # Checkpoints

    # Training
    img_size: int = 384               # R√©solution plus √©lev√©e pour ConvNeXt
    batch_size: int = 64
    num_workers: int = 4
    num_epochs: int = 30
    lr: float = 1e-4
    weight_decay: float = 0.05
    use_amp: bool = True

    # Regularization
    label_smoothing: float = 0.1
    dropout_rate: float = 0.5         # Head dropout
    head_dropout2: float = 0.3        # Second head dropout
    drop_path_rate: float = 0.3       # Stochastic depth

    # Mixup/CutMix augmentation
    mixup_alpha: float = 0.8
    cutmix_alpha: float = 1.0
    mixup_prob: float = 1.0
    mixup_switch_prob: float = 0.5

    # EMA (Exponential Moving Average)
    use_ema: bool = True              # EMA des poids du mod√®le
    ema_decay: float = 0.9999

    # Scheduler
    cosine_eta_min: float = 1e-6

    # Model
    convnext_model_name: str = "convnext_base"
    convnext_pretrained: bool = True

    # Runtime
    device: Optional[str] = None
    model_name: str = "convnext"
    export_split: str = "val"
    force_colab_loader: bool = False''', language='python')

    st.markdown("**Innovations de Design**")
    st.markdown("""
    1. **Macro design** : Stage ratios 1:1:3:1 (inspir√© de Swin)
    2. **Depthwise conv 7√ó7** : R√©ceptif champ plus large
    3. **Inverted bottleneck** : Expand ‚Üí Depthwise ‚Üí Project
    4. **Less activation** : Une seule GELU par bloc
    5. **Layer Normalization** : Au lieu de Batch Normalization
    6. **Separate downsampling** : Couches d√©di√©es pour r√©duction spatiale
    """)

    st.caption("üìÑ Source : `src/train/image_convnext.py`")

# ========== Tableau Comparatif ==========
st.markdown('<div class="section-header">üìä Tableau Comparatif des Mod√®les</div>', unsafe_allow_html=True)

comparison_data = {
    "Mod√®le": ["LeNet-5", "ResNet50", "ViT-Tiny", "Swin-Base", "ConvNeXt-Base"],
    "Ann√©e": ["1998", "2015", "2020", "2021", "2022"],
    "Type": ["CNN", "CNN (Residual)", "Transformer", "Transformer Hi√©rarchique", "CNN Modernis√©"],
    "Param√®tres": ["~60K", "~25M", "~5M", "~88M", "~89M"],
    "R√©solution": ["32√ó32", "224√ó224", "224√ó224", "224√ó224", "384√ó384"],
    "Complexit√©": ["Tr√®s faible", "Moyenne", "√âlev√©e (O(n¬≤))", "Moyenne (O(n))", "Moyenne"],
    "Transfer Learning": ["‚ùå", "‚úÖ ImageNet", "‚úÖ ImageNet", "‚úÖ ImageNet", "‚úÖ ImageNet"],
    "Innovation Cl√©": [
        "Premier CNN efficace",
        "Skip connections",
        "Attention globale",
        "Attention par fen√™tres",
        "CNN meets Transformer"
    ]
}

import pandas as pd
comparison_df = pd.DataFrame(comparison_data)
st.dataframe(comparison_df, use_container_width=True, hide_index=True)

# ========== √âvolution Chronologique ==========
st.markdown('<div class="section-header">üìà √âvolution de l\'Architecture</div>', unsafe_allow_html=True)

col1, col2 = st.columns([1, 2])

with col1:
    st.markdown("**Chronologie**")
    st.markdown("""
    - **1998** : LeNet-5
      ‚îî‚îÄ CNN basique
    - **2015** : ResNet
      ‚îî‚îÄ Connexions r√©siduelles
    - **2020** : ViT
      ‚îî‚îÄ Transformers pour la vision
    - **2021** : Swin
      ‚îî‚îÄ Transformers hi√©rarchiques
    - **2022** : ConvNeXt
      ‚îî‚îÄ Renaissance des CNNs
    """)

with col2:
    st.markdown("**Tendances et Le√ßons**")
    st.markdown("""
    1. **Profondeur** : De 7 couches (LeNet) √† 100+ (ResNet, Transformers)
    2. **Inductive bias** : CNNs (local) ‚Üí Transformers (global) ‚Üí Hybride (Swin, ConvNeXt)
    3. **Attention mechanisms** : R√©volution 2020-2022
    4. **Transfer learning** : Devenu standard depuis 2015
    5. **Augmentation** : Mixup/CutMix essentiels pour Transformers
    6. **Efficiency** : Swin et ConvNeXt optimisent le compromis performance/co√ªt
    """)

# ========== Performances dans le Projet ==========
st.markdown('<div class="section-header">üéØ Performances dans le Projet Rakuten</div>', unsafe_allow_html=True)

st.info("""
**R√©sultats sur l'ensemble de validation** (10,827 √©chantillons, 27 classes)

Les performances d√©taill√©es de chaque mod√®le sont disponibles dans les pages suivantes :
- **Page "Model Performance"** : M√©triques compl√®tes (accuracy, F1, precision, recall)
- **Page "Model Comparison"** : Comparaison directe entre mod√®les
- **Page "Ensemble Analysis"** : Fusion optimale des mod√®les

**Remarque** : Les mod√®les plus r√©cents (Swin, ConvNeXt) b√©n√©ficient de :
- Meilleure augmentation (Mixup/CutMix)
- R√©gularisation avanc√©e (stochastic depth, EMA)
- Optimisation moderne (cosine annealing, warmup)
""")

# Footer
st.markdown("---")
st.caption("üìù Documentation g√©n√©r√©e √† partir du code source | Rakuten Product Classification Project")
